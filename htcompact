#!/usr/bin/env python3
import configparser
import datetime
import getopt
import argparse
import logging
import os
import re
import socket
import sys
from logging.handlers import RotatingFileHandler

import htcondor
import numpy as np
from htcondor import JobEventType as jet
from rich import print as rprint
from rich.progress import track, Progress
from tabulate import tabulate
from plotille import Figure

from typing import List

log_inf_list = List[dict]
list_of_logs = List[str]
date_time = datetime.datetime
timedelta = datetime.timedelta

"""

This script is basically reading information from HTCondor log files and storing them into
dictionaries. By that its actually pretty simple to analyse the data.

The script makes the information compact and easy to under stand,
that's the reason for the name htcompact

Single logs can be read quite easily,
but also it's possible to summarize a whole directory with logs
to see for ex. the average runtime and usage of all the logs

"""

# Exit Codes
"""
 Normal Termination: 0
 Wrong Options or Arguments: 1
 No given files: 2
 TypeError: 3
 Keyboard interruption: 4
"""

# global variables
# option_shorts = "hsavm:"
# option_longs = ["help", "version", "verbose", "mode=",
#                 "std-log=", "std-err=", "std-out=",
#                 "ignore=", "show-more=", "no-config", "to-csv",
#                 "generate-log-file", "filter=", "extend",
#                 "print-event-table", "reverse-dns-lookup", "table-format="]

allowed_modes = {"a": "analyse",
                 "s": "summarize",
                 "as": "analysed-summary",
                 "d": "default"}

allowed_show_values = ["std-err", "std-out"]
allowed_ignore_values = ["execution-details", "times", "host-nodes",
                         "used-resources", "requested-resources",
                         "allocated-resources", "all-resources",
                         "errors"]
valid_table_formats = ["plain", "simple", "github", "grid",
                      "fancy_grid", "pipe","orgtbl", "rst",
                      "mediawiki", "html", "latex", "latex_raw",
                      "latex_booktabs", "tsv", "simple"]

default_configfile = "htcompact.conf"
accepted_states = ["true", "yes", "y", "enable", "enabled", "0"]

colors = {
        'red': "\033[0;31m",
        'green': "\033[0;32m",
        'yellow': "\033[0;33m",
        'magenta': "\033[0;35m",
        'cyan': "\033[0;36m",
        'blue': "\033[0;34m",
        'light_grey': "\033[37m",
        'back_to_default': "\033[0;39m"
    }

# thresholds for bad and low usage of resources
tolerated_usage_threshold = 0.1
bad_usage_threshold = 0.25


# def initialize():
#     """
#     This method initializes all global variables
#     :return:
#     """
#     global files, ignore_list, allowed_ignore_values, \
#         no_config, reverse_dns_lookup, \
#         store_dns_lookups, verbose_mode, \
#         to_csv, generate_log_file, \
#         filter_mode, filter_keywords, filter_extended, \
#         colors, tolerated_usage_threshold, bad_usage_threshold, \
#         std_log, std_err, std_out, \
#         table_format, reading_stdin, redirecting_output, \
#         show_list, allowed_show_values, mode
#
#     # global parameters, used for dynamical output of information
#     files = list()
#
#     # show more information, mostly given by HTCompact .err and .out files
#     mode = None
#
#     show_list = list()
#
#     # ignore information
#     ignore_list = list()
#
#     # if set do not use a config file, even if one was found
#     no_config = False
#
#     # Features:
#     reverse_dns_lookup = False
#     store_dns_lookups = dict()
#     to_csv = False
#
#     # logging tool
#     generate_log_file = False
#     verbose_mode = False
#
#     # filter mode
#     filter_mode = False
#     filter_keywords = list()
#     filter_extended = False
#
#     # escape sequences for colors
#     colors = {
#         'red': "\033[0;31m",
#         'green': "\033[0;32m",
#         'yellow': "\033[0;33m",
#         'magenta': "\033[0;35m",
#         'cyan': "\033[0;36m",
#         'blue': "\033[0;34m",
#         'light_grey': "\033[37m",
#         'back_to_default': "\033[0;39m"
#     }
#
#     # global variables with default values for err/log/out files
#     std_log = ""
#     std_err = ".err"
#     std_out = ".out"
#
#     # global defaults
#     table_format = "pretty"  # ascii by default
#
#     reading_stdin = False
#     redirecting_output = False

class GlobalPlayer(object):

    def __init__(self):
        self.colors = {
            'red': "\033[0;31m",
            'green': "\033[0;32m",
            'yellow': "\033[0;33m",
            'magenta': "\033[0;35m",
            'cyan': "\033[0;36m",
            'blue': "\033[0;34m",
            'light_grey': "\033[37m",
            'back_to_default': "\033[0;39m"
        }

        self.redirecting_stdout = None
        self.reading_stdin = None
        self.stdin_input = None


###############################
GlobalServant = GlobalPlayer()
###############################


def check_for_redirection():
    """
    This method should be activated first, when output is generated
    it does nothing, if output is printed to the terminal.
    If output gets redirected with > or | or other redirection tools, ignore escape sequences
    by setting them to ""
    :return:
    """

    GlobalServant.redirecting_stdout = not sys.stdout.isatty()
    GlobalServant.reading_stdin = not sys.stdin.isatty()

    if GlobalServant.reading_stdin:
        GlobalServant.stdin_input = sys.stdin.readlines()

    if GlobalServant.redirecting_stdout:

        GlobalServant.colors = {
            'red': "",
            'green': "",
            'yellow': "",
            'magenta': "",
            'cyan': "",
            'blue': "",
            'light_grey': "",
            'back_to_default': ""
        }


def define_parser(args: list) -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser()

    parser.add_argument("File or Directory", nargs="*", action="append",
                        help="any path to log or config file(s)",)

    parser.add_argument("--version",
                        help="Print out extended execution details",
                        action="store_true")

    parser.add_argument("-v", "--verbose",
                        help="Print out extended execution details",
                        action="store_true")
    parser.add_argument("--generate-log-file", nargs="?",
                        const="htcompact.log", default=None,
                        help="generates output about the process,"
                             " which is mostly useful for developers, "
                             "if no file is specified, default: htcompact.log")

    all_vals = []
    for item in allowed_modes.items():
        all_vals.extend(list(item))

    parser.add_argument("-m", help="Specifiy an interpretation mode",
                        choices=all_vals)
    parser.add_argument("--mode", help="Specifiy an interpretation mode",
                        choices=all_vals)

    parser.add_argument("-s", dest="summarizer_mode",
                        help="Short for --mode summarize,"
                             " combine with -a for analysed-summary mode",
                        action="store_true")

    parser.add_argument("-a", dest="analyser_mode",
                        help="Short for --mode analyse,"
                             " combine with -s for analysed-summary mode",
                        action="store_true")

    parser.add_argument("--std-log", help="Specify the log file suffix",
                        default="", type=str)
    parser.add_argument("--std-out", help="Specify the output file suffix",
                        default=".out", type=str)
    parser.add_argument("--std-err", help="Specify the error file suffix",
                        default=".err", type=str)

    ignore_metavar = "{"+allowed_ignore_values[0]+" ... "\
                     + allowed_ignore_values[-1]+"}"
    parser.add_argument("--ignore", nargs="+", action="append",
                        choices=allowed_ignore_values, dest="ignore_list",
                        metavar=ignore_metavar, default=[],
                        help="Ignore a section to not be printed")
    parser.add_argument("--show", nargs="+", action="append", dest="show_list",
                        choices=allowed_show_values, default=[],
                        help="Show more details")

    parser.add_argument("--no-config", action="store_true",
                        help="Ignore the config file "
                             "and run with script defaults")

    parser.add_argument("--filter", nargs="+", metavar="keywords",
                        help="Filter for the given keywords",
                        default=[], dest="filter_keywords",
                        action="append", type=str)
    parser.add_argument("--extend", action="store_true",
                        help="extend the filter keyword list "
                             "by specific error keywords")

    parser.add_argument("--print-event-table", action="store_true",
                        help="Prints out the event table")

    parser.add_argument("--reverse-dns-lookup", action="store_true",
                        help="Resolve the ip-address of an execution nodes"
                             " to their dns entry")

    table_metavar = "{"+valid_table_formats[0]+" ... "\
                    + valid_table_formats[-1]+ "}"
    parser.add_argument("--table-format", metavar=table_metavar,
                        default="pretty",
                        choices=valid_table_formats)

    cmd_parse = parser.parse_args(args)

    return cmd_parse


def manage_params(args: list) -> dict:
    """
    Uses the argparse parser defined in define_parser()
    returns a dict looking like
    {'verbose': True,
     'generate_log_file': 'htcompact.log',
     'mode': None,
     'std_log': '',
     'std_out': '.out',
     'std_err': '.err',
     'ignore': None,
     'show': None,
     'no_config': False,
     'filter': None,
     'extend': False,
     'print_event_table': False,
     'reverse_dns_lookup': False,
     'table_format': 'pretty',
     'files': []
     }

    :param args: list of args
    :return: dict with params
    """

    # listen to stdin and add these files
    if GlobalServant.reading_stdin:
        logging.debug("Listening to arguments from stdin")
        for line in GlobalServant.stdin_input:
            args.append(line.rstrip('\n'))

    cmd_parser = define_parser(args)

    # first of all check for prioritised/exit params
    if cmd_parser.version:
        print("Version: v1.1.0")
        sys.exit(0)
    if cmd_parser.print_event_table:
        print(get_event_information())
        sys.exit(0)

    try:
        if cmd_parser.extend and len(cmd_parser.filter) == 0:
            raise_value_error("--extend not allowed without --filter")
    except ValueError as err:
        print("htcompact: error: "+str(err))
        sys.exit(1)

    # parse the mode correctly
    if cmd_parser.analyser_mode and cmd_parser.summarizer_mode:
        mode = "analysed-summary"
    elif cmd_parser.analyser_mode:
        mode = "analyse"
    elif cmd_parser.summarizer_mode:
        mode = "summarize"
    elif cmd_parser.mode is not None:
        if cmd_parser.mode in allowed_modes.keys():
            mode = allowed_modes[cmd_parser.mode]
        else:
            mode = cmd_parser.mode
    else:
        mode = None  # will result in default mode

    cmd_dict = vars(cmd_parser)
    
    # rename to files
    cmd_dict["files"] = cmd_dict["File or Directory"][0]
    del cmd_dict["File or Directory"]
    
    # delete unneseccary information
    del cmd_dict["m"]
    del cmd_dict["summarizer_mode"]
    del cmd_dict["analyser_mode"]
    del cmd_dict["version"]

    # concat ignore list
    new_ignore_list = list()
    for li in cmd_dict["ignore_list"]:
        new_ignore_list.extend(li)
    cmd_dict["ignore_list"] = new_ignore_list

    # concat show list
    new_show_list = list()
    for li in cmd_dict["show_list"]:
        new_show_list.extend(li)
    cmd_dict["show_list"] = new_show_list

    # concat filter list
    new_filter_list = list()
    for li in cmd_dict["filter_keywords"]:
        new_filter_list.extend(li)
    cmd_dict["filter_keywords"] = new_filter_list

    cmd_dict["mode"] = mode
    return cmd_dict





# def remove_files_from_args(args: list, short_opts: str, long_opts: list):
#     """
#         I want to make it easier for the user to insert many files at once.
#         The script should detect option arguments and files
#
#         It will filter the given files and remove them from the argument list
#
#         !!!Exactly that is this method doing!!! (because getopt has no such function)
#
#     :param args: is supposed to be a list, in any case it should be the sys.argv[1:] list
#     :param short_opts: the short getopt options
#     :param long_opts: the long getopt options
#     :return: list of getopt arguments
#     """
#     if type(args) != list:
#         raise_value_error("Expecting a list as argument")
#
#     new_args = args.copy()
#     generate_files = list()
#     index = 0
#     # run through the command line arguments,
#     # skip the getopt argument and interpret everything else as a file
#     while True:
#         if index >= len(args):
#             break
#
#         arg = args[index]
#
#         is_long_arg = False
#
#         if arg.startswith("-"):  # for short_args
#             arg = arg[1:]
#             if arg.startswith("-"):  # for long_args
#                 arg = arg[1:]
#                 is_long_arg = True
#
#             # skip the argument if getopt arguments are setable
#             if not is_long_arg:
#                 for i, s_arg in enumerate(short_opts):
#                     if s_arg == arg and i < len(short_opts) - 1 and short_opts[
#                         i + 1] == ":":
#                         index += 1
#                         break
#             else:
#                 for l_arg in long_opts:
#                     if l_arg.startswith(arg) and l_arg.endswith("="):
#                         index += 1
#                         break
#
#             # if arg + ":" in short_opts:
#             #     index += 1
#             # elif arg + "=" in long_opts:
#             #     index += 1
#         else:
#             generate_files.append(arg)
#             new_args.remove(arg)
#
#         index += 1
#
#     # change files, if found
#     if len(generate_files) > 0:
#         global files
#         files = generate_files
#
#     # return valid arguments
#     return new_args


# def manage_prioritized_params(args: list):
#     if type(args) != list:
#         raise_value_error("Expecting a list as argument")
#
#     global option_shorts, option_longs
#     global no_config, generate_log_file, verbose_mode
#
#     # if for whatever reasons files were given
#     better_args = remove_files_from_args(args, option_shorts, option_longs)
#
#     try:
#         opts, args = getopt.getopt(better_args, option_shorts, option_longs)
#
#         for opt, arg in opts:
#
#             # system exit params
#             if opt in ["-h", "--help"]:
#                 print(small_help())
#                 sys.exit(0)
#             if opt.__eq__("--version"):
#                 print("Version: v1.1.0")
#                 sys.exit(0)
#             if opt.__eq__("--print-event-table"):
#                 print(get_event_information())
#                 sys.exit(0)
#
#             # prioritized management params,
#             if opt in ["-v", "--verbose"]:
#                 verbose_mode = True
#             elif opt.__eq__("--no-config"):
#                 no_config = True
#             elif opt.__eq__("--generate-log-file"):
#                 generate_log_file = True
#
#     # print error messages
#     except Exception as err:
#         rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
#         rprint(
#             "[dark_olive_green1]Use '--help' or 'man htcompact' for help [/dark_olive_green1]")
#         sys.exit(1)


# def manage_params(args: list):
#     """
#     Interprets the given command line arguments and changes the global variables in this scrips
#
#     """
#     global files  # list of files and directories
#     global mode  # what mode are we on
#     global std_log, std_err, std_out  # all default values for the HTCondor files
#     global show_list  # show more
#     global ignore_list  # ignore information variables
#     global to_csv
#     global filter_mode, filter_keywords, filter_extended  # search features
#     global table_format  # table_format can be changed
#     global reverse_dns_lookup  # if set host ip's will be looked up in dns server
#
#     global option_shorts, option_longs
#
#     summarizer_mode = False
#     analyser_mode = False
#
#     all_args = args
#
#     # listen to stdin and add these files
#     if reading_stdin:  # else the script will be waiting fro stdin
#         logging.debug("Listening to arguments from stdin")
#         for line in stdin_input:
#             all_args.append(line.rstrip('\n'))
#
#     better_args = remove_files_from_args(all_args, option_shorts, option_longs)
#
#     try:
#         opts, args = getopt.getopt(better_args, option_shorts, option_longs)
#
#         for opt, arg in opts:
#
#             # catch unusual but not wrong parameters starting with -
#             if arg.startswith("-"):
#                 rprint("[yellow]The argument for {0} is {1}, "
#                        "is that wanted?[/yellow]".format(opt, arg))
#                 logging.warning("The argument for {0} is {1}, "
#                                 "is that wanted?".format(opt, arg))
#
#             elif opt in ["-m", "--mode"]:
#
#                 if arg in allowed_modes.keys():
#                     mode = allowed_modes.get(arg)
#                 elif arg in allowed_modes.values():
#                     mode = arg
#                 else:
#                     err_msg = "Invalid argument '" + arg + \
#                               "'\nValid arguments:\n" + \
#                               ", ".join("'{}' or '{}'".format(key, value)
#                                         for key, value
#                                         in allowed_modes.items())
#                     raise_value_error(err_msg)
#
#                 logging.debug("Starting the: " + mode + " mode")
#
#             elif opt.__eq__("-s"):
#                 summarizer_mode = True
#             elif opt.__eq__("-a"):
#                 analyser_mode = True
#             elif opt in ["--filter"]:
#                 filter_mode = True
#                 filter_keywords = " ".join(re.split(',| ', arg)).split()
#             elif opt in ["--extend"]:
#                 filter_extended = True
#             elif opt.__eq__("--std-log"):
#                 # if std_log is empty,
#                 # every given file will be interpreted as log file
#                 if arg != "" and arg[0] != '.':
#                     arg = "." + arg  # add point if missing
#                 std_log = arg
#             elif opt.__eq__("--std-err"):
#                 if arg == "":
#                     raise_value_error("Empty string only allowed for std-log")
#                 if arg[0] != '.':
#                     arg = "." + arg  # add point if missing
#                 std_err = arg
#             elif opt.__eq__("--std-out"):
#                 if arg == "":
#                     raise_value_error("Empty string only allowed for std-log")
#                 if arg[0] != '.':
#                     arg = "." + arg  # add point if missing
#                 std_out = arg
#
#             # show more specific information
#             elif opt.__eq__("--show-more"):
#                 keywords = " ".join(re.split(',| ', arg)).split()
#                 for keyword in keywords:
#                     if keyword in allowed_show_values:
#                         show_list.append(keyword)
#                     else:
#                         raise_value_error(
#                             "Invalid argument '"
#                             + keyword
#                             + "'\nValid arguments:\n"
#                             + ", ".join(allowed_show_values))
#
#                 logging.debug("Show these information: "
#                               + ", ".join(show_list))
#
#             # all variables to ignore unwanted information
#             elif opt.__eq__("--ignore"):
#                 keywords = " ".join(re.split(',| ', arg)).split()
#                 for keyword in keywords:
#                     if keyword in allowed_ignore_values:
#                         ignore_list.append(keyword)
#                     else:
#                         raise_value_error(
#                             "Invalid argument '"
#                             + keyword
#                             + "'\nValid arguments:\n"
#                             + ", ".join(allowed_ignore_values))
#
#                 logging.debug("Ignore these information: "
#                               + ", ".join(ignore_list))
#
#             elif opt.__eq__("--to-csv"):
#                 to_csv = True
#                 rprint("[red]--to-csv not handled yet[/red]")
#                 sys.exit(0)
#
#             elif opt.__eq__("--table-format"):
#                 # types = "plain,simple,github,grid,fancy_grid,pipe," \
#                 #         "orgtbl,rst,mediawiki,html,latex,latex_raw," \
#                 #         "latex_booktabs,tsv,pretty"
#                 # only valid arguments
#                 if arg in valid_table_formats:
#                     table_format = arg
#                 else:
#                     logging.debug("The given table format doesn't exist")
#
#             elif opt.__eq__("--reverse-dns-lookup"):
#                 reverse_dns_lookup = True
#
#             # these are already managed in manage_prioritized_params
#             # need to be caught, cause of exception cases
#             elif opt.__eq__("--no-config"):
#                 pass
#             elif opt.__eq__("--generate-log-file"):
#                 pass
#             elif opt in ["-v", "--verbose"]:
#                 pass
#
#             else:
#                 rprint("[red]Option not handled yet[/red]")
#                 sys.exit(0)
#     # print error messages
#     except Exception as err:
#         logging.exception(
#             err)  # write a detailed description in the stdout.log file
#         rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
#         rprint("[dark_olive_green1]"
#                "Use '--help' or 'man htcompact' for help"
#                "[/dark_olive_green1]")
#         sys.exit(1)
#
#     if summarizer_mode and analyser_mode:
#         mode = "analysed-summary"
#     elif summarizer_mode:
#         mode = "summarize"
#     elif analyser_mode:
#         mode = "analyse"
#
#     if len(files) == 0:
#         logging.debug("No files given")
#         rprint("[red]No files given[/red]")
#         sys.exit(2)


# def small_help() -> str:
#     """
# Usage:
#   htcompact [file ...] [-hsav] [-m mode] [--help] [--version] [--verbose]
#             [--mode mode] [--std-log suffix] [--std-out suffix]
#             [--std-err suffix] [--ignore keywords] [--show-more keywords]
#             [--no-config] [--to-csv] [--generate-log-file]
#             [--filter keywords [--extend]] [--print-event-table]
#             [--reverse-dns-lookup] [--table-format tablefmt]
#
# ----------------------------Main features:---------------------------------
#
#  --mode summary
#          summarizes all given HTCondor log files and return a result
#          regarding the averange usages and runtimes, this only makes
#          sense, if multiple log files are given
#
#  --mode analyse
#         analyses a specific file, for occurred errors, ram history ->
#         histogram, execution status, runtime and much more
#
#  --mode analysed-summary
#         A combination of both modes, which gives a more detailed overview
#
#  More detailed descriptions and help on other options with:
#  'man htcompact'
#     """
#     # returns this docstring
#     return small_help.__doc__


def get_event_information(event_id="") -> str:
    events = [
        {
            "Event Number": "000",
            "Event Name": "Job submitted",
            "Event Description": "This event occurs when a user submits a job."
                                 " It is the first event you will see for a job, and it should only occur once.",
        },
        {
            "Event Number": "001",
            "Event Name": "Job executing",
            "Event Description": "This shows up when a job is running. It might occur more than once.",
        },
        {
            "Event Number": "002",
            "Event Name": "Error in executable",
            "Event Description": "The job could not be run because the executable was bad.",
        },
        {
            "Event Number": "003",
            "Event Name": "Job was checkpointed",
            "Event Description": "The job's complete state was written to a checkpoint file."
                                 " This might happen without the job being removed from a machine,"
                                 " because the checkpointing can happen periodically.",
        },
        {
            "Event Number": "004",
            "Event Name": "Job evicted from machine",
            "Event Description": "A job was removed from a machine before it finished,"
                                 " usually for a policy reason."
                                 " Perhaps an interactive user has claimed the computer,"
                                 " or perhaps another job is higher priority.",
        },
        {
            "Event Number": "005",
            "Event Name": "Job terminated",
            "Event Description": "The job has completed.",
        },
        {
            "Event Number": "006",
            "Event Name": "Image size of job updated",
            "Event Description": "An informational event,"
                                 " to update the amount of memory that the job is using while running."
                                 " It does not reflect the state of the job.",
        },
        {
            "Event Number": "007",
            "Event Name": "Shadow exception",
            "Event Description": "The condor_shadow, a program on the submit computer"
                                 " that watches over the job and performs some services for the job,"
                                 " failed for some catastrophic reason."
                                 " The job will leave the machine and go back into the queue.",
        },
        {
            "Event Number": "008",
            "Event Name": "Generic log event",
            "Event Description": "Not used.",
        },
        {
            "Event Number": "009",
            "Event Name": "Job aborted",
            "Event Description": "The user canceled the job.",
        },
        {
            "Event Number": "010",
            "Event Name": "Job was suspended",
            "Event Description": "The job is still on the computer,"
                                 " but it is no longer executing. This is usually for a policy reason,"
                                 " such as an interactive user using the computer.",
        },
        {
            "Event Number": "011",
            "Event Name": "Job was unsuspended",
            "Event Description": "The job has resumed execution, after being suspended earlier.",
        },
        {
            "Event Number": "012",
            "Event Name": "Job was held",
            "Event Description": "The job has transitioned to the hold state."
                                 " This might happen if the user applies the condor_hold command to the job.",
        },
        {
            "Event Number": "013",
            "Event Name": "Job was released",
            "Event Description": "The job was in the hold state and is to be re-run.",
        },
        {
            "Event Number": "014",
            "Event Name": "Parallel node executed",
            "Event Description": "A parallel universe program is running on a node.",
        },
        {
            "Event Number": "015",
            "Event Name": "Parallel node terminated",
            "Event Description": "A parallel universe program has completed on a node.",
        },
        {
            "Event Number": "016",
            "Event Name": "POST script terminated",
            "Event Description": "A node in a DAGMan work flow has a script that should be run after a job."
                                 " The script is run on the submit host."
                                 " This event signals that the post script has completed.",
        },
        {
            "Event Number": "017",
            "Event Name": "Job submitted to Globus",
            "Event Description": "A grid job has been delegated to Globus (version 2, 3, or 4)."
                                 " This event is no longer used.",
        },
        {
            "Event Number": "018",
            "Event Name": "Globus submit failed",
            "Event Description": "The attempt to delegate a job to Globus failed.",
        },
        {
            "Event Number": "019",
            "Event Name": "Globus resource up",
            "Event Description": "The Globus resource that a job wants to run on was unavailable,"
                                 " but is now available. This event is no longer used.",
        },
        {
            "Event Number": "020",
            "Event Name": "Detected Down Globus Resource",
            "Event Description": "The Globus resource that a job wants to run on has become unavailable."
                                 " This event is no longer used.",
        },
        {
            "Event Number": "021",
            "Event Name": "Remote error",
            "Event Description": "The condor_starter (which monitors the job on the execution machine) has failed.",
        },
        {
            "Event Number": "022",
            "Event Name": "Remote system call socket lost",
            "Event Description": "The condor_shadow and condor_starter "
                                 "(which communicate while the job runs) have lost contact.",
        },
        {
            "Event Number": "023",
            "Event Name": "Remote system call socket reestablished",
            "Event Description": "The condor_shadow and condor_starter "
                                 "(which communicate while the job runs)"
                                 " have been able to resume contact before the job lease expired.",
        },
        {
            "Event Number": "024",
            "Event Name": "Remote system call reconnect failure",
            "Event Description": "The condor_shadow and condor_starter (which communicate while the job runs)"
                                 " were unable to resume contact before the job lease expired.",
        },
        {
            "Event Number": "025",
            "Event Name": "Grid Resource Back Up",
            "Event Description": "A grid resource that was previously unavailable is now available.",
        },
        {
            "Event Number": "026",
            "Event Name": "Detected Down Grid Resource",
            "Event Description": "The grid resource that a job is to run on is unavailable.",
        },
        {
            "Event Number": "027",
            "Event Name": "Job submitted to grid resource",
            "Event Description": "A job has been submitted, and is under the auspices of the grid resource.",
        },
        {
            "Event Number": "028",
            "Event Name": "Job ad information event triggered.",
            "Event Description": "Extra job ClassAd attributes are noted."
                                 " This event is written as a supplement to other events"
                                 " when the configuration parameter"
                                 " EVENT_LOG_JOB_AD_INFORMATION_ATTRS is set."
        },
        {
            "Event Number": "029",
            "Event Name": "The job's remote status is unknown",
            "Event Description": "No updates of the job's remote status have been received for 15 minutes.",
        },
        {
            "Event Number": "030",
            "Event Name": "The job's remote status is known again",
            "Event Description": "An update has been received for a job"
                                 " whose remote status was previous logged as unknown.",
        },
        {
            "Event Number": "031",
            "Event Name": "Job stage in",
            "Event Description": "A grid universe job is doing the stage in of input files.",
        },
        {
            "Event Number": "032",
            "Event Name": "Job stage out",
            "Event Description": "A grid universe job is doing the stage out of output files."
        },
        {
            "Event Number": "033",
            "Event Name": "Job ClassAd attribute update",
            "Event Description": "A Job ClassAd attribute is changed"
                                 " due to action by the condor_schedd daemon. "
                                 "This includes changes by condor_prio."
        },
        {
            "Event Number": "034",
            "Event Name": "Pre Skip event",
            "Event Description": "For DAGMan, this event is logged"
                                 " if a PRE SCRIPT exits with the defined PRE_SKIP value"
                                 " in the DAG input file. This makes it possible for DAGMan"
                                 " to do recovery in a workflow that has such an event,"
                                 " as it would otherwise not have any event for the DAGMan node"
                                 " to which the script belongs, and in recovery,"
                                 " DAGMan's internal tables would become corrupted."
        }
    ]

    out_str = ""

    # print the whole table
    if event_id == "":
        for event in events:
            for key, value in event.items():
                out_str += str(key) + ": " + str(value) + "\n"
            out_str += "\n"
        return out_str

    # print a single event
    event_id = int(event_id)
    if 0 <= event_id <= 34:
        for key, value in events[event_id].items():
            out_str += str(key) + ": " + str(value)
        return out_str
    else:
        return "This event number does not exist."


def raise_value_error(message: str) -> ValueError:
    raise ValueError(message)


def raise_type_error(message: str) -> TypeError:
    raise TypeError(message)


# Todo: time differences over end of year
def gen_time_dict(
        submission_date: date_time,
        execution_date: date_time = None,
        termination_date: date_time = None
) -> (timedelta, timedelta, timedelta):
    """
    Takes in three dates, at least one must be given,
    return the datetime.timedelta objects,
    Depending on the given arguments,
    this function will try to find out the time differences between the events.
    If not all three values are given, it will return None

    Example:
    Submissio

    :param submission_date: Job Sumbission date from the user
    :param execution_date: The date when the job actually started executing
    :param termination_date: The date when the job was finished
    :return: (waiting_time, runtime, total_time)

    """
    waiting_time = None
    runtime = None
    total_time = None
    today = datetime.datetime.now()
    today = today.replace(microsecond=0)  # remove unnecessary microseconds

    time_desc = list()
    time_vals = list()
    running_over_neyear = False

    # calculate the time difference to last year,
    # if the date is higher that today of running jobs
    # this means the execution started before newyear
    if termination_date is None:
        if submission_date and submission_date > today:
            running_over_neyear = True
            submission_date = submission_date.replace(
                year=submission_date.year - 1)
        if execution_date and execution_date > today:
            running_over_neyear = True
            execution_date = execution_date.replace(
                year=execution_date.year - 1)

    if execution_date and submission_date:
        execution_date = execution_date
        # new year ?
        if submission_date > execution_date:
            running_over_neyear = True
            submission_date = submission_date.replace(
                year=submission_date.year - 1)
        waiting_time = execution_date - submission_date
    if termination_date:
        if waiting_time:
            pass
        if execution_date:
            # new year ?
            if execution_date > termination_date:
                running_over_neyear = True
                execution_date = execution_date.replace(
                    year=execution_date.year - 1)
            runtime = termination_date - execution_date
        if submission_date:
            # new year ?
            if submission_date > termination_date:
                running_over_neyear = True
                submission_date = submission_date.replace(
                    year=submission_date.year - 1)
            total_time = termination_date - submission_date
    # Process still running
    elif waiting_time:
        runtime = today - execution_date
    # Still waiting for execution
    elif submission_date:
        waiting_time = today - submission_date

    # now after collecting all available values try to produce a dict
    # if new year was hitted by one of them, show the year as well
    if running_over_neyear:
        if submission_date:
            time_desc.append("Submission date")
            time_vals.append(submission_date)
        if execution_date:
            time_desc.append("Execution date")
            time_vals.append(execution_date)
        if termination_date:
            time_desc.append("Termination date")
            time_vals.append(termination_date)
    else:
        if submission_date:
            time_desc.append("Submission date")
            time_vals.append(submission_date.strftime("%m/%d %H:%M:%S"))
        if execution_date:
            time_desc.append("Execution date")
            time_vals.append(execution_date.strftime("%m/%d %H:%M:%S"))
        if termination_date:
            time_desc.append("Termination date")
            time_vals.append(termination_date.strftime("%m/%d %H:%M:%S"))

    if waiting_time:
        time_desc.append("Waiting time")
        time_vals.append(waiting_time)
    if runtime:
        time_desc.append("Execution runtime")
        time_vals.append(runtime)
    if total_time:
        time_desc.append("Total runtime")
        time_vals.append(total_time)

    time_dict = {
        "Dates and times": time_desc,
        "Values": time_vals
    }

    return time_dict


def log_to_dict(
        file: str,
        sec: int = 0,
        reverse_dns_lookup=False) -> (dict, dict, dict, dict, dict):
    """
    Read the log file with the htcondor module.
    Return five dicts holding information about:
    execution node, used resources, times, used ram history, errors .

    :type file: str
    :param file: HTCondor log file
    :param sec: seconds to wait for new events
    :return: job_dict, res_dict, time_dict, ram_history, errors

    Consider that the return values can be None or empty dictionarys
    """
    job_events = list()
    res_dict = dict()
    time_dict = {
        "Submission date": None,
        "Execution date": None,
        "Termination date": None
    }
    ram_history = list()
    occurred_errors = list()

    has_terminated = False
    invalid_file = False

    try:
        jel = htcondor.JobEventLog(file)
        # Read all currently-available events
        # waiting for 'sec' seconds for the next event.
        for event in jel.events(sec):
            event_type_number = event.get('EventTypeNumber')
            # convert time to datetime object
            date = datetime.datetime.strptime(event.get('EventTime'),
                                              "%Y-%m-%dT%H:%M:%S")
            # update submit date, submission host
            if event.type == jet.SUBMIT:
                time_dict["Submission date"] = date

                match_from_host = re.match(r"<(.+):[0-9]+\?(.*)>",
                                           event.get('SubmitHost'))
                if match_from_host:
                    submitted_host = match_from_host[1]
                    job_events.append(('Submitted from', submitted_host))
                # ERROR
                else:
                    invalid_file = True
                    reason = "Can't read user address"
                    occurred_errors.append(
                        [event_type_number, "Now", "invalid user address",
                         reason])
                    job_events.append(('Submitted from', "invalid user"))
                    raise_value_error(
                        "Submission Host is wrong in file: " + file)

            # update execution date, execution node
            if event.type == jet.EXECUTE:
                time_dict["Execution date"] = date

                match_to_host = re.match(r"<(.+):[0-9]+\?(.*)>",
                                         event.get('ExecuteHost'))
                if match_to_host:
                    execution_host = match_to_host[1]
                    if reverse_dns_lookup:  # resolve ip to dns if set
                        execution_host = gethostbyaddr(execution_host)

                    job_events.append(('Executing on', execution_host))
                # ERROR
                else:
                    invalid_file = True
                    reason = "Can't read host address"
                    occurred_errors.append(
                        [event_type_number, "Now", "invalid host address",
                         reason])
                    job_events.append(('Executing on', "invalid host"))
                    raise_value_error(
                        "Execution host address is wrong in file: " + file)

            # update ram history dict
            if event.type == jet.IMAGE_SIZE:
                size_update = event.get('Size')
                memory_usage = event.get('MemoryUsage')
                resident_set_size = event.get('ResidentSetSize')
                ram_history.append((date, size_update, memory_usage,
                                    resident_set_size))

            # update resource dict and termination date
            if event.type == jet.JOB_TERMINATED:
                has_terminated = True
                time_dict["Termination date"] = date

                # get all resources, replace by np.nan if value is None
                cpu_usage = event.get('CpusUsage') if event.get(
                    'CpusUsage') is not None else np.nan
                cpu_requested = event.get('RequestCpus') if event.get(
                    'RequestCpus') is not None else np.nan
                cpu_allocated = event.get('Cpus') if event.get(
                    'Cpus') is not None else np.nan
                disk_usage = event.get('DiskUsage') if event.get(
                    'DiskUsage') is not None else np.nan
                disk_requested = event.get('RequestDisk') if event.get(
                    'RequestDisk') is not None else np.nan
                disk_allocated = event.get("Disk") if event.get(
                    'Disk') is not None else np.nan
                memory_usage = event.get('MemoryUsage') if event.get(
                    'MemoryUSage') is not None else np.nan
                memory_requested = event.get('RequestMemory') if event.get(
                    'RequestMemory') is not None else np.nan
                memory_allocated = event.get('Memory') if event.get(
                    'Memory') is not None else np.nan

                # put the data in the dict
                res_dict = {
                    "Resources": ["Cpu", "Disk", "Memory"],
                    "Usage": np.array([cpu_usage, disk_usage, memory_usage],
                                      dtype=float),
                    "Requested": np.array(
                        [cpu_requested, disk_requested, memory_requested],
                        dtype=float),
                    "Allocated": np.array(
                        [cpu_allocated, disk_allocated, memory_allocated],
                        dtype=float)
                }
                normal_termination = event.get('TerminatedNormally')
                # differentiate between normal and abnormal termination
                if normal_termination:
                    job_events.insert(0, ("Termination State",
                                          colors['green'] + "Normal" + colors[
                                              'back_to_default']))
                    return_value = event.get('ReturnValue')
                    job_events.append(("Return Value", return_value))
                else:
                    job_events.insert(0, ("Termination State",
                                          colors['red'] + "Abnormal" + colors[
                                              'back_to_default']))
                    signal = event.get('TerminatedBySignal')
                    job_events.append(("Terminated by Signal", signal))

            # update error dict and termination date
            if event.type == jet.JOB_ABORTED:
                has_terminated = True
                time_dict["Termination date"] = date

                reason = event.get('Reason')
                occurred_errors.append(
                    [event_type_number, date.strftime("%m/%d %H:%M:%S"),
                     "Aborted", reason])
                job_events.insert(0, ("Process was",
                                      colors['red'] + "Aborted" + colors[
                                          'back_to_default']))

            # update error dict
            if event.type == jet.JOB_HELD:
                reason = event.get('HoldReason')
                occurred_errors.append(
                    [event_type_number, date.strftime("%m/%d %H:%M:%S"),
                     "JOB_HELD", reason])

            # update error dict
            if event.type == jet.SHADOW_EXCEPTION:
                reason = event.get('Message')
                occurred_errors.append((event_type_number,
                                        date.strftime("%m/%d %H:%M:%S"),
                                        "SHADOW_EXCEPTION", reason))
        else:
            # End of the file
            pass

    except OSError as err:
        invalid_file = True
        if err.args[0] == "ULOG_RD_ERROR":
            rprint(f"[red]{err}: Not a valid htcondor logfile: {file}[/red]")
            reason = "Error while reading log file. " \
                     "File was manipulated or contains gpu usage."
            occurred_errors.append(["None", "Now", "ULOG_RD_ERROR", reason])
        else:
            rprint(f"[red]Not able to open the file: {file}[/red]")
    except ValueError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    except TypeError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")

    # generate a better time dict
    time_values = list(time_dict.values())
    better_time_dict = gen_time_dict(*time_values)

    # Job still running and file valid
    if not invalid_file and not has_terminated:
        if "Total runtime" in better_time_dict["Dates and times"]:
            rprint(
                "[red]This is not supposed to happen, check your code[/red]")
        elif "Execution runtime" in better_time_dict["Dates and times"]:
            state = "Executing"
        elif "Waiting time" in better_time_dict["Dates and times"]:
            state = "Waiting"
        else:
            state = "Unknown"
        job_events.insert(0, ("Process is", colors["blue"]
                              + state + colors["back_to_default"]))
    # file not fully readable
    elif invalid_file:
        better_time_dict = dict()  # times have no meaning here
        job_events.insert(0, ("Error", colors["red"]
                              + "Error while reading"
                              + colors["back_to_default"]))

    job_events_dict = dict()
    error_dict = dict()
    ram_history_dict = dict()
    # convert job_events to a nice and simple dictionary
    if len(job_events) > 0:
        desc, val = zip(*job_events)
        job_events_dict = {
            "Execution details": desc,
            "Values": val
        }

    # convert errors into a dictionary
    if len(occurred_errors) > 0:
        event_numbers, time_list, errors, reasons = zip(*occurred_errors)
        reasons = list(reasons)
        for i in range(len(reasons)):
            tmp_reason = reasons[i]
            if len(tmp_reason) > 50:
                split = int(len(tmp_reason) / 2)
                while tmp_reason[split] != ' ' and split < len(tmp_reason) - 1:
                    split += 1
                # if no space was found, change back to normal
                if split > len(tmp_reason) - 2:
                    split = int(len(tmp_reason) / 2)
                reasons[i] = tmp_reason[0:split] + '\n' + tmp_reason[split:]
        error_dict = {
            "Event Number": list(event_numbers),
            "Time": list(time_list),
            "Error": list(errors),
            "Reason": list(reasons)
        }
    # convert ram_history to a dictionary
    if len(ram_history) > 0:
        time_list, img_size, mem_usage, res_set_size = zip(*ram_history)
        ram_history_dict = {
            "Dates": list(time_list),
            "Image size updates": list(img_size),
            "Memory usages": list(mem_usage),
            "Resident Set Sizes": list(res_set_size)
        }

    return job_events_dict, res_dict, better_time_dict, \
           ram_history_dict, error_dict


# just read .err files content and return it as a string
def read_condor_error(file: str) -> str:
    """
    Reads a HTCondor .err file and returns its content

    :param file: a HTCondor .err file

    :raises: :class:'NameError': The param file was no .err file
             :class:'FileNotFoundError': if open does not work

    :return: file content

    """
    if not file.endswith(std_err):
        raise NameError(
            "The read_condor_error method is only for " + std_err + " files")

    try:
        err_file = open(file)
    except FileNotFoundError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    else:
        return "".join(err_file.readlines())


# just read .out files content and return it as a string
def read_condor_output(file: str) -> str:
    """
        Reads a HTCondor .out file and returns its content

        :param file: a HTCondor .out file

        :raises: :class:'NameError': The param file was no .out file
                 :class:'FileNotFoundError': if open does not work

        :return: file content

        """
    if not file.endswith(std_out):
        raise NameError(
            "The read_condor_output method is only for " + std_out + " files")
    try:
        out_file = open(file)
    except FileNotFoundError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    else:
        return "".join(out_file.readlines())


def gethostbyaddr(ip):
    """
        this function is supposed to filter a given ip for it's representative domain name like google.com
        :return: resolved domain name, else give back the ip
    """
    try:
        if ip in list(store_dns_lookups.keys()):
            return store_dns_lookups[ip]
        # else lookup
        reversed_dns = socket.gethostbyaddr(ip)
        logging.debug(
            'Lookup successful ' + ip + ' resolved as: ' + reversed_dns[0])
        # store
        store_dns_lookups[ip] = reversed_dns[0]
        # return
        return reversed_dns[0]
    except Exception:
        logging.debug('Not able to resolve the IP: ' + ip)
        # also store
        store_dns_lookups[ip] = ip
        return ip


def manage_thresholds(resources: dict,
                      tolerated_usage_threshold=0.1,
                      bad_usage_threshold=0.25) -> dict:
    """

        The important part is that the keywords "Usage", "Requested" exists
        and that at least 3 values are given: cpu, disk, memory

    :param resources:
    :return:
    """

    resources.update(Usage=list(
        resources["Usage"]))  # change to list, to avoid numpy type errors
    for i in range(len(resources['Resources'])):
        # thresholds used vs. requested
        if float(resources['Requested'][i]) != 0:

            deviation = float(resources['Usage'][i]) / float(
                resources['Requested'][i])

            # color red if more than bad_usage_thresholds % away from the requested value
            if deviation >= 1 + bad_usage_threshold or deviation <= 1 - bad_usage_threshold:
                resources['Usage'][i] = colors['red'] + str(
                    resources['Usage'][i]) + colors['back_to_default']

            # color yellow if more than low_usage_threhold % away from requested value
            elif deviation >= 1 + tolerated_usage_threshold or deviation <= 1 - tolerated_usage_threshold:
                resources['Usage'][i] = colors['yellow'] + str(
                    resources['Usage'][i]) + colors['back_to_default']
            # else it's okay, color green
            else:
                resources['Usage'][i] = colors['green'] + str(
                    resources['Usage'][i]) + colors['back_to_default']

    return resources


def show_htcondor_stderr(file: str) -> str:
    """

    :param file: HTCondor .err file
    :return: filtered content
    """

    # accept files without the std_err suffix
    if std_err.__ne__("") and file[-len(std_err):].__eq__(std_err):
        job_spec_id = file[:-len(std_err)]
    elif std_err.__ne__(""):
        job_spec_id = file
    else:
        raise_value_error("Missing specification for the std_err")

    output_string = ""
    try:

        error_content = read_condor_error(job_spec_id + std_err)
        for line in error_content.split("\n"):
            if 'std-err' in show_list:
                if "err" in line.lower():
                    output_string += colors['red'] + line + colors[
                        'back_to_default'] + "\n"
                elif "warn" in line.lower():
                    output_string += colors['yellow'] + line + colors[
                        'back_to_default'] + "\n"

    except NameError as err:
        logging.exception(err)
        rprint(
            "[red]The smart_output_error method requires a " + std_err + " file as parameter[/red]")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)" + std_err, relevant[1])
        rprint(
            f"[yellow]There is no related {std_err} file: {relevant[1]} in the directory:\n[/yellow]"
            f"[cyan]'{os.path.abspath(relevant[0])}'\n with the prefix: {match[1]}[/cyan]")
    except TypeError as err:
        logging.exception(err)
        # we continue here no exit needed
    finally:
        return output_string


def show_htcondor_stdout(file: str) -> str:
    """
    :param: HTCondor .out file
    :return: content
    """

    # accept files without the std_out suffix
    if std_out.__ne__("") and file[-len(std_out):].__eq__(std_out):
        job_spec_id = file[:-len(std_out)]
    elif std_out.__ne__(""):
        job_spec_id = file
    else:
        raise_value_error("Missing specification for the std_out file")

    output_string = ""
    try:

        output_content = read_condor_output(job_spec_id + std_out)
        output_string += output_content
    except NameError as err:
        logging.exception(err)
        rprint(
            "[red]The smart_output_output method requires a " + std_out + " file as parameter[/red]")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)" + std_out, relevant[1])
        rprint(
            f"[yellow]There is no related {std_out} file: {relevant[1]} in the directory:\n[/yellow]"
            f"[cyan]'{os.path.abspath(relevant[0])}'\n with the prefix: {match[1]}[/cyan]")
    except TypeError as err:
        logging.exception(err)
        # we continue here no exit needed
    finally:

        return output_string


def customize_log(file: str, std_log="", show_list=[]) -> dict:
    """
    reads a given HTCondor .log file with the log_to_dict() function,
    adds .err and .out files content, if specified

    :param file:    a HTCondor .log file
    :param std_log: standard log file, usually .log
    :param show_list: show std-err and std-out files content if present

    :return:        dict

    """
    # accept files without the std_log suffix
    if std_log.__ne__("") and file[-len(std_log):].__eq__(std_log):
        job_spec_id = file[:-len(std_log)]
    else:
        job_spec_id = os.path.splitext(file)[0]

    result_dict = dict()

    try:

        htcondor_log = log_to_dict(file)

        job_dict = htcondor_log[0]
        res_dict = htcondor_log[1]
        times = htcondor_log[2]

        result_dict[
            "file-description"] = f"{colors['green']}The job procedure of : {file}{colors['back_to_default']}"

        result_dict["execution-details"] = job_dict

        result_dict["times"] = times
        if not len(res_dict) == 0:  # make sure res_df is not None

            res_dict = manage_thresholds(res_dict)

            result_dict["all-resources"] = res_dict

        if 'std-err' in show_list:
            result_dict['stderr'] = show_htcondor_stderr(job_spec_id + std_err)
        if 'std-out' in show_list:
            result_dict['stdout'] = show_htcondor_stdout(job_spec_id + std_out)

    except NameError as err:
        logging.exception(err)
        rprint(f"[red]The customize logs functions requires a "
               f"{std_log} file as parameter[/red]")
    except ValueError as err:
        logging.exception(err)
        rprint(f"[red]{err}[/red]")
    except TypeError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        sys.exit(3)
    except Exception as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")

    return result_dict


def find_config(args: list) -> (str, list):
    """

        Try to find a config file in the given arguments.

        Go through the hierarchy in the order:
        - current_working_directory/file
        - current_project_folder/config/file
        - ~/.config/{script_name}/{file}"
        - /etc/{file}

        ONE config will be loaded if set: generate_logging_file

        -> else try to find default config file "htcompact.conf"
         in the same order excluding the current working directory,
         because this might lead to misbehaviour,
          when the user changes the working directory.

        :param args: command line arguments
        :return: file if found, else None, args without config, if found
    """

    script_name = sys.argv[0]
    if script_name.endswith(".py"):
        script_name = sys.argv[0][:-3]  # scriptname without .py
    config = configparser.ConfigParser()
    # search for the given file,
    # first directly,
    # then in /etc and then in ~/.config/htcompact/
    found_config = False

    arguments = args
    arguments.append(default_configfile)

    found_config_file = None
    found_default_in_cwd = False
    # run through every given argument and check if it's a config file
    for i, file in enumerate(arguments):
        # try to find the given file in the current directory,
        # in /etc or in ~/.config/htcompact/
        try:
            # check if the file is not empty, readable and acceptable
            if os.path.isfile(file) and os.path.getsize(file) != 0 \
                    and config.read(file):
                # only use the file if given,
                # means the default config should be ignored,
                # even if it's in cwd,
                # this will exclude misbehaviour, when cwd is changed
                if not (file.__eq__(default_configfile) and i == len(
                        arguments) - 1):
                    found_config = True
                    arguments.remove(file)
                else:
                    found_default_in_cwd = True

                found_config_file = file

            # try to find the config file in the current environment hierarchy
            elif os.path.isfile(f"{sys.prefix}/config/{file}") \
                    and config.read(f"{sys.prefix}/config/{file}"):
                found_config = True
                arguments.remove(file)
                # remember file
                found_config_file = f"{sys.prefix}/config/{file}"

            # try to find config file in ~/.config/script_name/
            elif os.path.isfile(f"~/.config/{script_name}/{file}") \
                    and config.read(f"~/.config/{script_name}/{file}"):
                found_config = True
                arguments.remove(file)
                # remember file
                found_config_file = f"~/.config/{script_name}/{file}"

            # try to find config file in /etc
            elif os.path.isfile(f"/etc/{file}") \
                    and config.read(f"/etc/{file}"):
                found_config = True
                arguments.remove(file)
                # remember file
                found_config_file = f"/etc/{file}"

            else:
                logging.debug(f"{file} not found or not a valid config file")

            # do not search any longer if found
            if found_config:
                break

        # file has no readable format for the configparser,
        # probably because it's not a config file
        except configparser.MissingSectionHeaderError:
            continue
        except configparser.DuplicateOptionError as err:
            rprint(f"[red]{err}[/red]")

    try:
        arguments.remove(
            default_configfile)  # remove again, if other config file was found
    except ValueError:
        pass

    # prepare output if not and break if not found
    if not found_config and not found_default_in_cwd:
        rprint(f"[yellow]No valid config file found in: \n"
               f"- {os.getcwd()},\n"
               f"- {sys.prefix}/config,\n"
               f"- ~/.config/htcompact/,\n"
               f"- /etc/ \n"
               f"-> using default settings[/yellow]")
        return None, args
    elif found_default_in_cwd:
        rprint(
            f"[yellow]Found default {default_configfile} "
            f"in the current working directory.\n"
            f"It will be ignore, to use it, pass it as an argument like:\n"
            f"htcompact {default_configfile} [logs] [options]\n"
            f"-> using default settings[/yellow]")
        return None, args

    config.read(found_config_file)
    sections = config.sections()
    # check if logging is tuned on
    if 'features' in sections:
        if 'generate_log_file' in config['features']:
            global generate_log_file
            if not generate_log_file:  # if already set, do NOT overwrite
                generate_log_file = \
                    config['features']['generate_log_file'].lower()\
                    in accepted_states
                logging.getLogger().disabled = not generate_log_file

    return found_config_file, arguments


# search for config file ( UNIX BASED )
def load_config(config_file: str):
    config = configparser.ConfigParser()

    try:

        config.read(config_file)
        # all sections in the config file
        sections = config.sections()

        # now try filter the config file for all available parameters

        rprint(f"[blue]Load config from file: {config_file}[/blue]")

        # all documents like files, etc.
        if 'documents' in sections:
            global files

            if 'files' in config['documents']:
                files = config['documents']['files'].split(" ")
                logging.debug(f"Changed document files to {files}")

        # all formats like the table format
        if 'formats' in sections:
            global table_format
            if 'table_format' in config['formats']:
                table_format = config['formats']['table_format']
                logging.debug(
                    f"Changed default table_format to: {table_format}")

        # all basic HTCondor file endings like .log, .err, .out
        if 'htc-files' in sections:
            global std_log, std_err, std_out

            if 'stdlog' in config['htc-files']:
                std_log = config['htc-files']['stdlog']
                logging.debug(
                    f"Changed default for HTCondor .log files to: {std_log}")

            if 'stderr' in config['htc-files']:
                std_err = config['htc-files']['stderr']
                logging.debug(
                    f"Changed default for HTCondor .err files to: {std_err}")

            if 'stdout' in config['htc-files']:
                std_out = config['htc-files']['stdout']
                logging.debug(
                    f"Changed default for HTCondor .out files to: {std_out}")

        # if show variables are set for further output
        if 'show-more' in sections:
            global show_list

            if 'show_list' in config['show-more']:
                for arg in " ".join(re.split(',| ', config['show-more'][
                    'show_list'])).split():
                    if arg in allowed_show_values:
                        show_list.append(arg)
                    else:
                        logging.debug(
                            "Don't know this ignore statement: " + arg)
                logging.debug(
                    f"Changed default ignore statements to: {show_list}")

        # what information should to be ignored
        if 'ignore' in sections:
            global ignore_list  # sources to ignore

            if 'ignore_list' in config['ignore']:
                for arg in " ".join(re.split(',| ', config['ignore'][
                    'ignore_list'])).split():
                    if arg in allowed_ignore_values:
                        ignore_list.append(arg)
                    else:
                        logging.debug("Don't know this show statement: " + arg)
                logging.debug(
                    f"Changed default show-more statements to: {ignore_list}")

        if 'thresholds' in sections:
            global tolerated_usage_threshold, bad_usage_threshold
            if 'tolerated_usage' in config['thresholds']:
                tolerated_usage_threshold = float(
                    config['thresholds']['tolerated_usage'])
                logging.debug(
                    f"Changed default tolerated_usage to: {tolerated_usage_threshold}")
            if 'bad_usage' in config['thresholds']:
                bad_usage_threshold = float(config['thresholds']['bad_usage'])
                logging.debug(
                    f"Changed default bad_usage to: {bad_usage_threshold}")

        if "modes" in sections:
            global filter_mode, mode
            if 'filter_mode' in config['modes']:
                filter_mode = config['modes'][
                                  'filter_mode'].lower() in accepted_states
                logging.debug(f"Changed filter_mode to: {filter_mode}")

            if 'mode' in config['modes']:
                mode_tmp = config['modes']['mode'].lower()
                if mode_tmp in allowed_modes.values():
                    mode = mode_tmp
                elif mode_tmp in allowed_modes.keys():
                    mode = allowed_modes.get(mode_tmp)
                else:
                    mode = None
                logging.debug(f"Changed mode to: {mode}")

        if "filter" in sections:
            global filter_keywords, filter_extended
            if 'filter_keywords' in config['filter']:
                filter_keywords = " ".join(re.split(',| ', config['filter'][
                    'filter_keywords'])).split()
                logging.debug(
                    f"Changed default filter_keywords to: {filter_keywords}")
            if 'filter_extended' in config['filter']:
                filter_extended = config['filter'][
                                      'filter_extended'].lower() in accepted_states
                logging.debug(
                    f"Changed default filter_extended to: {filter_extended}")

        if 'features' in sections:
            global reverse_dns_lookup, to_csv  # extra parameters
            # the first thing to check should be if logging is turend on

            if 'reverse_dns_lookup' in config['features']:
                reverse_dns_lookup = config['features'][
                                         'reverse_dns_lookup'].lower() in accepted_states
                logging.debug(
                    f"Changed default reverse_dns_lookup to: {reverse_dns_lookup}")

            if 'to_csv' in config['features']:
                to_csv = config['features'][
                             'to_csv'].lower() in accepted_states
                logging.debug(f"Changed default to_csv to: {to_csv}")

        return True

    except KeyError as err:
        logging.exception(err)


# in order that plotille has nothing like a int converter,
# I have to set it up manually to show the y - label in number format
def _int_formatter(val, chars, delta, left=False):
    """
    Usage of this is shown here:
    https://github.com/tammoippen/plotille/issues/11

    :param val:
    :param chars:
    :param delta:
    :param left:
    :return:
    """
    align = '<' if left else ''
    return '{:{}{}d}'.format(int(val), align, chars)


def default(log_files: list_of_logs,
            std_log="",
            std_err=".err",
            std_out=".out") -> log_inf_list:
    """
    Print the default output for a given list of log files

    This mode is just an easy view,
     on what the script is actually doing.

    :param log_files:
    :return: list of dicts
    """

    logging.info('Starting the default mode')

    list_of_dicts = list()
    current_path = os.getcwd()
    # go through all given logs
    for file in track(log_files, transient=True, description="Processing..."):

        # for the * operation it should skip std_err and std_out files
        if file.endswith(std_err) and not std_err.__eq__("") or file.endswith(
                std_out) and not std_out.__eq__(""):
            continue

        # else check if file is a valid file
        if os.path.isfile(file) or os.path.isfile(current_path + "/" + file):
            list_of_dicts.append(customize_log(file))

        # if that did not work try std_log at the end of that file
        elif not file.endswith(std_log):
            new_path = file + std_log
            # is this now a valid file ?
            if os.path.isfile(new_path) or os.path.isfile(
                    current_path + "/" + new_path):
                list_of_dicts.append(customize_log(new_path))
            # no file or directory found, even after manipulating the string
            else:
                logging.error(f"No such file with that name or prefix: {file}")
                rprint(
                    f"[red]No such file with that name or prefix: {file}[/red]")

        # The given .log file was not found
        else:
            logging.error(f"No such file: {file}")
            rprint(f"[red]No such file: {file}[/red]")

    if len(list_of_dicts) == 0:
        rprint(
            "[yellow]Nothing found, please use \"man htcompact\" or \"htcompact -h\" for help[/yellow]",
            end="")

    return list_of_dicts


def analyse(log_files: list_of_logs) -> log_inf_list:
    logging.info('Starting the analyser mode')

    if len(log_files) == 0:
        return "No files to analyse"
    # elif len(log_files) > 1 and not redirecting_stdout:
    #     print(
    #         "More than one file is given, this mode is meant to be used for single job analysis.\n"
    #         "This will change nothing, but you should rather do it just for a file one by one")
    #     if not reading_stdin:
    #         x = input("Want to continue (y/n): ")
    #         if x != "y":
    #             rprint('[red]Process stopped[/red]')
    #             sys.exit(0)

    result_list = list()

    for file in track(log_files, transient=True, description="Analysing..."):
        result_dict = dict()

        logging.debug(f"Analysing the HTCondor log file: {file}")
        result_dict[
            "file-description"] = f"{colors['green']}Job analysis of: {file}{colors['back_to_default']}"

        job_dict, res_dict, time_dict, \
        ram_history, occurred_errors = log_to_dict(file)

        if len(job_dict) != 0:
            result_dict["execution-details"] = job_dict

        if len(time_dict) > 0:
            result_dict["times"] = time_dict

        if len(res_dict) > 0:
            result_dict["all-resources"] = manage_thresholds(res_dict)

        # show HTCondor errors
        if len(occurred_errors) > 0:
            result_dict["errors"] = occurred_errors

        # managing the ram history
        if len(ram_history) > 0:
            ram = np.array(ram_history.get('Image size updates'))
            dates = np.array(ram_history.get('Dates'))

            if len(ram) > 1:

                fig = Figure()
                fig.width = 55
                fig.height = 15
                fig.set_x_limits(min_=min(dates))
                min_ram = int(min(ram))  # raises an error if not casted to int
                fig.set_y_limits(min_=min_ram)
                fig.y_label = "Usage"
                fig.x_label = "Time"

                # this will use the self written function _
                # num_formatter, to convert the y-label to int values
                fig.register_label_formatter(float, _int_formatter)
                fig.plot(dates, ram, lc='green', label="Continuous Graph")
                fig.scatter(dates, ram, lc='red', label="Single Values")

                # if redirected, the Legend is useless
                if GlobalServant.redirecting_stdout:
                    result_dict["ram-history"] = fig.show()
                else:
                    result_dict["ram-history"] = fig.show(legend=True)
            else:
                result_dict["ram-history"] = f"Single memory update found:\n" \
                    f"Memory usage on the {dates[0]} was updatet to {ram[0]} MB"

        result_list.append(result_dict)

    return result_list


def summarize(log_files: list_of_logs) -> log_inf_list:
    """
    Summarises all used resources and the runtime in total and average

    Runs through the log files via the log_to_dict function

    :return:
    """

    logging.info('Starting the summarizer mode')

    valid_files = len(log_files)
    # no given files
    if valid_files == 0:
        return "No files to summarize"

    # allocated all diffrent datatypes, easier to handle
    result_dict = dict()

    aborted_files = 0
    still_running = 0
    error_reading_files = 0
    other_exception = 0
    normal_runtime = datetime.timedelta()
    host_nodes = dict()

    total_usages = np.array([0, 0, 0], dtype=float)
    total_requested = np.array([0, 0, 0], dtype=float)
    total_allocated = np.array([0, 0, 0], dtype=float)

    for file in track(log_files, transient=True, description="Summarizing..."):
        try:
            job_dict, res_dict, time_dict, _, _ = log_to_dict(file)

            # continue if Process is still running
            if job_dict['Execution details'][0].__eq__("Process is"):
                still_running += 1
                rprint(f"[orange3]Process of {file} is still running, \n"
                       f"it will be ignored for this summation[/orange3]")
                continue

            elif job_dict['Execution details'][0].__eq__("Process was"):
                aborted_files += 1
                rprint(f"[orange3]Process of {file} has been aborted, \n"
                       f"it will be ignored for this summation[/orange3]")
                continue
            elif job_dict['Execution details'][0].__eq__("Error"):
                error_reading_files += 1
                rprint(f"[orange3]Process of {file} not fully readable,\n"
                       f"cause it's not matching the htcondor specs,\n"
                       f"it will be ignored for this summation[/orange3]")
                continue
            elif len(job_dict) == 0:
                logging.error(
                    "if this even get's printed out, more work is needed")
                rprint(f"[orange3]Process of {file} is strange, \n"
                       f"don't know how to handle this yet[/orange3]")
                other_exception += 1
                continue

            if "Total runtime" in time_dict["Dates and times"]:
                normal_runtime += time_dict['Values'][3]
            host = job_dict['Values'][2]
            if host in host_nodes:
                host_nodes[host][0] += 1
                host_nodes[host][1] += time_dict['Values'][3]
            else:
                host_nodes[host] = [1, time_dict['Values'][3]]

            total_usages += np.nan_to_num(res_dict["Usage"])
            total_requested += np.nan_to_num(res_dict["Requested"])
            total_allocated += np.nan_to_num(res_dict["Allocated"])

        # Error ocurres when Job was aborted
        except ValueError or KeyError as err:
            logging.exception(err)
            rprint(f"[red]Error with summarizing: {file}[/red]")
            continue
        except TypeError as err:
            logging.exception(err)
            rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
            sys.exit(3)

    # calc difference of successful executed jobs
    n = valid_files - aborted_files - still_running - other_exception - error_reading_files

    average_runtime = normal_runtime / n if n != 0 else normal_runtime
    average_runtime = datetime.timedelta(days=average_runtime.days,
                                         seconds=average_runtime.seconds)

    exec_dict = {
        "Job types": ["normal executed jobs"],
        "Occurrence": [n]
    }
    if aborted_files > 0:
        exec_dict["Job types"].append("Aborted jobs")
        exec_dict["Occurrence"].append(aborted_files)
    if still_running > 0:
        exec_dict["Job types"].append("Still running jobs")
        exec_dict["Occurrence"].append(still_running)
    if error_reading_files > 0:
        exec_dict["Job types"].append("Error while reading")
        exec_dict["Occurrence"].append(error_reading_files)
    if other_exception > 0:
        exec_dict["Job types"].append("Other exceptions")
        exec_dict["Occurrence"].append(other_exception)

    result_dict["execution-details"] = exec_dict

    # do not even try futher if the only files given have been aborted, are still running etc.
    if n == 0:
        return result_dict

    create_desc = "The following data only implies on sucessful executed jobs"
    if aborted_files > 0 or still_running > 0 or other_exception > 0:
        create_desc += f"\n{colors['light_grey']}" \
            f"Use the analysed-summary mode for more details about the other jobs" \
            f"{colors['back_to_default']}"

    result_dict["summation-description"] = create_desc

    time_desc_list = list()
    time_value_list = list()
    if normal_runtime != datetime.timedelta(0, 0, 0):
        time_desc_list.append("Total runtime")
        time_value_list.append(normal_runtime)
    if average_runtime:
        time_desc_list.append("Average runtime")
        time_value_list.append(average_runtime)

    result_dict["times"] = {
        "Times": time_desc_list,
        "Values": time_value_list
    }

    if n != 0:  # do nothing, if all valid jobs were aborted

        average_dict = {
            "Resources": ['Average Cpu', 'Average Disk (KB)',
                          'Average Memory (MB)'],
            "Usage": np.round(total_usages / n, 4),
            "Requested": np.round(total_requested / n, 2),
            "Allocated": np.round(total_allocated / n, 2)

        }

        average_dict = manage_thresholds(average_dict)

        result_dict["all-resources"] = average_dict

    if len(host_nodes) > 0:

        executed_jobs = list()
        runtime_per_node = list()
        for val in host_nodes.values():
            executed_jobs.append(val[0])
            average_job_duration = val[1] / val[0]
            runtime_per_node.append(
                datetime.timedelta(average_job_duration.days,
                                   average_job_duration.seconds))

        cpu_dict = {
            "Host Nodes": list(host_nodes.keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        }

        result_dict["host-nodes"] = cpu_dict

    return [result_dict]


def analysed_summary(log_files: list_of_logs) -> log_inf_list:
    """
        analyse the summarized log files,
        this is meant to give the ultimate output
        about every single log event in average etc.

        Runs through the log files via the log_to_dict function

        :return: string
        """

    logging.info('Starting the analysed summary mode')

    valid_files = len(log_files)
    # no given files
    if valid_files == 0:
        return "No files for the analysed summary"

    # fill this dict with information by the execution type of the jobs
    all_files = dict()
    list_of_gpu_names = list()  # list of gpus found
    occurrence_dict = dict()

    for file in track(log_files, transient=True, description="Summarizing..."):

        job_dict, res_dict, time_dict, ram_history, occurred_errors = log_to_dict(
            file)

        if len(occurred_errors) > 0:
            create_file_list = list()
            for i in range(len(occurred_errors["Event Number"])):
                create_file_list.append(file)
            occurred_errors['File'] = create_file_list

        refactor_job_dict = dict(
            zip(job_dict["Execution details"], job_dict["Values"]))
        job_keys = list(refactor_job_dict.keys())
        if "Executing on" in job_keys:
            to_host = refactor_job_dict["Executing on"]

        termination_type = job_dict["Values"][0]

        # if time dict exists
        time_keys = list()
        if time_dict:
            refactor_time_dict = dict(
                zip(time_dict["Dates and times"], time_dict["Values"]))
            time_keys = list(refactor_time_dict.keys())
        if "Waiting time" in time_keys:
            waiting_time = refactor_time_dict["Waiting time"]
        else:
            waiting_time = datetime.timedelta()
        if "Execution runtime" in time_keys:
            runtime = refactor_time_dict["Execution runtime"]
        else:
            runtime = datetime.timedelta()
        if "Total runtime" in time_keys:
            total_time = refactor_time_dict["Total runtime"]
        else:
            total_time = datetime.timedelta()

        try:
            if termination_type in all_files:
                # logging.debug(all_files[termination_type])
                all_files[termination_type][0] += 1  # count number
                all_files[termination_type][1] += waiting_time
                all_files[termination_type][2] += runtime
                all_files[termination_type][3] += total_time

                # add errors
                if len(occurred_errors) > 0:
                    for key in occurred_errors.keys():
                        all_files[termination_type][6][key].extend(
                            occurred_errors[key])

                if not len(all_files[termination_type][4]) == 0:
                    # add usages

                    all_files[termination_type][4]["Usage"] += np.nan_to_num(
                        res_dict["Usage"])
                    # add requested
                    all_files[termination_type][4][
                        "Requested"] += np.nan_to_num(res_dict["Requested"])
                    # allocated
                    all_files[termination_type][4][
                        "Allocated"] += np.nan_to_num(res_dict["Allocated"])

                # add cpu
                if to_host != "":
                    # cpu known ???
                    if to_host in all_files[termination_type][5].keys():
                        all_files[termination_type][5][to_host][0] += 1
                        all_files[termination_type][5][to_host][
                            1] += total_time
                    else:
                        all_files[termination_type][5][to_host] = [1,
                                                                   total_time]
                elif "Submitted from" in job_dict["Execution details"]:
                    # other waiting jobs ???
                    if 'Waiting for execution' in all_files[termination_type][
                        5].keys():
                        all_files[termination_type][5][
                            'Waiting for execution'][0] += 1
                        all_files[termination_type][5][
                            'Waiting for execution'][1] += total_time
                    else:
                        count_host_nodes = dict()
                        count_host_nodes['Waiting for execution'] = [1,
                                                                     total_time]
                        all_files[termination_type][5] = count_host_nodes
                else:
                    # other aborted before submission jobs ???
                    if 'Aborted before submission' in \
                            all_files[termination_type][5].keys():
                        all_files[termination_type][5][
                            'Aborted before submission'][0] += 1
                        all_files[termination_type][5][
                            'Aborted before submission'][1] += total_time
                    else:
                        count_host_nodes = dict()
                        count_host_nodes['Aborted before submission'] = [1,
                                                                         total_time]
                        all_files[termination_type][5] = count_host_nodes

            # else new entry
            else:
                # if host exists
                if "Executing on" in job_dict["Execution details"]:
                    # to_host = job_dict["Values"][2]
                    count_host_nodes = dict()
                    count_host_nodes[to_host] = [1, total_time]
                # else if still waiting
                elif "Submitted from" in job_dict["Execution details"]:
                    count_host_nodes = dict()
                    count_host_nodes['Waiting for execution'] = [1, total_time]
                # else aborted before submission ?
                else:
                    count_host_nodes = dict()
                    count_host_nodes['Aborted before submission'] = [1,
                                                                     total_time]

                # convert nan values to 0
                if len(res_dict) > 0:
                    res_dict["Usage"] = np.nan_to_num(res_dict["Usage"])
                    res_dict["Requested"] = np.nan_to_num(
                        res_dict["Requested"])
                    res_dict["Allocated"] = np.nan_to_num(
                        res_dict["Allocated"])

                all_files[termination_type] = [1, waiting_time, runtime,
                                               total_time,
                                               res_dict, count_host_nodes,
                                               occurred_errors]

        # Error ocurres when Job was aborted
        except ValueError or KeyError as err:
            logging.exception(err)
            rprint(f"[red]Error with summarizing: {file}[/red]")
            continue
        except TypeError as err:
            logging.exception(err)
            rprint(f"[red] {err}[/red]")
            sys.exit(3)

    # Now put everything together
    result_list = list()
    for term_state in all_files:
        term_info = all_files[term_state]
        result_dict = dict()

        # differentiate between terminated and running processes
        if "Error while reading" in term_state:
            result_dict["file-description"] = f"" \
                f"##################################################\n" \
                f"## All files, that caused an {colors['red']}error while reading{colors['back_to_default']}\n" \
                f"##################################################"
        elif not term_state in ["Waiting", "Executing"]:
            result_dict["file-description"] = f"" \
                f"##################################################\n" \
                f"## All files with the termination state: {term_state}\n" \
                f"##################################################"
        else:
            result_dict["file-description"] = f"" \
                f"###########################################\n" \
                f"## All files, that are currently {term_state}\n" \
                f"###########################################"

        n = int(term_info[0])
        occurrence_dict[term_state] = str(n)

        times = np.array([term_info[1], term_info[2], term_info[3]])
        av_times = times / n
        format_av_times = [
            datetime.timedelta(days=time.days, seconds=time.seconds) for time
            in av_times]

        time_dict = {
            "Times": ["Waiting time", "Runtime", "Total"],
            "Average": format_av_times,
            "Total": times
        }

        result_dict["times"] = time_dict

        if not len(term_info[4]) == 0:
            total_resources_dict = term_info[4]
            avg_dict = {
                'Resources': ['Average Cpu', ' Average Disk (KB)',
                              'Average Allocated'],
                'Usage': np.round(
                    np.array(total_resources_dict['Usage']) / term_info[0],
                    2).tolist(),
                'Requested': np.round(
                    np.array(total_resources_dict['Requested']) / term_info[0],
                    2).tolist(),
                'Allocated': np.round(
                    np.array(total_resources_dict['Allocated']) / term_info[0],
                    2).tolist()
            }
            if 'Assigned' in total_resources_dict.keys():
                avg_dict['Resources'].append('Gpu')
                avg_dict['Assigned'] = ['', '', '',
                                        ", ".join(list_of_gpu_names)]

            avg_dict = manage_thresholds(avg_dict)
            result_dict["all-resources"] = avg_dict

        executed_jobs = list()
        runtime_per_node = list()
        for val in term_info[5].values():
            executed_jobs.append(val[0])
            average_job_duration = val[1] / val[0]
            runtime_per_node.append(
                datetime.timedelta(average_job_duration.days,
                                   average_job_duration.seconds))

        host_nodes_dict = {
            "Host Nodes": list(term_info[5].keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        }

        result_dict["host-nodes"] = host_nodes_dict

        if len(term_info[6]) > 0:
            temp_err = term_info[6]
            del temp_err[
                "Reason"]  # remove reason, cause thats just too much information
            result_dict["errors"] = temp_err

        result_list.append(result_dict)

    new_occ = {
        "Termination type": list(occurrence_dict.keys()),
        "Appearance": list(occurrence_dict.values())
    }
    result_list.insert(0, {"execution-details": new_occ})

    return result_list


# search in the files for the keywords
def filter_for(log_files: list_of_logs,
               keywords: list,
               extend=False,
               mode=None) -> log_inf_list:
    """
    Filter for a list of keywords, which can be extended
    and print out every file which matches the pattern (not case sensitive)
    The filtered files can be analysed summarise, etc afterwards,
    else this function will return None

    :param log_files:
    :param keywords:
    :param extend:
    :return:
        list with dicts depending on the used mode, to forward the filtered files,

        None if no forwarding is set
    """
    logging.info('Starting the filter mode')

    # if the keywords are given as a string, try to create a list
    if type(keywords) == list:
        keyword_list = keywords
    elif type(keywords) == str:
        # remove spaces and commas from string
        keyword_list = " ".join(re.split(',| ',keywords)).split()
    else:
        logging.debug(
            f"Filter mode only accepts a string or list with keywords, not {keywords}")
        raise_type_error("Expecting a list or a string")

    # if extend is set, keywords like err will
    # also look for keywords like warn exception, aborted, etc.
    if extend:  # apply some rules
        # the error list
        err_list = ["err", "warn", "exception", "aborted", "abortion",
                    "abnormal", "fatal"]

        # remove keyword if already in err_list
        for i in range(len(keyword_list)):
            if (keyword_list[i]).lower() in err_list:
                keyword_list.remove(keyword_list[i])

        keyword_list.extend(err_list)  # extend search

        rprint(
            "[green]Keyword List was extended, now search for these keywords:[/green]",
            keyword_list)
    else:
        rprint("[green]Search for these keywords:[/green]", keyword_list)

    if len(keyword_list) == 1 and keyword_list[0] == "":
        logging.debug("Empty filter, don't know what to do")
        return f"{colors['yellow']}Don't know what to do with an empty filter,\n" \
            f"if you activate the filter mode in the config file, \n" \
            f"please add a [filter] section with the filter_keywords = your_filter"

    logging.debug(f"These are the keywords to look for: {keyword_list}")

    # now search
    found_at_least_one = False
    found_logs = []
    for file in track(log_files, transient=True, description="Filtering..."):
        found = False
        with open(file, "r") as read_file:
            for line in read_file:
                for keyword in keyword_list:
                    if re.search(keyword.lower(), line.lower()):
                        if not found_at_least_one:
                            print("Matches:")
                        rprint(f"[grey74]{keyword} in:\t{file}[/grey74] ")
                        found = True
                        found_at_least_one = True
                        break
                if found:
                    found_logs.append(file)
                    break

    return_dicts = None
    if not found_at_least_one:
        rprint("[red]Unable to find these keywords:[/red]", keyword_list)
        rprint("[red]maybe try again with similar expressions[/red]")

    elif mode is not None:
        print(f"Total count: {len(found_logs)}")
        if mode.__eq__("default"):
            return_dicts = default(found_logs)
        elif mode.__eq__("analysed-summary"):
            rprint("[magenta]Give an analysed summary"
                   " for these files[/magenta]")
            return_dicts = analysed_summary(found_logs)
        elif mode.__eq__("summarize"):
            rprint("[magenta]Summarize these files[/magenta]")
            return_dicts = summarize(found_logs)
        elif mode.__eq__("analyse"):
            rprint("[magenta]Analyse these files[/magenta]")
            return_dicts = analyse(found_logs)
    # if not reading from stdin or redirected
    elif not GlobalServant.reading_stdin \
            and not GlobalServant.redirecting_stdout:
        rprint("[blue]Want do do more?[/blue]")
        x = input(
            "default(d), summarize(s), analyse(a),"
            " analysed summary(as), exit(e): ")
        if x == "d":
            return_dicts = default(found_logs)
        elif x == "s":
            return_dicts = summarize(found_logs)
        elif x == "a":
            return_dicts = analyse(found_logs)
        elif x == "as":
            return_dicts = analysed_summary(found_logs)
        elif x == "e":
            sys.exit(0)
        else:
            print('Not a valid argument, quitting ...')
            sys.exit(0)

    return return_dicts


def validate_given_logs(file_list: list_of_logs,
                        std_log="", std_err=".err",
                        std_out=".log") -> list_of_logs:
    """
    This method is supposed to take the given log files
    (by config or command line argument)
    and tries to determine if these are valid log files,
     ignoring the std_log specification, if std_log = "".

    It will run through given directories and check every single file,
    if accessible, for the
    HTCondor log file standard,
    valid log files should start with lines like:

    000 (5989.000.000) 03/30 15:48:47


    The user will also be informed if a single given file was not found.

    """
    valid_files = list()
    total = len(file_list)

    logging.info('Validate given log files')
    with Progress(transient=True) as progress:

        task = progress.add_task("Validating...", total=total, start=False)

        for arg in file_list:

            path = os.getcwd()  # mainly search in current working directory
            logs_path = path + "/" + arg  # absolute path

            working_dir_path = ""
            working_file_path = ""

            if os.path.isdir(arg):
                working_dir_path = arg
            elif os.path.isdir(logs_path):
                working_dir_path = logs_path

            elif os.path.isfile(arg):
                working_file_path = arg
            elif os.path.isfile(logs_path):
                working_file_path = logs_path
            # check if only the id was given
            # and resolve it with the std_log specification
            elif os.path.isfile(arg + std_log):
                working_file_path = arg + std_log
            elif os.path.isfile(logs_path + std_log):
                working_file_path = logs_path + std_log

            # if path is a directory
            if working_dir_path.__ne__(""):
                rprint(
                    f"[light_coral]Search: {working_dir_path} "
                    f"for valid log files[/light_coral]")
                # run through all files and
                # separate the files into log, error and output files
                valid_dir_files = list()
                file_dir = os.listdir(working_dir_path)
                total += len(file_dir) - 1
                for file in file_dir:
                    progress.update(task, total=total, advance=1)

                    # if std_log is set, ignore other log files
                    if std_log.__ne__("") and not file.endswith(std_log):
                        logging.debug("Ignoring this file, " + file +
                                      ", because std-log is: " + std_log)
                        continue

                    # ignore hidden files
                    if file.startswith("."):
                        continue
                    # if it's not a sub folder
                    if working_dir_path.endswith('/'):
                        file_path = working_dir_path + file
                    else:
                        file_path = working_dir_path + '/' + file

                    if os.path.isfile(file_path):
                        with open(file_path, "r") as read_file:
                            if os.path.getsize(
                                    file_path) == 0:  # file is empty
                                continue

                            if re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)",
                                        read_file.readlines()[0]):
                                valid_dir_files.append(file_path)

                    else:
                        logging.debug(
                            f"Found a subfolder: "
                            f"{working_dir_path}/{file}"
                            f", it will be ignored")
                        progress.console.print(
                            f"[yellow]Found a subfolder:"
                            f" {working_dir_path}/{file},"
                            f" it will be ignored[/yellow]")

                valid_files.extend(valid_dir_files)
                rprint(
                    f"[green]Found {len(valid_dir_files)}"
                    f" valid log files out of "
                    f"{len(file_dir)} files[/green]\n")
            # else if path "might" be a valid HTCondor file
            elif working_file_path.__ne__(""):
                progress.update(task, advance=1)

                # if std_log is set, ignore other log files
                if std_log.__ne__("") and \
                        not working_file_path.endswith(std_log):
                    logging.debug("Ignoring this file, " +
                                  working_file_path +
                                  ", because std_log is set to: " + std_log)
                    continue

                if std_err.__ne__("") \
                        and working_file_path.endswith(std_err)\
                        or std_out.__ne__("") \
                        and working_file_path.endswith(std_out):
                    logging.debug(
                        f"Only log files accepted,"
                        f" ignored this file: {working_file_path}")
                    continue

                with open(working_file_path, "r") as read_file:

                    if os.path.getsize(
                            working_file_path) == 0:  # file is empty
                        progress.console.print(
                            f"[red]How dare you, the file is empty:"
                            f" {read_file.name} :([/red]")

                    elif re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)",
                                  read_file.readlines()[0]):
                        logging.debug(
                            f"{read_file.name} is a valid HTCondor log file")

                        valid_files.append(working_file_path)
                    else:
                        logging.debug(
                            f"The given file {read_file.name}"
                            f" is not a valid HTCondor log file")
                        progress.console.print(
                            f"[yellow]The given file {read_file.name} "
                            f"is not a valid HTCondor log file[/yellow]")
            else:
                logging.error(f"The given file: {arg} does not exist")
                rprint(f"[red]The given file: {arg} does not exist[/red]")

    return valid_files


def setup_logging_tool(log_file=None, verbose_mode=False):
    """
        Set up the logging device,
        to generate a log file, with --generate-log-file
        or to print more descriptive output with the verbose mode to stdout

        both modes are compatible together
    :return:
    """

    # disable the loggeing tool by default
    logging.getLogger().disabled = True

    # I don't know why a root handler is already set,
    # but we have to remove him in order
    # to get just the output of our own handler
    if len(logging.root.handlers) == 1:
        default_handler = logging.root.handlers[0]
        logging.root.removeHandler(default_handler)

    # if logging tool is set to use
    if log_file is not None:
        # activate logger if not already activated
        logging.getLogger().disabled = False

        # more specific view into the script itself
        logging_file_format = '%(asctime)s - [%(funcName)s:%(lineno)d]' \
                              ' %(levelname)s : %(message)s'
        file_formatter = logging.Formatter(logging_file_format)

        handler = RotatingFileHandler(log_file, maxBytes=1000000,
                                      backupCount=1)
        handler.setLevel(logging.DEBUG)
        handler.setFormatter(file_formatter)

        log = logging.getLogger()
        log.setLevel(logging.DEBUG)
        log.addHandler(handler)

    if verbose_mode:
        # activate logger if not already activated
        logging.getLogger().disabled = False

        logging_stdout_format = '%(asctime)s - %(levelname)s: %(message)s'
        stdout_formatter = logging.Formatter(logging_stdout_format)

        stdout_handler = logging.StreamHandler(sys.stdout)
        stdout_handler.setLevel(logging.DEBUG)
        stdout_handler.setFormatter(stdout_formatter)
        log = logging.getLogger()
        log.setLevel(logging.DEBUG)
        log.addHandler(stdout_handler)


def customize_results(log_files: list_of_logs,
                      mode: str,
                      ignore_list=list,
                      show_list=list,
                      filter_keywords=list,
                      filter_extended=False,
                      table_format="pretty",
                      **kwargs) -> str:
    output_string = ""

    if len(filter_keywords) > 0:
        results = filter_for(log_files,
                             keywords=filter_keywords,
                             extend=filter_extended,
                             mode=mode)
    elif mode.__eq__("default"):
        results = default(log_files)  # force default with -d
    elif mode.__eq__("analysed-summary"):
        results = analysed_summary(log_files)  # analysed summary ?
    elif mode.__eq__("summarize"):
        results = summarize(log_files)  # summarize information
    elif mode.__eq__("analyse"):
        results = analyse(log_files)  # analyse the given log_files
    else:
        results = default(log_files)  # anyways try to print default output

    # This can happen, when for example the filter mode is not forwarded
    if results is None:
        sys.exit(0)

    work_with = results
    # convert result to list, if given as dict
    if type(results) == dict:
        work_with = [results]

    # check for ignore values
    for i in range(len(work_with)):
        mystery = work_with[i]

        if "execution-details" in mystery:
            if "execution-details" in ignore_list:
                del mystery["execution-details"]
            else:
                mystery["execution-details"] = tabulate(
                    mystery["execution-details"],
                    headers='keys', tablefmt=table_format)

        if "times" in mystery:
            if "times" in ignore_list:
                del mystery["times"]
            else:
                mystery["times"] = tabulate(mystery["times"], showindex=False,
                                            headers='keys',
                                            tablefmt=table_format)

        if "all-resources" in mystery:
            resources = mystery["all-resources"]
            if "all-resources" in ignore_list:
                del mystery["all-resources"]
            else:
                if "used-resources" in ignore_list:
                    del mystery["all-resources"]["Usage"]
                if "requested-resources" in ignore_list:
                    del mystery["all-resources"]["Requested"]
                if "allocated-resources" in ignore_list:
                    del mystery["all-resources"]["Allocated"]

                mystery["all-resources"] = tabulate(resources, showindex=False,
                                                    headers='keys',
                                                    tablefmt=table_format)

        if "errors" in mystery:
            if "errors" in ignore_list:
                del mystery["errors"]
            else:
                mystery["errors"] = "Occurred errors: \n" + \
                                    tabulate(mystery["errors"],
                                             showindex=False, headers='keys',
                                             tablefmt='grid')

        if "host-nodes" in mystery:
            if "host-nodes" in ignore_list:
                del mystery["host-nodes"]
            else:
                mystery["host-nodes"] = tabulate(mystery["host-nodes"],
                                                 showindex=False,
                                                 headers='keys',
                                                 tablefmt=table_format)

        # TODO: show-more section

        for _i in mystery:
            if mystery[_i] is not None:
                output_string += mystery[_i]
            else:
                logging.debug("This musst be fixed, NoneType found.")
                rprint(
                    "[red]NoneType object found, this should not happen[/red]")

            if mystery[_i] == mystery[list(mystery.keys())[-1]]:
                output_string += "\n"
            else:
                output_string += "\n\n"

        output_string += "\n"

    return output_string


def run(commandline_args):
    """
    Run this script

    This searches first for a given config file.
    After that, given parameters in the terminal will be interpreted, so they have a higher priority
    given files will be validated and the logging tool will be managed
    After that the user input will be used to process the output to the terminal,
    which will contain information about the given log files

    :return:
    """
    try:
        start = datetime.datetime.now()  # start date for runtime

        #initialize()  # initialize global parameters

        check_for_redirection()
        # if exit parameters are given that will interrupt this script,
        # catch them here so the config won't be unnecessary loaded
        param_dict = manage_params(commandline_args)
        print(param_dict)

        # if not --no-config is set:
        # remove the first file, that can be interpreted as a config file,
        # all other files will be interpreted as log files,
        # so DON'T apply more than one config file to the script
        found_conf = None
        if not param_dict["no_config"]:
            found_conf, commandline_args = find_config(commandline_args)

        setup_logging_tool(param_dict["generate_log_file"],
                           param_dict["verbose"])

        logging.debug("-------Start of htcompact scipt-------")

        if param_dict["verbose"]:
            logging.info('Verbose mode turned on')

        if GlobalServant.reading_stdin:
            logging.debug("Reading from stdin")
        if GlobalServant.redirecting_stdout:
            logging.debug("Output is getting redirected")

        if found_conf is not None:
            load_config(found_conf)

        # after that, interpret the command line arguments,
        # these will have a higher priority, than the config setup
        # and by that arguments set in the config file, will be ignored
        #manage_params(commandline_args)

        valid_files = validate_given_logs(param_dict["files"],
                                          std_log=param_dict["std_log"],
                                          std_err=param_dict["std_err"],
                                          std_out=param_dict["std_out"])
        if len(valid_files) == 0:
            rprint("[red]No valid HTCondor log files found[/red]")
            sys.exit(2)

        # output_str = customize_results(valid_files, param_dict["mode"],
        #                                ignore_list=param_dict["ignore"],
        #                                show_list=param_dict["show"],
        #                                filter_keywords=param_dict["filter"],
        #                                filter_extended=param_dict["extend"],
        #                                reading_stdin=reading_stdin,
        #                                redirecting_stdout=redirecting_stdout,
        #                                table_format=param_dict["table_format"])
        output_str = customize_results(log_files=valid_files,**param_dict)

        print(output_str)  # write it to the console

        end = datetime.datetime.now()  # end date for runtime

        logging.debug(f"Runtime: {end - start}")  # runtime of this script

        logging.debug("-------End of htcompact script-------")

        sys.exit(0)

    except TypeError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        sys.exit(3)

    except KeyboardInterrupt:
        logging.info("Script was interrupted by the user")
        print("Script was interrupted")
        sys.exit(4)


if __name__ == "__main__":
    """
    This is the main function, 
    which runs the script, if not imported as a module

    :return: exit status 0-4
    """
    run(sys.argv[1:])
