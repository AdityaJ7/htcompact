#!/usr/bin/env python3
import re
import sys
import os
import getopt
import datetime
import logging

import configparser
import pandas as pd
import numpy as np
from tabulate import tabulate
from plotille import Figure

"""

This script is basically reading information from HTCondor log files and storing them into
pandas DataFrames. By that its actually pretty simple to analyse the data.

The script makes the information compact and easy to under stand, that's the reason for the name htcompact

Single logs can be read quite easily, but also it's possible to summarise a wholÃ¶e directory with logs 
to see for ex. the avarage runtime and usage of all the logs

"""

# Todo: logging should be declared in setup and command line arguments
# make default changes to logging tool
log_disabled = False
logging.getLogger().disabled = log_disabled  # no more logging
if not log_disabled:
    logging.basicConfig(filename="stdout.log", level=logging.DEBUG,
                        format='%(asctime)s - [%(funcName)s:%(lineno)d] %(levelname)s : %(message)s')

# global parameters, used for dynamical output of information
accepted_states = ["true", "yes", "y", "ja", "j", "enable", "enabled", "wahr", "0"]
files = list()
border_str = ""
option_shorts = "hsae:"
option_longs = ["help", "std-log=", "std-err=", "std-out=",
                "show-errors", "show-output", "show-warnings",
                "ignore-resources", "ignore-job-information", "ignore-allocated-resources",
                "to-csv", "summarise", "analyse", "search=", "get-event=",
                "resolve-ip", "table-format="]

# variables for given parameters
show_std_output = False
show_std_warnings = False
show_std_errors = False

# ignore infromation
ignore_allocated_resources = False
ignore_job_information = False
ignore_resources = False

# Features:
resolve_ip_to_hostname = False
reverse_dns_lookup = False  # Todo: implement in function (filter_for_host)
summarise = False
analyse = False
search = False
search_values = ""
to_csv = False

# escape sequences for colors
red = "\033[0;31m"
green = "\033[0;32m"
yellow = "\033[0;33m"
magenta = "\033[0;35m"
cyan = "\033[0;36m"
lightgrey = "\033[37m"
back_to_default = "\033[0;39m"

# thresholds for bad and low usage of resources
low_usage = 0.75
bad_usage = 1.2

# global variables with default values for err/log/out files
std_log = ""
std_err = ""
std_out = ""

# global defaults
config_file = "htcsetup.conf"
table_format = "pretty"  # ascii by default

# Todos:
# Todo: did a lot of test, but it needs more
# Todo: background colors etc. for terminal usage
# Todo: filter erros better, less priority
# Todo: realise the further specs on: https://jugit.fz-juelich.de/inm7/infrastructure/scripts/-/issues/1
# a redirection in the terminal via > should ignore escape sequences


# may not be needed in future but will be used for now
def remove_files_from_args(args, short_opts, long_opts):
    """
        I want to make it easier for the user to insert many files at once.
        The script should detect option arguments and files

        It will filter the given files and remove them from the argument list

        !!!Exactly that is this method doing!!! (because getopt has no such function)

    :param args: is supposed to be a list, in any case it should be the sys.argv[1:] list
    :param short_opts: the short getopt options
    :param long_opts: the long getopt options
    :return: list of getopt arguments
    """
    new_args = args.copy()
    generate_files = list()
    index = 0
    # run through the command line arguments, skip the getopt argument and interprete everything else as a file
    while True:
        if index >= len(args):
            break

        arg = args[index]

        if arg.startswith("-"):  # for short_args
            arg = arg[1:]
            if arg.startswith("-"):  # for long_args
                arg = arg[1:]

            # skip the argument if getopt arguments are setable
            if arg+":" in short_opts:
                index += 1
            elif arg+"=" in long_opts:
                index += 1
        else:
            generate_files.append(arg)
            new_args.remove(arg)

        index += 1

    # change files, if found
    if len(generate_files) > 0:
        global files
        files = generate_files

    # return valid arguments
    return new_args


def manage_exit_params():

    global option_shorts, option_longs

    # if for whatever reasons files were given
    better_args = remove_files_from_args(sys.argv[1:], option_shorts, option_longs)

    try:
        opts, args = getopt.getopt(better_args, option_shorts, option_longs)

        for opt, arg in opts:

            if opt in ["-h", "--help"]:
                man_help()
                sys.exit(0)
            elif opt in ["-e", "--get-event"]:
                for desc in get_event_information(arg):
                    print(desc)
                sys.exit(0)

    # print error messages
    except Exception as err:
        logging.exception(err)  # write a detailed description in the stdout.log file
        print((red + "{0}: {1}" + back_to_default).format(err.__class__.__name__, err))
        print(small_help())
        sys.exit(1)


# Todo: thresholds by command line
def manage_params():
    """
    Interprets the given command line arguments and changes the global variables in this scrips

    """
    global files  # list of files and directories
    global std_log, std_err, std_out  # all default values for the HTCondor files
    global show_std_output, show_std_warnings, show_std_errors  # show more information variables
    global ignore_resources, ignore_job_information, ignore_allocated_resources  # ignore information variables
    global to_csv, summarise, analyse, search, search_values  # features
    global table_format  # table_format can be changed
    global resolve_ip_to_hostname  # if set hots ip will changed to a cpu-number

    global option_shorts, option_longs

    better_args = remove_files_from_args(sys.argv[1:], option_shorts, option_longs)

    try:
        opts, args = getopt.getopt(better_args, option_shorts, option_longs)

        for opt, arg in opts:

            # catch unusual but not wrong parameters starting with -
            if arg.startswith("-"):
                print(yellow+"The argument for {0} is {1}, is that wanted?".format(opt, arg)+back_to_default)
                logging.warning("The argument for {0} is {1}, is that wanted?".format(opt, arg))

            elif opt in ["-s", "--summarise"]:
                summarise = True
                logging.debug("Summariser mode turned on")
            elif opt in ["-a", "--analyse"]:
                analyse = True
                logging.debug("Analyser mode turned on")
            elif opt in ["--search"]:
                search = True
                search_values = arg
                logging.debug(f"Search mode turned on")
            # all HTCondor files, given by the user if they are not saved in .log/.err/.out files
            elif opt == "--std-log":
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_log = arg
            elif opt == "--std-err":
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_err = arg
            elif opt == "--std-out":
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_out = arg
            # all variables, to show more specific information
            elif opt.__eq__("--show-output"):
                show_std_output = True
            elif opt.__eq__("--show-warnings"):
                show_std_warnings = True
            elif opt.__eq__("--show-errors"):
                show_std_errors = True

            # all variables to ignore unwanted information
            elif opt.__eq__("--ignore-allocated-resources"):
                ignore_allocated_resources = True
            elif opt.__eq__("--ignore-resources"):
                ignore_resources = True
            elif opt.__eq__("--ignore-job-information"):
                ignore_job_information = True

            # all tabulate variables
            elif opt.__eq__("--to-csv"):
                to_csv = True

            elif opt.__eq__("--table-format"):
                types = "plain,simple,github,grid,fancy_grid,pipe," \
                        "orgtbl,rst,mediawiki,html,latex,latex_raw," \
                        "latex_booktabs,tsv,pretty"
                # only valid arguments
                if arg in types.split(","):
                    table_format = arg
                else:
                    logging.debug("The given table format doesn't exist")

            elif opt.__eq__("--resolve-ip"):
                resolve_ip_to_hostname = True

            else:
                print(small_help())
                sys.exit(0)
    # print error messages
    except Exception as err:
        logging.exception(err)  # write a detailed description in the stdout.log file
        print((red+"{0}: {1}"+back_to_default).format(err.__class__.__name__, err))
        print(small_help())
        sys.exit(1)

    if len(files) == 0:
        print(small_help())
        sys.exit(2)


def small_help():
    """
Usage: htcompact (log_files|config_file) [ Arguments ]

----------------------------Main features:---------------------------------

[-s|--summarise]            Takes all given logs and summarises resources,
                            runtime and other informations in total and
                            average

[-a|--analyse]              Takes a specific log and analyses it,
                            however multiple logs are possible.
                            The result will have all time diffrences,
                            a histogram of ram usage over time,
                            ocurred errors and much more


More help on other options with: man htcompact
    """
    # returns this docstring
    return small_help.__doc__


def man_help():

    check_places = ['/share/man/man1/htcompact.1', '/env/share/man/man1/htcompact.1',
                    '/man/man1/htcompact.1', '/../man/man1/htcompact.1']
    cwd = os.getcwd()
    for place in check_places:
        path = cwd+place
        if os.path.isfile(path):
            os.system(f"groff -Tlatin1 -mandoc {path}")


def get_event_information(event_id):
    """
Event Number: 000
Event Name: Job submitted
Event Description: This event occurs when a user submits a job. It is the first event you will see for a job, and it should only occur once.

Event Number: 001
Event Name: Job executing
Event Description: This shows up when a job is running. It might occur more than once.

Event Number: 002
Event Name: Error in executable
Event Description: The job could not be run because the executable was bad.

Event Number: 003
Event Name: Job was checkpointed
Event Description: The job's complete state was written to a checkpoint file. This might happen without the job being removed from a machine, because the checkpointing can happen periodically.

Event Number: 004
Event Name: Job evicted from machine
Event Description: A job was removed from a machine before it finished, usually for a policy reason. Perhaps an interactive user has claimed the computer, or perhaps another job is higher priority.

Event Number: 005
Event Name: Job terminated
Event Description: The job has completed.

Event Number: 006
Event Name: Image size of job updated
Event Description: An informational event, to update the amount of memory that the job is using while running. It does not reflect the state of the job.

Event Number: 007
Event Name: Shadow exception
Event Description: The condor_shadow, a program on the submit computer that watches over the job and performs some services for the job, failed for some catastrophic reason. The job will leave the machine and go back into the queue.

Event Number: 008
Event Name: Generic log event
Event Description: Not used.

Event Number: 009
Event Name: Job aborted
Event Description: The user canceled the job.

Event Number: 010
Event Name: Job was suspended
Event Description: The job is still on the computer, but it is no longer executing. This is usually for a policy reason, such as an interactive user using the computer.

Event Number: 011
Event Name: Job was unsuspended
Event Description: The job has resumed execution, after being suspended earlier.

Event Number: 012
Event Name: Job was held
Event Description: The job has transitioned to the hold state. This might happen if the user applies the condor_hold command to the job.

Event Number: 013
Event Name: Job was released
Event Description: The job was in the hold state and is to be re-run.

Event Number: 014
Event Name: Parallel node executed
Event Description: A parallel universe program is running on a node.

Event Number: 015
Event Name: Parallel node terminated
Event Description: A parallel universe program has completed on a node.

Event Number: 016
Event Name: POST script terminated
Event Description: A node in a DAGMan work flow has a script that should be run after a job. The script is run on the submit host. This event signals that the post script has completed.

Event Number: 017
Event Name: Job submitted to Globus
Event Description: A grid job has been delegated to Globus (version 2, 3, or 4). This event is no longer used.

Event Number: 018
Event Name: Globus submit failed
Event Description: The attempt to delegate a job to Globus failed.

Event Number: 019
Event Name: Globus resource up
Event Description: The Globus resource that a job wants to run on was unavailable, but is now available. This event is no longer used.

Event Number: 020
Event Name: Detected Down Globus Resource
Event Description: The Globus resource that a job wants to run on has become unavailable. This event is no longer used.

Event Number: 021
Event Name: Remote error
Event Description: The condor_starter (which monitors the job on the execution machine) has failed.

Event Number: 022
Event Name: Remote system call socket lost
Event Description: The condor_shadow and condor_starter (which communicate while the job runs) have lost contact.

Event Number: 023
Event Name: Remote system call socket reestablished
Event Description: The condor_shadow and condor_starter (which communicate while the job runs) have been able to resume contact before the job lease expired.

Event Number: 024
Event Name: Remote system call reconnect failure
Event Description: The condor_shadow and condor_starter (which communicate while the job runs) were unable to resume contact before the job lease expired.

Event Number: 025
Event Name: Grid Resource Back Up
Event Description: A grid resource that was previously unavailable is now available.

Event Number: 026
Event Name: Detected Down Grid Resource
Event Description: The grid resource that a job is to run on is unavailable.

Event Number: 027
Event Name: Job submitted to grid resource
Event Description: A job has been submitted, and is under the auspices of the grid resource.

Event Number: 028
Event Name: Job ad information event triggered.
Event Description: Extra job ClassAd attributes are noted. This event is written as a supplement to other events when the configuration parameter EVENT_LOG_JOB_AD_INFORMATION_ATTRS is set.

Event Number: 029
Event Name: The job's remote status is unknown
Event Description: No updates of the job's remote status have been received for 15 minutes.

Event Number: 030
Event Name: The job's remote status is known again
Event Description: An update has been received for a job whose remote status was previous logged as unknown.

Event Number: 031
Event Name: Job stage in
Event Description: A grid universe job is doing the stage in of input files.

Event Number: 032
Event Name: Job stage out
Event Description: A grid universe job is doing the stage out of output files.

Event Number: 033
Event Name: Job ClassAd attribute update
Event Description: A Job ClassAd attribute is changed due to action by the condor_schedd daemon. This includes changes by condor_prio.

Event Number: 034
Event Name: Pre Skip event
Event Description: For DAGMan, this event is logged if a PRE SCRIPT exits with the defined PRE_SKIP value in the DAG input file. This makes it possible for DAGMan to do recovery in a workflow that has such an event, as it would otherwise not have any event for the DAGMan node to which the script belongs, and in recovery, DAGMan's internal tables would become corrupted.
"""
    doc_str = get_event_information.__doc__.split('\n')
    for i, line in enumerate(doc_str):
        match = re.match(r"Event Number: ([0-9]{3})", line)
        if match:
            if int(match[1]) == int(event_id):
                return doc_str[i:i+3]
    # else
    return ["This event number does not exist", "Valid event numbers range from 000 to 034"]


# reads all information, but returns them in two lists
def read_condor_log(file):
    """

    reads a given HTCondor std_log file and separates the information in two lists,
    for further and easier access

    :param file:    a HTCondor std_log file

    :raises: :class:'FileNotFoundError': if open does not work

    :return:    (job_event, job_event_information)
                a list of job_events and a list of job_event-relevant information
                these are related by the same index like:
                job_event[1]-> relevant information: job_event_information[1]
    """

    log_job_events = list()  # saves the job_events
    job_event_information = list()  # saves the information for each job event, if there are any
    temp_job_event_list = list()  # just a temporary list, gets inserted into job_event_information

    with open(file) as log_file:
        # for every line in the log_file
        for line in log_file:

            line = line.rstrip("\n")  # remove newlines
            match_log\
                = re.match(r"([0-9]{3})"  # matches event number
                           r" (\([0-9]+.[0-9]+.[0-9]{3}\))"  # matches clusterid and process id
                           r" ([0-9]{2}/[0-9]{2})"  # matches date
                           r" ([0-9]{2}:[0-9]{2}:[0-9]{2})"  # matches time
                           r"((?: \w+)*)."  # matches the job event name
                           r"(.*)", line)  # matches further information
            if match_log:
                job_event_number = match_log[1]  # job event number, can be found in job_description.txt
                clusterid_procid_inf = match_log[2]  # same numbers, that make the name of the file
                date = match_log[3]  # filter the date in the form Month/Day in numbers
                time = match_log[4]  # filter the time
                job_event_name = match_log[5]  # filter the job event name
                job_relevant_inf = match_log[6]  # filter the job relevant information
                log_job_events.append([job_event_number, clusterid_procid_inf,
                                       date, time, job_event_name, job_relevant_inf])
                # print(job_event_number, clusterid_procid_inf, date, time, job_event_name, job_relevant_inf)
            else:
                # end of job event
                if "..." in line:
                    job_event_information.append(temp_job_event_list)
                    temp_job_event_list = list()  # clear list
                    continue
                # else
                #if re.match("[\t ]+", line):
                else:
                    temp_job_event_list.append(line)
    return log_job_events, job_event_information


def raise_value_error(message):
    raise ValueError(message)


# Todo: time seperated ( submitting / executing / terminating)
# Todo: Read HTCondor Errors that occurred
# Todo: for loop instead of single run
def log_to_dataframe(file):
    try:
        job_events, job_raw_information = read_condor_log(file)

        # submitted_date = datetime.datetime.strptime(job_events[0][2] + " " + job_events[0][3], "%m/%d %H:%M:%S")

        if job_events[-1][0].__eq__("005"):  # if the last job event is : Job terminated

            # calculate the runtime for the job
            executed_date = datetime.datetime.strptime(job_events[1][2] + " " + job_events[1][3], "%m/%d %H:%M:%S")
            terminating_date = datetime.datetime.strptime(job_events[-1][2] + " " + job_events[-1][3], "%m/%d %H:%M:%S")
            runtime = terminating_date - executed_date  # calculation of the time runtime

            match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job_events[1][5])
            if match_host:
                host = match_host[1]
                # if resolve ip to hostname, change the ip to cpu: last number
                if resolve_ip_to_hostname:
                    host = "cpu node: " + host.split('.')[-1]
                port = match_host[2]
            else:
                logging.exception("Host and port haven't been matched correctly")
                print(
                    red + "your log file has faulty values for the host ip make sure it's a valid IPv4" + back_to_default)

            # make a fancy design for the job_information
            job_labels = ["Executing on Host", "Port", "Runtime"]  # holds the labels
            job_information = [host, port, runtime]  # holds the related job information

            # filter the termination state ...
            if True:
                # check if termination state is normal
                termination_state_inf = job_raw_information[-1][0]
                match_termination_state = re.match(r"[\t ]+\([0-9]+\) ((?:\w+ )*)", termination_state_inf)
                if match_termination_state:
                    job_labels.append("Termination State")

                    if "Normal termination" in match_termination_state[1]:
                        job_information.append(match_termination_state[1])
                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", termination_state_inf)
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)
                    elif "Abnormal termination" in match_termination_state[1]:
                        job_information.append(red+match_termination_state[1]+back_to_default)

                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", termination_state_inf)
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)

                        termination_desc = list()
                        # append the reason for the Abnormal Termination
                        for desc in job_raw_information[-1][1:]:
                            if re.match(r"[\t ]+\(0\)",desc):
                                termination_desc.append(desc[5:])
                            else:
                                break

                        if not len(termination_desc) == 0:
                            job_labels.append("Description")
                            job_information.append("\n".join(termination_desc))
                        else:
                            logging.debug(f"No termination description for {file} found")

                    else:
                        logging.debug(f"Don't know this Termination State: {match_termination_state[1]} yet")

                else:
                    print(red + f"Termination error in HTCondor log file: {file}" + back_to_default)

            # now put everything together in a table
            job_df = pd.DataFrame({
                "Description": job_labels,
                "Values": job_information
            })

            # these where the job information now focus on the used resources

            relevant_str = "\n".join(job_raw_information[-1])  # job information of the last job (Job terminated)

            # next part removes not useful lines
            if True:  # just for readability
                # remove unnecessary lines
                lines = relevant_str.splitlines()
                # while not lines[0].startswith("\tPartitionable"):
                while not re.match(r"[\t ]+Partitionable", lines[0]):
                    lines.remove(lines[0])

                lines.remove(lines[0])
                partitionable_res = lines
                # done, partitionable_resources contain now only information about used resources

            # match all resources
            # ****************************************************************************************************
            match = re.match(r"[\t ]+Cpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+)", partitionable_res[0])
            if match:
                cpu_usage, cpu_request, cpu_allocated = match[1], match[2], match[3]
            else:
                raise_value_error("Something went wrong reading the cpu information")

            match = re.match(r"[\t ]+Disk \(KB\) *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[1])
            if match:
                disk_usage, disk_request, disk_allocated = match[1], match[2], match[3]
            else:
                raise raise_value_error("Something went wrong reading the disk information")

            # check if the log has gpu information in between
            gpu_match_index = 2  # can be ignored if gpu is given memory has to access the next index
            gpu_found = False  # easier to check to add information in the DataFrame
            match = re.match(r"[\t ]+Gpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+) \"(.*)\"",
                             partitionable_res[gpu_match_index])
            if match:
                gpu_usage, gpu_request, gpu_allocated, gpu_name = match[1], match[2], match[3], match[4]
                gpu_match_index += 1
                gpu_found = True

            match = re.match(r"[\t ]+Memory \(MB\)  *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[gpu_match_index])
            if match:
                memory_usage, memory_request, memory_allocated = match[1], match[2], match[3]
            else:
                raise_value_error("Something went wrong reading the memory information")
            # ****************************************************************************************************

            # list of resources and their labels
            resource_labels = ["Cpu", "Disk", "Memory"]
            usage = [cpu_usage, disk_usage, memory_usage]
            requested = [cpu_request, disk_request, memory_request]
            allocated = [cpu_allocated, disk_allocated, memory_allocated]

            if gpu_found:
                resource_labels.append("Gpus")
                usage.append(gpu_usage)
                requested.append(gpu_request)
                allocated.append(gpu_allocated)

            # Error handling: change empty values to NaN in the first column
            for i in range(3):
                if usage[i] == "":
                    usage[i] = np.nan
                if requested[i] == "":
                    requested[i] = np.nan
                if allocated[i] == "":
                    allocated[i] = np.nan

            # put the data in the DataFrame
            res_df = pd.DataFrame({
                "Rescources": resource_labels,
                "Usage": usage,
                "Requested": requested,
                # "Allocated": allocated
            })

            # if the user wants allocated resources then add it to the DataFrame as well
            temp_index = 3
            if not ignore_allocated_resources:
                res_df.insert(temp_index, "Allocated", allocated)
                temp_index += 1
            if gpu_found:
                res_df.insert(temp_index, "Assigned", ["", "", "", gpu_name])

        elif job_events[-1][0].__eq__("009"):  # job aborted

            # calculate the runtime for the job
            if job_events[-1].__ne__(job_events[0]):
                executed_date = datetime.datetime.strptime(job_events[1][2] + " " + job_events[1][3], "%m/%d %H:%M:%S")
                terminating_date = datetime.datetime.strptime(job_events[-1][2] + " " + job_events[-1][3], "%m/%d %H:%M:%S")
                runtime = terminating_date - executed_date  # calculation of the time runtime
            else:
                runtime = datetime.timedelta(0)

            user = ((job_raw_information[-1][0]).split(" ")[-1])[:-1]
            job_df = pd.DataFrame({
                "Description": ['Aborted by:', 'Runtime'],
                "Values": [user, runtime]
            })
            res_df = None
            logging.debug(f"{file}: Job arboted by {user}")

    except NameError as err:
        logging.exception(err)
        print("The smart_output_logs method requires a " + std_log + " file as parameter")
    except FileNotFoundError or ValueError as err:
        logging.exception(err)
        print(red + str(err) + back_to_default)
    except IndexError as err:
        logging.debug(f"Index Error with {file}: ")
        logging.exception(err)
    except Exception as err:
        logging.debug(err)
    # finally
    else:
        return job_df, res_df


def smart_output_logs(file):
    """
    reads a given HTCondor .log file with the read_condor_logs() function

    :param file:    a HTCondor .log file
    :param header:  Shows the header of the columns
    :param index:   Shows the index of the rows

    :return:        (output_string)
                    an output string that shows information like:
                    The job procedure of : ../logs/454_199.log
                    +-------------------+--------------------+
                    | Executing on Host |      10.0.9.1      |
                    |       Port        |        9618        |
                    |      Runtime      |      1:12:20       |
                    | Termination State | Normal termination |
                    |   Return Value    |         0          |
                    +-------------------+--------------------+
                    +--------+-------+-----------+-----------+
                    |        | Usage | Requested | Allocated |
                    +--------+-------+-----------+-----------+
                    |  Cpu   | 0.30  |     1     |     1     |
                    |  Disk  |  200  |    200    |  3770656  |
                    | Memory |   3   |     1     |    128    |
                    +--------+-------+-----------+-----------+

    """
    try:

        job_df, res_df = log_to_dataframe(file)

        output_string = ""

        # Todo: csv style one line
        if to_csv:
            pass
        else:
            global border_str
            output_string += green + "The job procedure of : " + file + back_to_default + "\n"
            border_str = "-" * len(output_string) + "\n"

            if not isinstance(res_df, type(None)):  # make sure res_df is not None

                # reuested vs used disk RAM
                if float(res_df.iloc[1, 1]) / float(res_df.iloc[1, 2]) > bad_usage:
                    res_df.iloc[1, 1] = red+res_df.iloc[1, 1]+back_to_default  # changed color to red
                elif float(res_df.iloc[1, 1]) / float(res_df.iloc[1, 2]) < low_usage:
                    res_df.iloc[1, 1] = yellow+res_df.iloc[1, 1]+back_to_default  # changed color to yellow

                # requested vs used Memory
                if float(res_df.iloc[2, 1]) / float(res_df.iloc[2, 2]) > bad_usage:
                    res_df.iloc[2, 1] = red+res_df.iloc[2, 1]+back_to_default  # changed color to red
                elif float(res_df.iloc[2, 1]) / float(res_df.iloc[2, 2]) < low_usage:
                    res_df.iloc[2, 1] = yellow+res_df.iloc[2, 1]+back_to_default  # changed color to yellow

            if not ignore_job_information:
                output_string += tabulate(job_df, tablefmt=table_format, showindex=False) + "\n"
            if not ignore_resources:
                output_string += tabulate(res_df, headers='keys', tablefmt=table_format, showindex=False) + "\n"

    except NameError as err:
        logging.exception(err)
        print("The smart_output_logs method requires a "+std_log+" file as parameter")
    except Exception as err:
        logging.exception(err)
        print(red+str(err)+back_to_default)
    # finally
    else:
        return output_string


# Todo: maybe less information
# just read .err files content and return it as a string
def read_condor_error(file):
    """
    Reads a HTCondor .err file and returns its content

    :param file: a HTCondor .err file

    :raises: :class:'NameError': The param file was no .err file
             :class:'FileNotFoundError': if open does not work

    :return: file content

    """
    if not file.endswith(std_err):
        raise NameError("The read_condor_error method is only for "+std_err+" files")

    err_file = open(file)
    return "".join(err_file.readlines())


# just read .out files content and return it as a string
def read_condor_output(file):
    """
        Reads a HTCondor .out file and returns its content

        :param file: a HTCondor .out file

        :raises: :class:'NameError': The param file was no .out file
                 :class:'FileNotFoundError': if open does not work

        :return: file content

        """
    if not file.endswith(std_out):
        raise NameError("The read_condor_output method is only for "+std_out+" files")

    out_file = open(file)
    return "".join(out_file.readlines())


# Todo:
def filter_for_host(ip):
    """
    this function is supposed to filter a given ip for it's representive url like juseless.inm7.de:cpu1
    :return:
    """


def smart_output_error(file):
    """

    :param file: a HTCondor .err file
    :return: the content of the given file as a string
    """

    # Todo:
    # - errors from htcondor are more important (std.err output)
    # - are there errors ? true or false -> maybe print those in any kind of fromat
    # - maybe check for file size !!!
    # - is the file size the same ? what is normal / different errors (feature -> for  -> later)

    output_string = ""
    try:
        error_content = read_condor_error(file)
        for line in error_content.split("\n"):
            if "err" in line.lower() and show_std_errors:
                output_string += red + line + back_to_default + "\n"
            elif "warn" in line.lower() and show_std_warnings:
                output_string += yellow + line + back_to_default + "\n"

    except NameError as err:
        logging.exception(err)
        print("The smart_output_error method requires a "+std_err+" file as parameter")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)"+std_err, relevant[1])
        print(yellow + "There is no related {0} file: {1} in the directory:{2}\n'{3}'\n"
              " with the prefix: {4}{5}"
              .format(std_err, relevant[1], cyan, os.path.abspath(relevant[0]), match[1], back_to_default))
    except TypeError as err:
        logging.exception(err)
        print(red+str(err)+back_to_default)
    finally:
        return output_string


def smart_output_output(file):
    """
    Todo: filter the output ?
    :param file: a HTCondor .out file
    :return: the content of that file as a string
    """

    output_string = ""
    try:
        output_string = read_condor_output(file)
    except NameError as err:
        logging.exception(err)
        print("The smart_output_output method requires a "+std_out+" file as parameter")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)"+std_out, relevant[1])
        print(yellow + "There is no related {0} file: {1} in the directory:{2}\n'{3}'\n"
                       " with the prefix: {4}{5}"
              .format(std_out, relevant[1], cyan, os.path.abspath(relevant[0]), match[1], back_to_default))

    finally:
        return output_string


def smart_manage_all(file):
    """
    Combine all informations, that the user wants for a given job_spec_id like 398_31.
    -the first layer represents the HTCondor .log file
    -the second layer represents the HTCondor .err file
    -the third layer represents the HTCondor .out file

    information will be put together regarding the global booleans set by the user.
    job_spec_id, is the ClusterID_ProcessID of a job executed by HTCondor like job_4323_21 or 231_0

    :param file: file name without endling like job4323_1, job4323_1.<err|out|log> will cut the end off
    :return: a string that combines HTCondor log/err and out files and returns it
    """
    try:
        if std_log.__ne__(""):
            job_spec_id = file[:-len(std_log)]
        else:
            job_spec_id = file

        output_string = smart_output_logs(job_spec_id + std_log)  # normal smart_output of log files

        if show_std_errors:  # show errors ?
            output_string += smart_output_error(job_spec_id + std_err)

        if show_std_output:  # show output content ?
            output_string += smart_output_output(job_spec_id + std_out)
    except Exception as err:
        logging.exception(err)
        print(red+str(err)+back_to_default)
        sys.exit(1)
    else:
        return output_string


# Todo: change for no specified type of log error and output files
# Todo: err and output not visible ???
def output_given_logs():
    output_string = ""
    current_path = os.getcwd()
    # go through all given logs and check for each if it is a directory or file and if std_log was missing or not
    for i, file in enumerate(files):

        # for the * operation it should skip std_err and std_out files
        if file.endswith(std_err) and not std_err.__eq__("") or file.endswith(std_out) and not std_out.__eq__(""):
            continue

        # else check if file is a valid file
        if os.path.isfile(file) or os.path.isfile(current_path + "/" + file):
            output_string += smart_manage_all(file)

        # if that did not work try std_log at the end of that file
        elif not file.endswith(std_log):
            new_path = file + std_log
            # is this now a valid file ?
            if os.path.isfile(new_path) or os.path.isfile(current_path + "/" + new_path):
                output_string += smart_manage_all(new_path)
            # no file or directory found, even after manipulating the string
            else:
                logging.error("No such file with that name or prefix: {0}".format(file))
                output_string += red+"No such file with that name or prefix: {0}".format(file)+back_to_default

        # The given .log file was not found
        else:
            output_string += red + "No such file: {0}".format(file) + back_to_default
            logging.error(f"No such file: {file}")

        # don't change output if csv style is wanted
        # Todo:
        if to_csv:
            pass

        elif not i == len(files)-1:
            output_string += "\n" + border_str + "\n"  # if empty it still contains a newline

    if output_string == "":
        print(yellow+"Nothing found to analyse, please use htcompact --help for help"+back_to_default)

    return output_string


# search for config file ( UNIX BASED )
# Todo: Test
#       Change some varibales
def load_config():
    """

    Reads config file and changes global parameter by its configuration

    :return: False if not found, True if found and
    """

    script_name = sys.argv[0]
    if script_name.endswith(".py"):
        script_name = sys.argv[0][:-3]  # scriptname without .py
    config = configparser.ConfigParser()
    # search for the given file, first directly, then in /etc and then in ~/.config/htcompact/
    found_config = False

    print(cyan, end="")
    arguments = sys.argv[1:]
    config_paths = ["env/"+config_file, config_file]  # default config, search in env and current directroy if possible
    arguments.extend(config_paths)  # extend to arguments

    # run through every given argument and try to determine if it's a valid config file
    # Todo: maybe search in the current directory for config files
    for file in arguments:
        # try to find the given file in the current directory, in /etc or in ~/.config/htcompact/
        try:
            if os.path.isfile(file) and config.read(file):
                logging.debug(f"Load config from the file: {file}")
                print(f"Load config from the file: {file}")
                found_config = True
                arguments.remove(file)
                logging.debug("Removed {0} from arguments".format(file))
            # try to find config file in /etc
            elif os.path.isfile(f"/etc/{file}") and config.read(f"/etc/{file}"):
                logging.debug(f"Load config from: /etc/{file}")
                print(f"Load config from: /etc/{file}")
                found_config = True
                arguments.remove(file)
                logging.debug("Removed {0} from arguments".format(file))
            # try to find config file in ~/.config/script_name/
            elif os.path.isfile(f"~/.config/{script_name}/{file}") \
                    and config.read(f"~/.config/{script_name}/{file}"):
                logging.debug(f"Load config from: ~/.config/{script_name}/{file}")
                print(f"Load config from: ~/.config/{script_name}/{file}")
                found_config = True
                arguments.remove(file)
                logging.debug("Removed {0} from arguments".format(file))
            else:
                logging.debug(f"{file} not a file or directory")

            if found_config:
                break

        # File has no readable format for the configparser, probably because it's not a config file
        except configparser.MissingSectionHeaderError:
            continue

    try:
        arguments.remove(config_file)  # remove again, if other config file was found
    except ValueError:
        pass
    try:
        arguments.remove("env/"+config_file)  # remove again if other config file was found
    except ValueError:
        pass

    sys.argv[1:] = arguments

    print(back_to_default, end="")

    # prepare output if not and break if not found
    if not found_config:
        print(f"{yellow}No valid config file given and no default "
              f"{config_file} file found in: \n"
              f"({os.getcwd()}, ~/.config/htcompact/, /etc/) \n"
              f"using default settings{back_to_default}")
        return False

    try:
        # all sections in the config file
        sections = config.sections()

        # now try filter the config file for all available parameters

        # all documents like files, etc.
        if 'documents' in sections:
            global files

            if 'files' in config['documents']:
                files = config['documents']['files'].split(" ")
                logging.debug(f"Changed document files to {files}")

        # all formats like the table format
        if 'formats' in sections:
            global table_format
            if 'table_format' in config['formats']:
                table_format = config['formats']['table_format']
                logging.debug(f"Changed default table_format to: {table_format}")

        # all basic HTCondor file endings like .log, .err, .out
        if 'htc-files' in sections:
            global std_log, std_err, std_out

            if 'stdlog' in config['htc-files']:
                std_log = config['htc-files']['stdlog']
                logging.debug(f"Changed default for HTCondor .log files to: {std_log}")

            if 'stderr' in config['htc-files']:
                std_err = config['htc-files']['stderr']
                logging.debug(f"Changed default for HTCondor .err files to: {std_err}")

            if 'stdout' in config['htc-files']:
                std_out = config['htc-files']['stdout']
                logging.debug(f"Changed default for HTCondor .out files to: {std_out}")

        # if show variables are set for further output
        if 'show-more' in sections:
            global show_std_output, show_std_warnings, show_std_errors  # sources to show

            if 'show_errors' in config['show-more']:
                show_std_errors = config['show-more']['show_errors'].lower() in accepted_states
                logging.debug(f"Changed default show_errors to: {show_std_errors}")

            if 'show_output' in config['show-more']:
                show_std_output = config['show-more']['show_output'].lower() in accepted_states
                logging.debug(f"Changed default show_output to: {show_std_output}")

            if 'show_warnings' in config['show-more']:
                show_std_warnings = config['show-more']['show_warnings'].lower() in accepted_states
                logging.debug(f"Changed default show_warnings to: {show_std_warnings}")

        # what information should to be ignored
        if 'ignore' in sections:
            global ignore_resources, ignore_job_information, ignore_allocated_resources  # sources to ignore

            if 'ignore_resources' in config['ignore']:
                ignore_resources = config['ignore']['ignore_resources'].lower() in accepted_states
                logging.debug(f"Changed default ignore_resources to: {ignore_resources}")
            if 'ignore_job_information' in config['ignore']:
                ignore_job_information = config['ignore']['ignore_job_information'].lower() in accepted_states
                logging.debug(f"Changed default ignore_job_information to: {ignore_job_information}")
            if 'ignore_allocated_resources' in config['ignore']:
                ignore_allocated_resources = config['ignore']['ignore_allocated_resources'].lower() \
                                             in accepted_states
                logging.debug(f"Changed default ignore_allocated_res to: {ignore_allocated_resources}")

        if 'thresholds' in sections:
            global low_usage, bad_usage
            if 'low_usage' in config['thresholds']:
                low_usage = float(config['thresholds']['low_usage'])
                logging.debug(f"Changed default low_usage to: {low_usage}")
            if 'bad_usage' in config['thresholds']:
                bad_usage = float(config['thresholds']['bad_usage'])
                logging.debug(f"Changed default bad_usage to: {bad_usage}")

        # Todo: reverse DNS-Lookup etc.
        if 'features' in sections:
            global resolve_ip_to_hostname, reverse_dns_lookup, summarise, analyse, to_csv  # extra parameters
            if 'resolve_ip_to_hostname' in config['features']:
                resolve_ip_to_hostname = config['features']['resolve_ip_to_hostname'].lower() in accepted_states
                logging.debug(f"Changed default resolve_ip_to_hostname to: {resolve_ip_to_hostname}")
            if 'summarise' in config['features']:
                summarise = config['features']['summarise'].lower() in accepted_states
                logging.debug(f"Changed default summarise to: {summarise}")
            if 'analyse' in config['features']:
                analyse = config['features']['analyse'].lower() in accepted_states
                logging.debug(f"Changed default analyse to: {analyse}")
            if 'to_csv' in config['features']:
                to_csv = config['features']['to_csv'].lower() in accepted_states
                logging.debug(f"Changed default to_csv to: {to_csv}")

        return True

    except KeyError as err:
        logging.exception(err)
        return False


# in order that plotille has nothing like a int converter,
# I have to set it up manually to show the y - label in number format
def _int_formatter(val, chars, delta, left=False):
    """
    Usage of this is shown here:
    https://github.com/tammoippen/plotille/issues/11

    :param val:
    :param chars:
    :param delta:
    :param left:
    :return:
    """
    align = '<' if left else ''
    return '{:{}{}d}'.format(int(val), align, chars)


# Todo: explicitly Job event 000, 001, 005, 006 and 009 are filtered and all other job events that throw errors
#       until now there is not the need for other filters but it might be needed to extend this fucntion
def analyse_logs(log_files):

    if len(log_files) == 0:
        return "No files to analyse"
    elif len(log_files) > 1 and sys.stdout.isatty():
        print("More than one file is given, this mode is meant to be used for single job analysis.\n"
              "This will change nothing, but you should rather do it just for a file one by one")
        x = input("Want to continue (y/n): ")
        if x != "y":
            return "Process stoped"

    output_string = ""

    for file in log_files:

        try:
            job_events, job_raw_information = read_condor_log(file)
        except NameError as err:
            logging.exception(err)
            print("The smart_output_logs method requires a " + std_log + " file as parameter")

        # initialise with empty values
        submitted_date, executed_date, terminated_date = None, None, None
        ram_histroy = list()
        occurred_errors = list()
        job_df, res_df = pd.DataFrame(columns=["Description", "Values"]), pd.DataFrame({})

        logging.debug(f"Analysing the HTCondor log file: {file}")
        output_string += green + "Job analysis of: " + file + back_to_default + "\n"

        for job, job_inf in zip(job_events, job_raw_information):
            job_id = job[0]

            # job submitted
            if job_id == "000":
                submitted_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job[5])
                if match_host:
                    from_host = match_host[1]
                    # port = match_host[2]
                    job_df = job_df.append(pd.DataFrame({"Description": "Submitted from host",
                                                         "Values": [from_host]}))

            # job executing
            elif job_id == "001":
                executed_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job[5])
                if match_host:
                    host = match_host[1]
                    # if resolve ip to hostname, change the ip to cpu: last number
                    if resolve_ip_to_hostname:
                        host = "cpu node: " + host.split('.')[-1]
                    port = match_host[2]

                    job_df = job_df.append(pd.DataFrame({"Description": ["Executing on Host", "Port"],
                                                         "Values": [host, port]}))
                else:
                    logging.exception("Host and port haven't been matched correctly")
                    print(red + "your log file has faulty values for the host ip,"
                                " make sure it's a valid IPv4" + back_to_default)

            # job terminated
            elif job_id == "005":  # if the last job event is : Job terminated
                terminated_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                # make a fancy design for the job_information
                job_labels = []  # holds the labels
                job_information = []  # holds the related job information

                # check if termination state is normal or abnormal
                match_termination_state = re.match(r"[\t ]+\([0-9]+\) ((?:\w+ )*)", job_inf[0])
                if match_termination_state:
                    job_labels.append("Termination State")  # append the termination state

                    # Normal termination
                    if "Normal termination" in match_termination_state[1]:
                        job_information.append(match_termination_state[1])
                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", job_inf[0])
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)

                    # Abnormal Termination
                    elif "Abnormal termination" in match_termination_state[1]:
                        job_information.append(red+match_termination_state[1]+back_to_default)

                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", job_inf[0])
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)

                        termination_desc = list()
                        # append the reason for the Abnormal Termination
                        for desc in job_raw_information[-1][1:]:
                            if re.match(r"[\t ]+\(0\)", desc):
                                termination_desc.append(desc[5:])
                            else:
                                break

                        if not len(termination_desc) == 0:
                            job_labels.append("Description")
                            job_information.append("\n".join(termination_desc))
                        else:
                            logging.debug(f"No termination description for {file} found")

                    else:
                        logging.debug(f"Don't know this Termination State: {match_termination_state[1]} yet")

                else:
                    print(red + f"Termination error in HTCondor log file: {file}" + back_to_default)

                # now put everything together in a table
                job_temp_df = pd.DataFrame({
                    "Description": job_labels,
                    "Values": job_information
                })

                job_df = job_df.append(job_temp_df)

                # these where the job information now focus on the used resources
                relevant_str = "\n".join(job_raw_information[-1])  # job information of the last job (Job terminated)

                # next part removes not useful lines
                if True:  # just for readability
                    # remove unnecessary lines
                    lines = relevant_str.splitlines()
                    # while not lines[0].startswith("\tPartitionable"):
                    while not re.match(r"[\t ]+Partitionable", lines[0]):
                        lines.remove(lines[0])

                    lines.remove(lines[0])
                    partitionable_res = lines
                    # done, partitionable_resources contain now only information about used resources

                # match all resources
                # ****************************************************************************************************
                match = re.match(r"[\t ]+Cpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+)", partitionable_res[0])
                if match:
                    cpu_usage, cpu_request, cpu_allocated = match[1], match[2], match[3]
                else:
                    raise_value_error("Something went wrong reading the cpu information")

                match = re.match(r"[\t ]+Disk \(KB\) *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[1])
                if match:
                    disk_usage, disk_request, disk_allocated = match[1], match[2], match[3]
                else:
                    raise raise_value_error("Something went wrong reading the disk information")

                # check if the log has gpu information in between
                gpu_match_index = 2  # can be ignored if gpu is given memory has to access the next index
                gpu_found = False  # easier to check to add information in the DataFrame
                match = re.match(r"[\t ]+Gpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+) \"(.*)\"",
                                 partitionable_res[gpu_match_index])
                if match:
                    gpu_usage, gpu_request, gpu_allocated, gpu_name = match[1], match[2], match[3], match[4]
                    gpu_match_index += 1
                    gpu_found = True

                match = re.match(r"[\t ]+Memory \(MB\)  *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[gpu_match_index])
                if match:
                    memory_usage, memory_request, memory_allocated = match[1], match[2], match[3]
                else:
                    raise_value_error("Something went wrong reading the memory information")
                # ****************************************************************************************************

                # list of resources and their labels
                resource_labels = ["Cpu", "Disk (KB)", "Memory (MB)"]
                usage = [cpu_usage, disk_usage, memory_usage]
                requested = [cpu_request, disk_request, memory_request]
                allocated = [cpu_allocated, disk_allocated, memory_allocated]

                if gpu_found:
                    resource_labels.append("Gpus")
                    usage.append(gpu_usage)
                    requested.append(gpu_request)
                    allocated.append(gpu_allocated)

                # Error handling: change empty values to NaN in the first column
                for i in range(3):
                    if usage[i] == "":
                        usage[i] = np.nan
                    if requested[i] == "":
                        requested[i] = np.nan
                    if allocated[i] == "":
                        allocated[i] = np.nan

                # put the data in the DataFrame
                res_df = pd.DataFrame({
                    "Rescources": resource_labels,
                    "Usage": usage,
                    "Requested": requested,
                    # "Allocated": allocated
                })
                # if the user wants allocated resources then add it to the DataFrame as well
                temp_index = 3
                if not ignore_allocated_resources:
                    res_df.insert(temp_index, "Allocated", allocated)
                    temp_index += 1
                if gpu_found:
                    res_df.insert(temp_index, "Assigned", ["", "", "", gpu_name])

            # job image size updated
            elif job_id == "006":
                date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                image_size = job[5]
                memory_usage = re.match("[\t ]+([0-9]+)", job_inf[0])[1]
                resident_set_size = re.match("[\t ]+([0-9]+)", job_inf[1])[1]
                ram_histroy.append([date, int(image_size), int(memory_usage), int(resident_set_size)])

            # job aborted
            elif job_id == "009":
                terminated_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                aborted_by_user = ((job_inf[0]).split(" ")[-1])[:-1]
                # job_df = job_df.append(pd.DataFrame({"Description": "Job was aborted by the user",
                #                                      "Values": [yellow+aborted_by_user+back_to_default]}))
                occurred_errors.append([job_id, terminated_date, "Terminated by user: "+aborted_by_user])

            else:
                # search for errors
                timestamp = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                error_strings = ["error", "err", "warning", "warn", "exception"]
                for job_desc in job_inf:
                    for err in error_strings:
                        if err in job_desc.lower():
                            occurred_errors.append([job_id, timestamp,
                                                    re.sub(' \t', '', job_desc)])
                            break  # avoid duplicates

        # managing the time information
        time_dict = dict()
        waiting_time = None
        if submitted_date:
            time_dict["Submission date"] = submitted_date.strftime("%m/%d %H:%M:%S")
        if executed_date:
            time_dict["Execution date"] = executed_date.strftime("%m/%d %H:%M:%S")
            if submitted_date:
                waiting_time = executed_date - submitted_date
        if terminated_date:
            time_dict["Termination date"] = terminated_date.strftime("%m/%d %H:%M:%S")
            if waiting_time:
                time_dict["Waiting time"] = waiting_time
            if executed_date:
                runtime = terminated_date - executed_date
                time_dict["Execution runtime"] = runtime
            if submitted_date:
                total_time = terminated_date - submitted_date
                time_dict["Total runtime"] = total_time
        elif waiting_time:
            time_dict["Termination date"] = "No valid termination occurred"
            time_dict["Waiting time"] = waiting_time

        if len(time_dict) > 0:
            output_string += magenta + "Dates and times" + back_to_default + '\n'
            time_df = pd.DataFrame(time_dict.values(), index=time_dict.keys())
            output_string += tabulate(time_df) + "\n\n"

        if not job_df.empty:
            output_string += magenta + "Execution details" + back_to_default + '\n'
            output_string += tabulate(job_df, showindex=False) + '\n\n'
        if not res_df.empty:
            output_string += magenta + "Resource table" + back_to_default + '\n'
            output_string += tabulate(res_df, showindex=False, headers="keys", tablefmt=table_format) + '\n\n'

        # show HTCondor errors
        if len(occurred_errors) > 0:
            # Todo: is this covering all cases ?
            event_numbers = []
            time_list = []
            err_reason = []

            for err in occurred_errors:
                event_numbers.append(err[0])
                time_list.append(err[1].strftime("%m/%d %H:%M:%S"))
                # if the line is to long, split it
                if len(err[2]) > 50:
                    split = int(len(err[2])/2)
                    while err[2][split] != ' ' and split < len(err[2])-1:
                        split += 1
                    if split > len(err[2])-2:  # if no space was found, change back to normal
                        split = int(len(err[2]) / 2)
                    err_reason.append(err[2][0:split] + '\n' + err[2][split:])
                else:
                    err_reason.append(err[2])

            err_df = pd.DataFrame({
                "Time": time_list,
                "Event Number": event_numbers,
                "Reason": err_reason
            })

            output_string += magenta + "Occurred HTCondor errors" + back_to_default + '\n'
            output_string += tabulate(err_df, showindex=False, headers='keys') + '\n\n'

        # managing the ram history
        if len(ram_histroy) > 0:
            output_string += magenta + "Memory usage over time in MB" + back_to_default + '\n'
            if len(ram_histroy) > 1:
                np_ram = np.array(ram_histroy)
                ram = np_ram[:, 2]  # second column which is memory usage
                dates = np_ram[:, 0]  # first column

                fig = Figure()
                fig.width = 55
                fig.height = 15
                fig.set_x_limits(min_=min(dates))
                fig.set_y_limits(min_=min(ram))
                fig.y_label = "Usage"
                fig.x_label = "Time"

                # this will use the self written function _num_formatter, to convert the y-label to int values
                fig.register_label_formatter(float, _int_formatter)
                fig.plot(dates, ram, lc='green', label="Continuous Graph")  # Todo: implement a way to show thresholds
                fig.scatter(dates, ram, lc='red', label="Single Values")

                if not sys.stdout.isatty():  # if redirected, the Legend is useless
                    output_string += fig.show() + "\n"
                else:
                    output_string += fig.show(legend=True) + "\n"
            else:
                single_date = ram_histroy[0][0].strftime("%m/%d %H:%M:%S")
                single_ram = ram_histroy[0][2]
                output_string += f"Single memory update found:\n" \
                    f"Memory usage on the {single_date} was updatet to {single_ram} MB\n"

            output_string += "\n"

    return output_string


# Todo: what means sucessful executed ?
def summarise_logs(log_files):
    """
    Summarises all used resources and the runtime in total and average

    Runs threw the log files via the log_to_dataframe function

    :return:
    """

    valid_files = len(log_files)
    # no given files
    if valid_files == 0:
        return "No files to summarise"

    # allocated all diffrent datatypes, easier to handle
    output_string = ""

    aborted_files = 0
    normal_runtime = datetime.timedelta()
    aborted_runtime = datetime.timedelta()
    cpu_nodes = dict()

    total_cpu_usage = float(0)
    total_disk_usage = int(0)
    total_memory_usage = int(0)

    total_cpu_requested = int(0)
    total_disk_requested = int(0)
    total_memory_requested = int(0)

    total_cpu_allocated = int(0)
    total_disk_allocated = int(0)
    total_memory_allocated = int(0)

    # if gpu given
    gpu_found = False
    total_gpu_usage = float(0)
    total_gpu_requested = int(0)
    total_gpu_allocated = int(0)
    list_of_gpu_names = list()

    percent_mark = 10  # calculate the percentage of the running summarisation
    start_status = 50  # value that decides, when it's worth showing a status of the progress

    for i, file in enumerate(log_files):
        try:
            log = log_to_dataframe(file)

            log_description, log_resources = log[0], log[1]

            # if the Job was aborted, it still might have a runtime
            if log_description.at[0, 'Description'].__eq__("Aborted by:"):
                # print(f"The job described in {file} was aborted")
                aborted_runtime += log_description.at[1, 'Values']
                aborted_files += 1
                continue

            normal_runtime += log_description.at[2, 'Values']
            cpu = log_description.at[0, 'Values']
            if cpu in cpu_nodes:
                cpu_nodes[cpu][0] += 1
                cpu_nodes[cpu][1] += log_description.at[2, 'Values']
            else:
                cpu_nodes[cpu] = [1, log_description.at[2, 'Values']]

            total_cpu_usage += float(log_resources.at[0, 'Usage']) \
                if str(log_resources.at[0, 'Usage']).lower() != 'nan' else 0
            total_disk_usage += int(log_resources.at[1, 'Usage'])
            total_memory_usage += int(log_resources.at[2, 'Usage'])

            total_cpu_requested += int(log_resources.at[0, 'Requested'])
            total_disk_requested += int(log_resources.at[1, 'Requested'])
            total_memory_requested += int(log_resources.at[2, 'Requested'])

            if not ignore_allocated_resources:
                total_cpu_allocated += int(log_resources.at[0, 'Allocated'])
                total_disk_allocated += int(log_resources.at[1, 'Allocated'])
                total_memory_allocated += int(log_resources.at[2, 'Allocated'])

            status = round(((i+1)/valid_files)*100)
            if int(status/percent_mark) > 0 and valid_files > start_status:
                percent_mark += 10
                print(f"Status: {status}% of all files summarised")

            if 3 in log_resources.index:  # this means gpu information is given
                gpu_found = True
                total_gpu_usage += float(log_resources.at[3, 'Usage'])
                total_gpu_requested += int(log_resources.at[3, 'Requested'])
                if not ignore_allocated_resources:
                    total_gpu_allocated += int(log_resources.at[3, 'Allocated'])
                if log_resources.at[3, 'Assigned'] not in list_of_gpu_names:  # append new gpus
                    list_of_gpu_names.append(log_resources.at[3, 'Assigned'])

        # Error ocurres when Job was aborted
        except ValueError or KeyError as err:
            logging.exception(err)
            print(f"Summarisation error with {file}")
            continue

    n = valid_files - aborted_files  # calc diffrenc of successful executed jobs

    average_runtime = normal_runtime / n if n != 0 else normal_runtime
    average_runtime = datetime.timedelta(days=average_runtime.days, seconds=average_runtime.seconds)
    total_runtime = normal_runtime + aborted_runtime

    output_string += "-"*75 + "\n"

    output_string += f"{valid_files} valid HTCondor job files found\n"
    # it is more obvious, to see None when there was no time to detect
    if aborted_files > 0:
        output_string += f"{aborted_files} of the jobs were aborted\n\n"

    time_dict = dict()

    if normal_runtime != datetime.timedelta(0, 0, 0):
        time_dict["Total runtime of all successful jobs"] = normal_runtime
    if aborted_runtime != datetime.timedelta(0, 0, 0):
        # some jobs were aborted, diffrentiate between normal runtime, aborted runtime and total runtime
        time_dict["Total runtime of all aborted jobs"] = aborted_runtime
        time_dict["Total runtime"] = total_runtime
    if average_runtime:
        time_dict["Average runtime per job\n(only successful executed jobs)"] = average_runtime

    runtime_df = pd.DataFrame(time_dict.items())
    output_string += tabulate(runtime_df, showindex=False, tablefmt='simple') + "\n\n"

    if n != 0:  # do nothing, if all valid jobs were aborted
        df_total = pd.DataFrame({
            "Resources": ['Total Cpu', 'Total Disk (KB)', 'Total Memory (MB)'],
            "Usage": [str(round(total_cpu_usage, 2)),
                      str(total_disk_usage),
                      str(total_memory_usage)],  # necessary
            "Requested": [total_cpu_requested,
                          total_disk_requested,
                          total_memory_requested]

        })
        df_average = pd.DataFrame({
            "Resources": ['Average Cpu', 'Avergae Disk (KB)', 'Average Memory (MB)'],
            "Usage": [round(total_cpu_usage / n, 2),
                      round(total_disk_usage / n, 2),
                      round(total_memory_usage / n, 2)],  # necessary
            "Requested": [round(total_cpu_requested / n, 2),
                          round(total_disk_requested / n, 2),
                          round(total_memory_requested / n, 2)]

        })
        insert_index = 3  # for the row at the right place

        if not ignore_allocated_resources:
            df_total.insert(insert_index, "Allocated", [total_cpu_allocated,
                                             total_disk_allocated,
                                             total_memory_allocated])
            df_average.insert(insert_index, "Allocated", [round(total_cpu_allocated / n, 2),
                                               round(total_disk_allocated / n, 2),
                                               round(total_memory_allocated / n, 2)])

        # if gpu information was found
        if gpu_found:
            df_gpu_total = pd.DataFrame({
                "Resources": ['Total Gpu'],
                "Usage": [total_gpu_usage],  # necessary
                "Requested": [total_gpu_requested]
            })
            df_gpu_average = pd.DataFrame({
                "Resources": ['Average Gpu'],
                "Usage": [round(total_gpu_usage / n, 2)],  # Todo: divide by gpu occurrence ????
                "Requested": [round(total_gpu_requested / n, 2)]
            })
            if not ignore_allocated_resources:
                df_gpu_total.insert(insert_index, "Allocated", [total_gpu_allocated])
                df_gpu_average.insert(insert_index, "Allocated", [round(total_gpu_allocated / n, 2)])
                insert_index += 1

            df_total = df_total.append(df_gpu_total)
            df_average = df_average.append(df_gpu_average)

            # Todo: each gpu gets a single row
            df_total.insert(insert_index, "Assigned", ["", "", "", ", ".join(list_of_gpu_names)])
            df_average.insert(insert_index, "Assigned", ["", "", "", ", ".join(list_of_gpu_names)])
            insert_index += 1

        # Todo: add later
        #output_string += "Total used resources:\n"
        #output_string += tabulate(df_total, showindex=False, headers='keys', tablefmt=table_format) + "\n\n"

        output_string += "The following data only applies to successful executed jobs\n\n"
        output_string += "Used resources in average:\n"
        output_string += tabulate(df_average, showindex=False, headers='keys', tablefmt=table_format) + "\n\n"

    # Todo: cpu nodes might change over lifetime of a job
    if len(cpu_nodes) > 0:

        executed_jobs = list()
        runtime_per_node = list()
        for val in cpu_nodes.values():
            executed_jobs.append(val[0])
            average_job_duration = val[1]/val[0]
            runtime_per_node.append(datetime.timedelta(average_job_duration.days,average_job_duration.seconds))

        df_cpu = pd.DataFrame({
            "Cpu Nodes": list(cpu_nodes.keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        })
        output_string += tabulate(df_cpu, showindex=False, headers='keys', tablefmt=table_format) + "\n"
    output_string += "-"*75
    return output_string


# search in the files for the keywords
# Todo: extend
# Todo: show which keywords have been found and which not
# Todo: add -a and -s option so that for example all logs with errors can be summarised or analysed
def search_for(log_files, keywords, extend=False):

    keyword_list = keywords.split()

    # if extend is set, keywords like err will also look for keywords like warn exception, aborted, etc.
    if extend:  # apply some rules
        # the error list
        err_list = ["error", "err", "warning", "warn", "exception", "aborted", "abortion", "abnormal"]

        for i in range(len(keyword_list)):
            if (keyword_list[i]).lower() in err_list:
                keyword_list.remove(keyword_list[i])
                keyword_list.extend(err_list)

        print(green + f"Keyword List was extended, now search for these keywords: {keyword_list}" + back_to_default)
    else:
        print(green + f"Search for these keywords: {keyword_list}" + back_to_default)

    logging.debug(f"These are the keywords to look for: {keyword_list}")

    logs = validate_given_logs(log_files)
    # now search
    found_at_least_one = False
    found_logs = []
    for file in logs:
        found = False
        with open(file, "r") as read_file:
            for line in read_file:
                for keyword in keyword_list:
                    if re.search(keyword.lower(), line.lower()):
                        if not found_at_least_one:
                            print("Matches:")
                        print(lightgrey+file+back_to_default)
                        found = True
                        found_at_least_one = True
                        break
                if found:
                    found_logs.append(file)
                    break

    output_string = ""
    if not found_at_least_one:
        print(red + f"Unable to find these keywords {keyword_list}" + back_to_default)
        print(red + f"maybe try again with similar expressions" + back_to_default)

    else:
        print(f"Total count: {len(found_logs)}")
        if summarise:
            print(f"{magenta}Try to summarise these files{back_to_default}")
            output_string = summarise_logs(found_logs)
        elif analyse:
            print(f"{magenta}Try to analyse these files{back_to_default}")
            output_string = analyse_logs(found_logs)

    return output_string


def validate_given_logs(file_list):
    """
    This method is supposed to take the given log files (by config or command line argument)
    and tries to determine if these are valid log files, ignoring the std_log specification, if std_log = "".

    It will run through given directories and check every single file, if accessible, for the
    HTCondor log file standard, valid log files should start with lines like:

    000 (5989.000.000) 03/30 15:48:47 Job submitted from host: <10.0.8.10:9618?addrs=10.0.8.10-9618&noUDP&sock=3775629_0774_3>

    if done, it will change the global files list and store only valid log files.
    The user will be remind, what files were accepted as "valid".

    The user will also be informed if a given file was not found.

    Todo: In addition there should be an option --force, that makes the script stop, if the file was not found or marked as valid

    Todo: user should be able to get the option to decide, when the same files appear more than once
    -> my guess: yes or no question, if nothing is given in under 10 seconds, it should go with no
    -> this should prevent, that the script is stucked, if the user is for example running it over night

    """
    valid_files = list()

    for arg in file_list:

        path = os.getcwd()  # current working directory , should be condor job summariser script
        logs_path = path + "/" + arg  # absolute path

        working_dir_path = ""
        working_file_path = ""

        if os.path.isdir(arg):
            working_dir_path = arg
        elif os.path.isdir(logs_path):
            working_dir_path = logs_path

        elif os.path.isfile(arg):
            working_file_path = arg
        elif os.path.isfile(logs_path):
            working_file_path = logs_path
        # check if only the id was given and resolve it with the std_log specification
        elif os.path.isfile(arg+std_log):
            working_file_path = arg+std_log
        elif os.path.isfile(logs_path+std_log):
            working_file_path = logs_path+std_log

        # if path is a directory
        if working_dir_path.__ne__(""):
            # run through all files and separate the files into log/error and output files
            for file in os.listdir(working_dir_path):
                # ignore hidden files
                if file.startswith("."):
                    continue
                # if it's not a sub folder
                if working_dir_path.endswith('/'):
                    file_path = working_dir_path + file
                else:
                    file_path = working_dir_path + '/' + file

                if os.path.isfile(file_path):
                    with open(file_path, "r") as read_file:
                        # Todo: better specification with re
                        if os.path.getsize(file_path) == 0:  # file is empty
                            continue

                        if re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)", read_file.readlines()[0]):
                            #logging.debug(f"{read_file.name} is a valid HTCondor log file")
                            valid_files.append(file_path)

                else:
                    logging.debug(f"Found a subfolder: {working_dir_path}/{file}, it will be ignored")
                    print(yellow+f"Found a subfolder: {working_dir_path}/{file}, it will be ignored"+back_to_default)

        # else if path "might" be a valid HTCondor file
        elif working_file_path.__ne__(""):
            with open(working_file_path, "r") as read_file:

                if os.path.getsize(working_file_path) == 0:  # file is empty
                    print(red+"How dare you, you gave me an empty file :("+back_to_default)

                elif re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)", read_file.readlines()[0]):
                    logging.debug(f"{read_file.name} is a valid HTCondor log file")
                    valid_files.append(working_file_path)
                else:
                    logging.debug(f"The given file {read_file.name} is not a valid HTCondor log file")
                    print(yellow+f"The given file {read_file.name} is not a valid HTCondor log file"+back_to_default)
        else:
            logging.error(f"The given file: {arg} does not exist")
            print(red+f"The given file: {arg} does not exist"+back_to_default)

    return valid_files


def escape_seq_when_redirect():
    """
    This method should be activated first, when output is generated
    it does nothing, if output is printed to the terminal.
    If output gets redirected with > or | or other redirection tools, ignore escape sequences
    by setting them to ""
    :return:
    """
    # if output gets redirected with > or | or other redirection tools, ignore escape sequences
    if not sys.stdout.isatty():
        global red, green, yellow, magenta, cyan, lightgrey, back_to_default
        red = ""
        green = ""
        yellow = ""
        magenta = ""
        cyan = ""
        lightgrey = ""
        back_to_default = ""
        logging.debug("Output is getting redirected, all escape sequences were set to \"\"")


if __name__ == "__main__":
    """
    This is the main function, which searchs first for a given config file.
    After that, given parameters in the terminal will be interpreted, so they have a higher priority

    Then it will summarise all given log files together and print the output

    :return:
    """
    try:

        logging.debug("-------Start of htcompact scipt-------")
        escape_seq_when_redirect()  # set the color sequenzes to ""

        # if exit parameters are given that will interrupt this script, like --help,
        # catch them here so the config won't be unnecessary loaded
        manage_exit_params()

        # interpret the first file, that can be interpreted as a config file and remove it, other given config files,
        # (you will see when you try, files will be interpreted as HTCondor log files)
        # so it's not possible to give multiple config files
        load_config()

        # after that, interpret the command line arguments, these will have a higher priority, than the config setup
        # and by that arguments set in the config file, will be ignored
        manage_params()

        files = validate_given_logs(files)  # validate the files, make a list of all valid config files

        if search:
            output_str = search_for(files, search_values)
        elif summarise:
            output_str = summarise_logs(files)  # summarise information
        elif analyse:
            output_str = analyse_logs(files)  # analyse the given files
        else:
            output_str = "\n" + output_given_logs()  # print out all given files if possible

        print(output_str)  # write it to the console

        logging.debug("-------End of htcompact script-------")

    except KeyboardInterrupt:
        print("Script was interrupted")


