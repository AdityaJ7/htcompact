#!/usr/bin/env python3
import re
import sys
import os
import getopt
import datetime
import logging
from logging.handlers import RotatingFileHandler

import socket
import configparser
import numpy as np
from rich import print as rprint
from rich import logging as rlog
from rich.progress import track, Progress
from tabulate import tabulate
from plotille import Figure

from typing import List
log_inf_list = List[dict]
list_of_logs = List[str]

"""

This script is basically reading information from HTCondor log files and storing them into
dictionaries. By that its actually pretty simple to analyse the data.

The script makes the information compact and easy to under stand,
that's the reason for the name htcompact

Single logs can be read quite easily,
but also it's possible to summarize a whole directory with logs
to see for ex. the average runtime and usage of all the logs

"""

# Exit Codes
"""
 Normal Termination: 0
 Wrong Options or Arguments: 1
 No given files: 2
 Type Error within summation: 3
 Keyboard interruption: 4
"""

# Todos:
# Todo: rich progress bar collision with verbose mode looks ugly
# Todo: summary mode remake
# Todo: script is not accepting IPv6 yet


def initialize():
    """
    This method initializes all global variables
    :return:
    """
    global accepted_states, files, option_shorts, option_longs, \
        ignore_list, allowed_ignore_values, no_config, reverse_dns_lookup, \
        store_dns_lookups, verbose_mode, \
        to_csv, generate_log_file, filter_mode, filter_keywords, filter_extended, \
        colors, low_usage_threshold, bad_usage_threshold, std_log, std_err, std_out, \
        default_configfile, table_format, reading_stdin, redirecting_output,\
        show_list, allowed_show_values, mode, allowed_modes

    # global parameters, used for dynamical output of information
    accepted_states = ["true", "yes", "y", "ja", "j", "enable", "enabled", "wahr", "0"]
    files = list()
    option_shorts = "hsavm:"
    option_longs = ["help", "version", "verbose", "mode=",
                    "std-log=", "std-err=", "std-out=",
                    "ignore=", "show-more=", "no-config", "to-csv",
                    "generate-log-file", "filter=", "extend", "print-event-table",
                    "reverse-dns-lookup", "table-format="]

    # show more information, mostly given by HTCompact .err and .out files
    mode = None
    allowed_modes = {"a": "analyse",
                     "s": "summarize",
                     "as": "analysed-summary",
                     "d": "default"}

    show_list = list()
    allowed_show_values = ["errors", "warnings", "output"]

    # ignore information
    ignore_list = list()
    allowed_ignore_values = ["execution-details", "times", "host-nodes",
                             "used-resources", "requested-resources",
                             "allocated-resources", "all-resources",
                             "errors"]
    # Todo: ignore script warnings ?

    # if set do not use a config file, even if one was found
    no_config = False

    # Features:
    reverse_dns_lookup = False
    store_dns_lookups = dict()
    to_csv = False

    # logging tool
    generate_log_file = False
    verbose_mode = False

    # filter mode
    filter_mode = False
    filter_keywords = list()
    filter_extended = False

    # escape sequences for colors
    colors = {
        'red': "\033[0;31m",
        'green': "\033[0;32m",
        'yellow': "\033[0;33m",
        'magenta': "\033[0;35m",
        'cyan': "\033[0;36m",
        'blue': "\033[0;34m",
        'light_grey': "\033[37m",
        'back_to_default': "\033[0;39m"
    }

    # thresholds for bad and low usage of resources
    low_usage_threshold = 0.75
    bad_usage_threshold = 1.2

    # global variables with default values for err/log/out files
    std_log = ""
    std_err = ".err"
    std_out = ".out"

    # global defaults
    default_configfile = "htcompact.conf"
    table_format = "pretty"  # ascii by default

    reading_stdin = False
    redirecting_output = False


def check_for_redirection():
    """
    This method should be activated first, when output is generated
    it does nothing, if output is printed to the terminal.
    If output gets redirected with > or | or other redirection tools, ignore escape sequences
    by setting them to ""
    :return:
    """
    # if output gets redirected with > or | or other redirection tools, ignore escape sequences
    global reading_stdin, redirecting_output, stdin_input  # it is easier to save this globaly, because if whe change

    reading_stdin = not sys.stdin.isatty()
    redirecting_output = not sys.stdout.isatty()

    if reading_stdin:
        stdin_input = sys.stdin.readlines()

    if redirecting_output:
        global colors
        colors = {
            'red': "",
            'green': "",
            'yellow': "",
            'magenta': "",
            'cyan': "",
            'blue': "",
            'light_grey': "",
            'back_to_default': ""
        }


# consider: adjust this method to accept all values,
#  that are related to an option and interpret
#  the first one which does not as a file again
#  -> something like --ignore allocated-resources times ... should be possible
# Todo: consider regex in this implementation: --ignore alloc tim ...
#  with focus on longest prefix match
# may not be needed in future but will be used for now
def remove_files_from_args(args: list, short_opts : str, long_opts: list):
    """
        I want to make it easier for the user to insert many files at once.
        The script should detect option arguments and files

        It will filter the given files and remove them from the argument list

        !!!Exactly that is this method doing!!! (because getopt has no such function)

    :param args: is supposed to be a list, in any case it should be the sys.argv[1:] list
    :param short_opts: the short getopt options
    :param long_opts: the long getopt options
    :return: list of getopt arguments
    """
    if type(args) != list:
        raise_value_error("Expecting a list as argument")

    new_args = args.copy()
    generate_files = list()
    index = 0
    # run through the command line arguments,
    # skip the getopt argument and interpret everything else as a file
    while True:
        if index >= len(args):
            break

        arg = args[index]

        if arg.startswith("-"):  # for short_args
            arg = arg[1:]
            if arg.startswith("-"):  # for long_args
                arg = arg[1:]

            # skip the argument if getopt arguments are setable
            if arg+":" in short_opts:
                index += 1
            elif arg+"=" in long_opts:
                index += 1
        else:
            generate_files.append(arg)
            new_args.remove(arg)

        index += 1

    # change files, if found
    if len(generate_files) > 0:
        global files
        files = generate_files

    # return valid arguments
    return new_args


def manage_prioritized_params(args: list):

    if type(args) != list:
        raise_value_error("Expecting a list as argument")

    global option_shorts, option_longs
    global no_config, generate_log_file, verbose_mode

    # if for whatever reasons files were given
    better_args = remove_files_from_args(args, option_shorts, option_longs)

    try:
        opts, args = getopt.getopt(better_args, option_shorts, option_longs)

        for opt, arg in opts:

            # system exit params
            if opt in ["-h", "--help"]:
                print(small_help())
                sys.exit(0)
            if opt.__eq__("--version"):
                # Todo:
                print(f"Version: v1.0.7")
                sys.exit(0)
            if opt.__eq__("--print-event-table"):
                print(get_event_information())
                sys.exit(0)

            # prioritized management params,
            if opt in ["-v", "--verbose"]:
                verbose_mode = True
            elif opt.__eq__("--no-config"):
                no_config = True
            elif opt.__eq__("--generate-log-file"):
                generate_log_file = True
                logging.getLogger().disabled = False

    # print error messages
    except Exception as err:
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        rprint("[dark_olive_green1]Use '--help' or 'man htcompact' for help [/dark_olive_green1]")
        sys.exit(1)


# Todo: thresholds by command line
def manage_params(args: list):
    """
    Interprets the given command line arguments and changes the global variables in this scrips

    """
    global files  # list of files and directories
    global mode  # what mode are we on
    global std_log, std_err, std_out  # all default values for the HTCondor files
    global show_list  # show more
    global ignore_list  # ignore information variables
    global to_csv
    global filter_mode, filter_keywords, filter_extended  # search features
    global table_format  # table_format can be changed
    global reverse_dns_lookup  # if set host ip's will be looked up in dns server

    global option_shorts, option_longs

    summarizer_mode = False
    analyser_mode = False

    all_args = args

    # listen to stdin and add these files
    if reading_stdin:  # else the script will be waiting fro stdin
        logging.debug("Listening to arguments from stdin")
        for line in stdin_input:
            all_args.append(line.rstrip('\n'))

    better_args = remove_files_from_args(all_args, option_shorts, option_longs)

    try:
        opts, args = getopt.getopt(better_args, option_shorts, option_longs)

        for opt, arg in opts:

            # catch unusual but not wrong parameters starting with -
            if arg.startswith("-"):
                rprint("[yellow]The argument for {0} is {1}, "
                       "is that wanted?[/yellow]".format(opt, arg))
                logging.warning("The argument for {0} is {1}, "
                                "is that wanted?".format(opt, arg))

            elif opt in ["-m", "--mode"]:

                if arg in allowed_modes.keys():
                    mode = allowed_modes.get(arg)
                elif arg in allowed_modes.values():
                    mode = arg
                else:
                    raise_value_error("Invalid argument '" + arg + "'\n"
                                      "Valid arguments:\n" +
                                      ", ".join("'{}' or '{}'".format(key, value)
                                                for key, value in allowed_modes.items()))

                logging.debug("Starting the: "+mode+" mode")

            elif opt.__eq__("-s"):
                summarizer_mode = True
            elif opt.__eq__("-a"):
                analyser_mode = True
            elif opt in ["--filter"]:
                filter_mode = True
                filter_keywords = " ".join(re.split(',| ', arg)).split()
            elif opt in ["--extend"]:
                filter_extended = True
            # all HTCondor files, given by the user if they are not saved in .log/.err/.out files
            elif opt.__eq__("--std-log"):
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_log = arg
            elif opt.__eq__("--std-err"):
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_err = arg
            elif opt.__eq__("--std-out"):
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_out = arg

            # show more specific information
            elif opt.__eq__("--show-more"):
                keywords = " ".join(re.split(',| ', arg)).split()
                for keyword in keywords:
                    if keyword in allowed_show_values:
                        show_list.append(keyword)
                    else:
                        raise_value_error("Invalid argument '" + keyword + "'\n"
                                          "Valid arguments:\n" + ", ".join(allowed_show_values))
                logging.debug("Show these information: " + ", ".join(show_list))

            # all variables to ignore unwanted information
            elif opt.__eq__("--ignore"):
                keywords = " ".join(re.split(',| ', arg)).split()
                for keyword in keywords:
                    if keyword in allowed_ignore_values:
                        ignore_list.append(keyword)
                    else:
                        raise_value_error("Invalid argument '" + keyword + "'\n"
                                          "Valid arguments:\n" + ", ".join(allowed_ignore_values))
                logging.debug("Ignore these information: " + ", ".join(ignore_list))

            # Todo: to-csv
            # elif opt.__eq__("--to-csv"):
            #     to_csv = True
            #     rprint("[red]--to-csv not handled yet[/red]")
            #     sys.exit(0)

            elif opt.__eq__("--table-format"):
                types = "plain,simple,github,grid,fancy_grid,pipe," \
                        "orgtbl,rst,mediawiki,html,latex,latex_raw," \
                        "latex_booktabs,tsv,pretty"
                # only valid arguments
                if arg in types.split(","):
                    table_format = arg
                else:
                    logging.debug("The given table format doesn't exist")

            elif opt.__eq__("--reverse-dns-lookup"):
                reverse_dns_lookup = True

            # these are already managed in manage_prioritized_params
            # need to be caught, cause of exception cases
            elif opt.__eq__("--no-config"):
                pass
            elif opt.__eq__("--generate-log-file"):
                pass
            elif opt in ["-v", "--verbose"]:
                pass

            else:
                rprint("[red]Option not handled yet[/red]")
                sys.exit(0)
    # print error messages
    except Exception as err:
        logging.exception(err)  # write a detailed description in the stdout.log file
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        rprint("[dark_olive_green1]Use '--help' or 'man htcompact' for help [/dark_olive_green1]")
        sys.exit(1)

    if summarizer_mode and analyser_mode:
        mode = "analysed-summary"
    elif summarizer_mode:
        mode = "summarize"
    elif analyser_mode:
        mode = "analyse"

    if len(files) == 0:
        logging.debug("No files given")
        rprint("[red]No files given[/red]")
        sys.exit(2)


def small_help() -> str:
    """
Usage:
  htcompact [file ...] [-hsav] [-m mode] [--help] [--version] [--verbose]
            [--mode mode] [--std-log suffix] [--std-out suffix]
            [--std-err suffix] [--ignore keywords] [--show-more keywords]
            [--no-config] [--to-csv] [--generate-log-file]
            [--filter keywords [--extend]] [--print-event-table]
            [--reverse-dns-lookup] [--table-format tablefmt]

----------------------------Main features:---------------------------------

 --mode summary
         summarizes all given HTCondor log files and return a result
         regarding the averange usages and runtimes, this only makes
         sense, if multiple log files are given

 --mode analyse
        analyses a specific file, for occurred errors, ram history ->
        histogram, execution status, runtime and much more

 --mode analysed-summary
        A combination of both modes, which gives a more detailed overview

 More detailed descriptions and help on other options with:
 'man htcompact'
    """
    # returns this docstring
    return small_help.__doc__


def get_event_information(event_id="") -> str:
    """
Event Number: 000
Event Name: Job submitted
Event Description: This event occurs when a user submits a job. It is the first event you will see for a job, and it should only occur once.

Event Number: 001
Event Name: Job executing
Event Description: This shows up when a job is running. It might occur more than once.

Event Number: 002
Event Name: Error in executable
Event Description: The job could not be run because the executable was bad.

Event Number: 003
Event Name: Job was checkpointed
Event Description: The job's complete state was written to a checkpoint file. This might happen without the job being removed from a machine, because the checkpointing can happen periodically.

Event Number: 004
Event Name: Job evicted from machine
Event Description: A job was removed from a machine before it finished, usually for a policy reason. Perhaps an interactive user has claimed the computer, or perhaps another job is higher priority.

Event Number: 005
Event Name: Job terminated
Event Description: The job has completed.

Event Number: 006
Event Name: Image size of job updated
Event Description: An informational event, to update the amount of memory that the job is using while running. It does not reflect the state of the job.

Event Number: 007
Event Name: Shadow exception
Event Description: The condor_shadow, a program on the submit computer that watches over the job and performs some services for the job, failed for some catastrophic reason. The job will leave the machine and go back into the queue.

Event Number: 008
Event Name: Generic log event
Event Description: Not used.

Event Number: 009
Event Name: Job aborted
Event Description: The user canceled the job.

Event Number: 010
Event Name: Job was suspended
Event Description: The job is still on the computer, but it is no longer executing. This is usually for a policy reason, such as an interactive user using the computer.

Event Number: 011
Event Name: Job was unsuspended
Event Description: The job has resumed execution, after being suspended earlier.

Event Number: 012
Event Name: Job was held
Event Description: The job has transitioned to the hold state. This might happen if the user applies the condor_hold command to the job.

Event Number: 013
Event Name: Job was released
Event Description: The job was in the hold state and is to be re-run.

Event Number: 014
Event Name: Parallel node executed
Event Description: A parallel universe program is running on a node.

Event Number: 015
Event Name: Parallel node terminated
Event Description: A parallel universe program has completed on a node.

Event Number: 016
Event Name: POST script terminated
Event Description: A node in a DAGMan work flow has a script that should be run after a job. The script is run on the submit host. This event signals that the post script has completed.

Event Number: 017
Event Name: Job submitted to Globus
Event Description: A grid job has been delegated to Globus (version 2, 3, or 4). This event is no longer used.

Event Number: 018
Event Name: Globus submit failed
Event Description: The attempt to delegate a job to Globus failed.

Event Number: 019
Event Name: Globus resource up
Event Description: The Globus resource that a job wants to run on was unavailable, but is now available. This event is no longer used.

Event Number: 020
Event Name: Detected Down Globus Resource
Event Description: The Globus resource that a job wants to run on has become unavailable. This event is no longer used.

Event Number: 021
Event Name: Remote error
Event Description: The condor_starter (which monitors the job on the execution machine) has failed.

Event Number: 022
Event Name: Remote system call socket lost
Event Description: The condor_shadow and condor_starter (which communicate while the job runs) have lost contact.

Event Number: 023
Event Name: Remote system call socket reestablished
Event Description: The condor_shadow and condor_starter (which communicate while the job runs) have been able to resume contact before the job lease expired.

Event Number: 024
Event Name: Remote system call reconnect failure
Event Description: The condor_shadow and condor_starter (which communicate while the job runs) were unable to resume contact before the job lease expired.

Event Number: 025
Event Name: Grid Resource Back Up
Event Description: A grid resource that was previously unavailable is now available.

Event Number: 026
Event Name: Detected Down Grid Resource
Event Description: The grid resource that a job is to run on is unavailable.

Event Number: 027
Event Name: Job submitted to grid resource
Event Description: A job has been submitted, and is under the auspices of the grid resource.

Event Number: 028
Event Name: Job ad information event triggered.
Event Description: Extra job ClassAd attributes are noted. This event is written as a supplement to other events when the configuration parameter EVENT_LOG_JOB_AD_INFORMATION_ATTRS is set.

Event Number: 029
Event Name: The job's remote status is unknown
Event Description: No updates of the job's remote status have been received for 15 minutes.

Event Number: 030
Event Name: The job's remote status is known again
Event Description: An update has been received for a job whose remote status was previous logged as unknown.

Event Number: 031
Event Name: Job stage in
Event Description: A grid universe job is doing the stage in of input files.

Event Number: 032
Event Name: Job stage out
Event Description: A grid universe job is doing the stage out of output files.

Event Number: 033
Event Name: Job ClassAd attribute update
Event Description: A Job ClassAd attribute is changed due to action by the condor_schedd daemon. This includes changes by condor_prio.

Event Number: 034
Event Name: Pre Skip event
Event Description: For DAGMan, this event is logged if a PRE SCRIPT exits with the defined PRE_SKIP value in the DAG input file. This makes it possible for DAGMan to do recovery in a workflow that has such an event, as it would otherwise not have any event for the DAGMan node to which the script belongs, and in recovery, DAGMan's internal tables would become corrupted.
"""
    if event_id == "":
        return get_event_information.__doc__
    doc_str = get_event_information.__doc__.split('\n')
    for i, line in enumerate(doc_str):
        match = re.match(r"Event Number: ([0-9]{3})", line)
        if match:
            if int(match[1]) == int(event_id):
                return "\n".join(doc_str[i:i+3])
    # else
    return "This event number does not exist, Valid event numbers range from 000 to 034"


def raise_value_error(message: str) -> ValueError:
    raise ValueError(message)


def raise_type_error(message: str) -> TypeError:
    raise TypeError(message)


# Todo: time seperated ( submitting / executing / terminating)
# Todo: Read HTCondor Errors that occurred
def log_to_dict(file: str) -> (dict, dict):
    """
        Read the log file and filter useful information.
        Structur it in a way, that later two different dictionaries will be retuned.

    :type file: str
    :param file: HTCondor log file
    :return: job_dict, res_dict

            job_dict should never be empty,
            it holds information like runtime, return value, host, etc.

            res_dict can be empty, it holds all the
            used, requested and allocated resources
    """

    try:
        job_events, job_raw_information = read_condor_log(file)

        job_dict = dict()
        res_dict = dict()

        # if the last job event is : Job terminated
        if job_events[-1][0].__eq__("005"):

            # calculate the runtime for the job
            executed_date = datetime.datetime.strptime(
                job_events[1][2] + " " + job_events[1][3], "%m/%d %H:%M:%S")
            terminating_date = datetime.datetime.strptime(
                job_events[-1][2] + " " + job_events[-1][3], "%m/%d %H:%M:%S")
            runtime = terminating_date - executed_date  # calculation of the time runtime

            match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job_events[1][5])
            if match_host:
                host = match_host[1]
                # if reverse dns lookup, change the ip to cpu: last number
                if reverse_dns_lookup:
                    host = gethostbyaddr(host)
                port = match_host[2]
            else:
                logging.exception("Host and port haven't been matched correctly")
                rprint("[red]faulty values for the host ip make sure it's a valid IPv4[/red]")
                host = "Unknown"
                port = "Unknown"

            # make a fancy design for the job_information
            job_labels = ["Executing on Host", "Port", "Runtime"]  # holds the labels
            job_information = [host, port, runtime]  # holds the related job information

            # filter the termination state ...
            if True:
                # check if termination state is normal
                termination_state_inf = job_raw_information[-1][0]
                match_termination_state = re.match(r"[\t ]+\([0-9]+\) ((?:\w+ )*)", termination_state_inf)
                job_labels.insert(0, "Termination State")
                if match_termination_state:

                    if "Normal termination" in match_termination_state[1]:
                        job_information.insert(0, match_termination_state[1])
                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)",
                                                      termination_state_inf)
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return value in the HTCondor log file: {file}")
                            rprint(f"[red]Not a valid return value in the HTCondor log file: {file}[/red]")
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)
                    elif "Abnormal termination" in match_termination_state[1]:
                        job_information.insert(0, colors['red']+match_termination_state[1]+colors['back_to_default'])

                        match_signal = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)",
                                                      termination_state_inf)
                        signal = match_signal[1] if match_signal else "None"
                        if signal == "None":
                            logging.debug(f"Not a valid return value in the HTCondor log file: {file}")
                            rprint(f"[red]Not a valid return value in the HTCondor log file: {file}[/red]")
                        else:
                            job_labels.append("Return Value")
                            job_information.append(signal)

                        termination_desc = list()
                        # append the reason for the Abnormal Termination
                        for desc in job_raw_information[-1][1:]:
                            if re.match(r"[\t ]+\(0\)", desc):
                                termination_desc.append(desc[5:])
                            else:
                                break

                        if not len(termination_desc) == 0:
                            job_labels.append("Description")
                            job_information.append("\n".join(termination_desc))
                        else:
                            logging.debug(f"No termination description for {file} found")

                    else:
                        job_information.insert(0, match_termination_state[1])
                        logging.debug(f"Don't know this Termination State: {match_termination_state[1]} yet")

                else:
                    job_information.insert(0, "Not valid")
                    rprint(f"[red]Termination error in HTCondor log file: {file}[/red]")

            # now put everything together in a table
            job_dict = {
                "Description": job_labels,
                "Values": job_information
            }

            # these where the job information now focus on the used resources

            relevant_str = "\n".join(job_raw_information[-1])  # job information of the last job (Job terminated)

            # next part removes not useful lines
            if True:  # just for readability
                # remove unnecessary lines
                lines = relevant_str.splitlines()
                # while not lines[0].startswith("\tPartitionable"):
                while not re.match(r"[\t ]+Partitionable", lines[0]):
                    lines.remove(lines[0])

                lines.remove(lines[0])
                partitionable_res = lines
                # done, partitionable_resources contain now only information about used resources

            # match all resources
            # ***************************************************************************************
            match = re.match(r"[\t ]+Cpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+)",
                             partitionable_res[0])
            if match:
                cpu_usage, cpu_request, cpu_allocated = match[1], match[2], match[3]
            else:
                logging.debug("Something went wrong reading the cpu information in: " + file)
                raise_type_error("Something went wrong reading the cpu information")

            match = re.match(r"[\t ]+Disk \(KB\) *: *([0-9]+) *([0-9]+) *([0-9]+)",
                             partitionable_res[1])
            if match:
                disk_usage, disk_request, disk_allocated = match[1], match[2], match[3]
            else:
                logging.debug("Something went wrong reading the disk information in: " + file)
                raise raise_type_error("Something went wrong reading the disk information")

            # check if the log has gpu information in between
            gpu_match_index = 2  # can be ignored if gpu is given, memory has to access the next index
            gpu_found = False  # easier to check to add information in the dict
            match = re.match(r'[\t ]+Gpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+) "?(.*)"?',
                             partitionable_res[gpu_match_index])
            if match:
                gpu_usage, gpu_request, gpu_allocated, gpu_name = match[1], match[2], match[3], match[4]
                gpu_match_index += 1
                gpu_found = True

            match = re.match(r"[\t ]+Memory \(MB\)  *: *([0-9]+) *([0-9]+) *([0-9]+)",
                             partitionable_res[gpu_match_index])
            if match:
                memory_usage, memory_request, memory_allocated = match[1], match[2], match[3]
            else:
                logging.debug("Something went wrong reading the memory information in: " + file)
                raise_type_error("Something went wrong reading the memory information")
            # ****************************************************************************************************

            # list of resources and their labels
            resource_labels = ["Cpu", "Disk", "Memory"]
            usage = [cpu_usage, disk_usage, memory_usage]
            requested = [cpu_request, disk_request, memory_request]
            allocated = [cpu_allocated, disk_allocated, memory_allocated]

            if gpu_found:
                resource_labels.append("Gpus")
                usage.append(gpu_usage)
                requested.append(gpu_request)
                allocated.append(gpu_allocated)

            # Error handling: change empty values to NaN in the first column
            for i in range(3):
                if usage[i] == "":
                    usage[i] = np.nan
                if requested[i] == "":
                    requested[i] = np.nan
                if allocated[i] == "":
                    allocated[i] = np.nan

            # put the data in the dict
            res_dict = {
                "Resources": resource_labels,
                "Usage": usage,
                "Requested": requested,
                "Allocated": allocated
            }

            if gpu_found:
                res_dict["Assigned"] = ["", "", "", gpu_name]

        # Todo: check this section
        # This means the job was terminated, is still running or some other cases
        # In this case I want to give the user more information about the process
        else:

            waiting_time = None
            runtime = None
            desc_list = list()  # append to this list for further details
            value_list = list()  # append the related values to this list

            # first job event was submitted
            if job_events[0][0].__eq__("000"):
                submitted_date = datetime.datetime.strptime(job_events[0][2] + " " + job_events[0][3], "%m/%d %H:%M:%S")

                match_from_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job_events[0][5])
                if match_from_host:
                    from_host = match_from_host[1]
                    desc_list.append('Send from IP')
                    value_list.append(from_host)

                desc_list.append('Submission date')
                value_list.append(submitted_date.strftime("%m/%d %H:%M:%S"))

            if job_events[-1][0].__eq__("009"):  # job aborted

                desc_list.insert(0, "Process was")
                value_list.insert(0, colors['yellow'] + "aborted" + colors['back_to_default'])

                # calculate the runtime for the job
                if job_events[-1].__ne__(job_events[0]):
                    executed_date = datetime.datetime.strptime(
                        job_events[1][2] + " " + job_events[1][3], "%m/%d %H:%M:%S")
                    terminating_date = datetime.datetime.strptime(
                        job_events[-1][2] + " " + job_events[-1][3], "%m/%d %H:%M:%S")

                    waiting_time = executed_date - submitted_date  # calculate the waiting time
                    today = datetime.datetime.now()  # get todays date
                    today = today.replace(year=1900, microsecond=0)  # format the date
                    runtime = today - executed_date  # calculate the runtime

                    desc_list.append('Executing since')
                    value_list.append(executed_date.strftime("%m/%d %H:%M:%S"))
                    desc_list.append('Total waiting time')
                    value_list.append(waiting_time)
                    desc_list.append('Total runtime')
                    value_list.append(runtime)
                    desc_list.append('Termination date')
                    value_list.append(terminating_date.strftime("%m/%d %H:%M:%S"))

                user = ((job_raw_information[-1][0]).split(" ")[-1])[:-1]
                logging.debug(f"{file}: Job aborted by {user}")

                desc_list.append('Aborted by user:')
                value_list.append(colors['yellow']+user+colors['back_to_default'])

            # Todo: prove this section
            # This usally means the job is still running
            else:

                if job_events[-1].__ne__(job_events[0]):

                    executed_date = datetime.datetime.strptime(
                        job_events[1][2] + " " + job_events[1][3], "%m/%d %H:%M:%S")

                    waiting_time = executed_date - submitted_date  # calculate the waiting time
                    today = datetime.datetime.now()
                    today = today.replace(year=1900, microsecond=0)
                    runtime = today - executed_date  # calculate the runtime

                    desc_list.append('Executing since')
                    value_list.append(executed_date.strftime("%m/%d %H:%M:%S"))
                else:
                    today = datetime.datetime.now()
                    today = today.replace(year=1900, microsecond=0)
                    waiting_time = today - submitted_date

                if job_events[-1][0].__eq__("000"):
                    state = "Waiting"
                else:
                    state = "Executing"

                desc_list.insert(0, "Process is")
                value_list.insert(0, state)

                if waiting_time is not None:
                    if runtime is None:
                        desc_list.append('Still waiting since')
                    else:
                        desc_list.append('Total waiting time')
                    value_list.append(waiting_time)

                if runtime is not None:
                    desc_list.append('Already running since')
                    value_list.append(runtime)

            job_dict = {
                "Description": desc_list,
                "Values": value_list
            }

    except NameError as err:
        logging.exception(err)
        rprint(f"[red]The smart_output_logs method requires a {std_log} file as parameter[/red]")
    except FileNotFoundError or ValueError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    except IndexError as err:
        logging.debug(f"Index Error with {file}: ")
        logging.exception(err)
    except TypeError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        sys.exit(3)
    except Exception as err:
        logging.debug(err)
    # finally
    else:

        return job_dict, res_dict


# reads all information, but returns them in two lists
def read_condor_log(file: str) -> (list, list):
    """

    reads a given HTCondor std_log file and separates the information in two lists,
    for further and easier access

    :type file: str
    :param file:    a HTCondor std_log file

    :raises: :class:'FileNotFoundError': if open does not work

    :return:    (job_event, job_event_information)
                a list of job_events and a list of job_event-relevant information
                these are related by the same index like:
                job_event[1]-> relevant information: job_event_information[1]

                job_event looks like:
                [event_number, job_id, date, time, description, additional information]

    """

    log_job_events = list()  # saves the job_events
    job_event_information = list()  # saves the information for each job event, if there are any
    temp_job_event_list = list()  # just a temporary list, gets inserted into job_event_information

    with open(file) as log_file:
        # for every line in the log_file
        for line in log_file:

            line = line.rstrip("\n")  # remove newlines
            match_log\
                = re.match(r"([0-9]{3})"  # matches event number
                           r" (\([0-9]+.[0-9]+.[0-9]{3}\))"  # matches clusterid and process id
                           r" ([0-9]{2}/[0-9]{2})"  # matches date
                           r" ([0-9]{2}:[0-9]{2}:[0-9]{2})"  # matches time
                           r"((?: \w+)*)."  # matches the job event name
                           r"(.*)", line)  # matches further information
            if match_log:
                job_event_number = match_log[1]  # job event number, can be found in job_description.txt
                clusterid_procid_inf = match_log[2]  # same numbers, that make the name of the file
                date = match_log[3]  # filter the date in the form Month/Day in numbers
                time = match_log[4]  # filter the time
                job_event_name = match_log[5]  # filter the job event name
                job_relevant_inf = match_log[6]  # filter the job relevant information
                log_job_events.append([job_event_number, clusterid_procid_inf,
                                       date, time, job_event_name, job_relevant_inf])
                # print(job_event_number, clusterid_procid_inf, date, time, job_event_name, job_relevant_inf)
            else:
                # end of job event
                if "..." in line:
                    job_event_information.append(temp_job_event_list)
                    temp_job_event_list = list()  # clear list
                    continue
                # else
                #if re.match("[\t ]+", line):
                else:
                    temp_job_event_list.append(line)
    return log_job_events, job_event_information


# Todo: maybe less information
# just read .err files content and return it as a string
def read_condor_error(file: str) -> str:
    """
    Reads a HTCondor .err file and returns its content

    :param file: a HTCondor .err file

    :raises: :class:'NameError': The param file was no .err file
             :class:'FileNotFoundError': if open does not work

    :return: file content

    """
    if not file.endswith(std_err):
        raise NameError("The read_condor_error method is only for "+std_err+" files")

    try:
        err_file = open(file)
    except FileNotFoundError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    else:
        return "".join(err_file.readlines())


# just read .out files content and return it as a string
def read_condor_output(file: str) -> str:
    """
        Reads a HTCondor .out file and returns its content

        :param file: a HTCondor .out file

        :raises: :class:'NameError': The param file was no .out file
                 :class:'FileNotFoundError': if open does not work

        :return: file content

        """
    if not file.endswith(std_out):
        raise NameError("The read_condor_output method is only for "+std_out+" files")
    try:
        out_file = open(file)
    except FileNotFoundError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    else:
        return "".join(out_file.readlines())


def gethostbyaddr(ip):
    """
        this function is supposed to filter a given ip for it's representative domain name like google.com
        :return: resolved domain name, else give back the ip
    """
    try:
        if ip in list(store_dns_lookups.keys()):
            return store_dns_lookups[ip]
        # else lookup
        reversed_dns = socket.gethostbyaddr(ip)
        logging.debug('Lookup successful ' + ip + ' resolved as: ' + reversed_dns[0])
        # store
        store_dns_lookups[ip] = reversed_dns[0]
        # return
        return reversed_dns[0]
    except Exception:
        logging.debug('Not able to resolve the IP: '+ip)
        # also store
        store_dns_lookups[ip] = ip
        return ip


# Todo: question about if thresholds for used cpu, gpu should exists
def manage_thresholds(resources: dict) -> dict:
    """
        Takes a dict which should look like this:
        average_dict = {
            "Resources": ['Average Cpu', 'Average Disk (KB)', 'Average Memory (MB)'],
            "Usage": [round(total_cpu_usage / n, 2),
                      round(total_disk_usage / n, 2),
                      round(total_memory_usage / n, 2)],  # necessary
            "Requested": [round(total_cpu_requested / n, 2),
                          round(total_disk_requested / n, 2),
                          round(total_memory_requested / n, 2)],
            "Allocated": [round(total_cpu_allocated / n, 2),
                          round(total_disk_allocated / n, 2),
                          round(total_memory_allocated / n, 2)]

        }

        The important part is that the keywords "Usage", "Requested" exists
        and that at least 3 values are given: cpu, disk, memory

        Additionally gpu usage is possible
    :param resources:
    :return:
    """
    # thresholds used vs. requested disk
    if float(resources['Usage'][1]) / float(resources['Requested'][1]) > bad_usage_threshold:
        resources['Usage'][1] = colors['red'] + str(resources['Usage'][1]) + colors['back_to_default']
    elif float(resources['Usage'][1]) / float(resources['Requested'][1]) < low_usage_threshold:
        resources['Usage'][1] = colors['yellow'] + str(resources['Usage'][1]) + colors['back_to_default']
    # thresholds used vs. requested memory
    if float(resources['Usage'][2]) / float(resources['Requested'][2]) > bad_usage_threshold:
        resources['Usage'][2] = colors['red'] + str(resources['Usage'][2]) + colors['back_to_default']
    elif float(resources['Usage'][2]) / float(resources['Requested'][2]) < low_usage_threshold:
        resources['Usage'][2] = colors['yellow'] + str(resources['Usage'][2]) + colors['back_to_default']

    # # Gpu Usage found
    # if len(resources['Usage']) == 4 and len(resources['Requested']) == 4:
    #     # thresholds used vs. requested gpu
    #     if resources['Usage'][3] / resources['Requested'][3] > bad_usage_threshold:
    #         resources['Usage'][3] = colors['red'] + str(resources['Usage'][3]) + colors['back_to_default']
    #     elif resources['Usage'][3] / resources['Requested'][3] < low_usage_threshold:
    #         resources['Usage'][3] = colors['yellow'] + str(resources['Usage'][3]) + colors['back_to_default']

    return resources


def show_htcondor_stderr(file: str) -> str:
    """

    :param file: a HTCondor .err file
    :return: the content of the given file as a string
    """

    # Todo:
    # - errors from htcondor are more important (std.err output)
    # - are there errors ? true or false -> maybe print those in any kind of fromat
    # - maybe check for file size !!!
    # - is the file size the same ? what is normal / different errors (feature -> for  -> later)

    # accept files without the std_err suffix
    if std_err.__ne__("") and file[-len(std_err):].__eq__(std_err):
        job_spec_id = file[:-len(std_err)]
    elif std_err.__ne__(""):
        job_spec_id = file
    else:
        raise_value_error("Missing specification for the std_err")

    output_string = ""
    try:

        error_content = read_condor_error(job_spec_id + std_err)
        for line in error_content.split("\n"):
            if "err" in line.lower() and 'errors' in show_list:
                output_string += colors['red'] + line + colors['back_to_default'] + "\n"
            elif "warn" in line.lower() and 'warnings' in show_list:
                output_string += colors['yellow'] + line + colors['back_to_default'] + "\n"

    except NameError as err:
        logging.exception(err)
        rprint("[red]The smart_output_error method requires a "+std_err+" file as parameter[/red]")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)"+std_err, relevant[1])
        rprint(f"[yellow]There is no related {std_err} file: {relevant[1]} in the directory:\n[/yellow]"
               f"[cyan]'{os.path.abspath(relevant[0])}'\n with the prefix: {match[1]}[/cyan]")
    except TypeError as err:
        logging.exception(err)
        # we continue here no exit needed
        # rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    finally:
        return output_string


def show_htcondor_stdout(file: str) -> str:
    """
    :param file: a HTCondor .out file
    :return: the content of that file as a string
    """

    # accept files without the std_out suffix
    if std_out.__ne__("") and file[-len(std_out):].__eq__(std_out):
        job_spec_id = file[:-len(std_out)]
    elif std_out.__ne__(""):
        job_spec_id = file
    else:
        raise_value_error("Missing specification for the std_out file")

    output_string = ""
    try:

        output_content = read_condor_output(job_spec_id + std_out)
        output_string += output_content
    except NameError as err:
        logging.exception(err)
        rprint("[red]The smart_output_output method requires a "+std_out+" file as parameter[/red]")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)"+std_out, relevant[1])
        rprint(f"[yellow]There is no related {std_out} file: {relevant[1]} in the directory:\n[/yellow]"
               f"[cyan]'{os.path.abspath(relevant[0])}'\n with the prefix: {match[1]}[/cyan]")
    except TypeError as err:
        logging.exception(err)
        # we continue here no exit needed
        # rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    finally:

        return output_string


def customize_log(file: str) -> dict:
    """
    reads a given HTCondor .log file with the read_condor_logs() function

    :param file:    a HTCondor .log file
    :param header:  Shows the header of the columns
    :param index:   Shows the index of the rows

    :return:        (output_string)
                    an output string that shows information like:
                    The job procedure of : ../logs/454_199.log
                    +-------------------+--------------------+
                    | Executing on Host |      10.0.9.1      |
                    |       Port        |        9618        |
                    |      Runtime      |      1:12:20       |
                    | Termination State | Normal termination |
                    |   Return Value    |         0          |
                    +-------------------+--------------------+
                    +--------+-------+-----------+-----------+
                    |        | Usage | Requested | Allocated |
                    +--------+-------+-----------+-----------+
                    |  Cpu   | 0.30  |     1     |     1     |
                    |  Disk  |  200  |    200    |  3770656  |
                    | Memory |   3   |     1     |    128    |
                    +--------+-------+-----------+-----------+

    """
    # accept files without the std_log suffix
    if std_log.__ne__("") and file[-len(std_log):].__eq__(std_log):
        job_spec_id = file[:-len(std_log)]
    else:
        job_spec_id = os.path.splitext(file)[0]

    result_dict = dict()

    try:

        job_dict, res_dict = log_to_dict(file)

        # Todo: csv style one line
        if to_csv:
            pass
        else:
            result_dict["file-description"] = f"{colors['green']}The job procedure of : {file}{colors['back_to_default']}"

            result_dict["execution-details"] = job_dict
            if not len(res_dict) == 0:  # make sure res_df is not None

                res_dict = manage_thresholds(res_dict)

                result_dict["all-resources"] = res_dict

        if 'errors' in show_list or 'warnings' in show_list:
            result_dict['stderr'] = show_htcondor_stderr(job_spec_id + std_err)
        if 'output' in show_list:
            result_dict['stdout'] = show_htcondor_stdout(job_spec_id + std_out)

    except NameError as err:
        logging.exception(err)
        rprint(f"[red]The smart_output_logs method requires a {std_log} file as parameter[/red]")
    except ValueError as err:
        logging.exception(err)
        rprint(f"[red]{err}[/red]")
    except TypeError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        sys.exit(3)
    except Exception as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")

    return result_dict


def find_config(args: list) -> (str, list):
    """

        Try to find a config file in the given arguments.
        Go through the hierarchy in the order:
        - current_working_directory/file
        - current_project_folder/config/file
        - ~/.config/{script_name}/{file}"
        - /etc/{file}

        ONE config will be loaded if set: generate_logging_file

        -> else try to find default config file "htcompact.conf"
         in the same order excluding the current working directory,
         because this might lead to misbehaviour, when the user changes the directory.

        :return: file if found, else None, args without config, if found
    """

    script_name = sys.argv[0]
    if script_name.endswith(".py"):
        script_name = sys.argv[0][:-3]  # scriptname without .py
    config = configparser.ConfigParser()
    # search for the given file, first directly, then in /etc and then in ~/.config/htcompact/
    found_config = False

    arguments = args
    arguments.append(default_configfile)

    found_config_file = None
    found_default_in_cwd = False
    # run through every given argument and try to determine if it's a valid config file
    for i, file in enumerate(arguments):
        # try to find the given file in the current directory, in /etc or in ~/.config/htcompact/
        try:
            # search in current working directory
            if os.path.isfile(file) and config.read(file):
                # only use the file if given, means the default config should be ignored,
                # even if it's in cwd, this will exclude misbehaviour, when cwd is changed
                if not (file.__eq__(default_configfile) and i == len(arguments) - 1):
                    found_config = True
                    arguments.remove(file)
                else:
                    found_default_in_cwd = True

                found_config_file = file

            # try to find the config file in the current environment hierarchy
            elif os.path.isfile(f"{sys.prefix}/config/{file}") and config.read(f"{sys.prefix}/config/{file}"):
                found_config = True
                arguments.remove(file)
                found_config_file = f"{sys.prefix}/config/{file}"  # remember the file

            # try to find config file in ~/.config/script_name/
            elif os.path.isfile(f"~/.config/{script_name}/{file}") \
                    and config.read(f"~/.config/{script_name}/{file}"):
                found_config = True
                arguments.remove(file)
                found_config_file = f"~/.config/{script_name}/{file}"  # remember the file

            # try to find config file in /etc
            elif os.path.isfile(f"/etc/{file}") and config.read(f"/etc/{file}"):
                found_config = True
                arguments.remove(file)
                found_config_file = f"/etc/{file}"  # remember the file

            else:
                logging.debug(f"{file} not found or is not a valid config file")

            # do not search any longer if found
            if found_config:
                break

        # File has no readable format for the configparser, probably because it's not a config file
        except configparser.MissingSectionHeaderError:
            continue
        except configparser.DuplicateOptionError as err:
            rprint(f"[red]{err}[/red]")

    try:
        arguments.remove(default_configfile)  # remove again, if other config file was found
    except ValueError:
        pass

    # prepare output if not and break if not found
    if not found_config and not found_default_in_cwd:
        rprint(f"[yellow]No valid config file found in: \n"
               f"- {os.getcwd()},\n"
               f"- {sys.prefix}/config,\n"
               f"- ~/.config/htcompact/,\n"
               f"- /etc/ \n"
               f"-> using default settings[/yellow]")
        return None, args
    elif found_default_in_cwd:
        rprint(f"[yellow]Found default {default_configfile} in the current working directory.\n"
               f"It will be ignore, to use this, make sure to pass it as an argument like:\n"
               f"htcompact {default_configfile} [logs] [options]\n"
               f"-> using default settings[/yellow]")
        return None, args

    config.read(found_config_file)
    sections = config.sections()
    # check if logging is tuned on
    if 'features' in sections:
        if 'generate_log_file' in config['features']:
            global generate_log_file
            if not generate_log_file:  # if already set, do NOT overwrite
                generate_log_file = config['features']['generate_log_file'].lower() in accepted_states
                logging.getLogger().disabled = not generate_log_file

    return found_config_file, arguments


# search for config file ( UNIX BASED )
def load_config(config_file: str):

    config = configparser.ConfigParser()

    try:

        config.read(config_file)
        # all sections in the config file
        sections = config.sections()

        # now try filter the config file for all available parameters

        rprint(f"[blue]Load config from file: {config_file}[/blue]")

        # all documents like files, etc.
        if 'documents' in sections:
            global files

            if 'files' in config['documents']:
                files = config['documents']['files'].split(" ")
                logging.debug(f"Changed document files to {files}")

        # all formats like the table format
        if 'formats' in sections:
            global table_format
            if 'table_format' in config['formats']:
                table_format = config['formats']['table_format']
                logging.debug(f"Changed default table_format to: {table_format}")

        # all basic HTCondor file endings like .log, .err, .out
        if 'htc-files' in sections:
            global std_log, std_err, std_out

            if 'stdlog' in config['htc-files']:
                std_log = config['htc-files']['stdlog']
                logging.debug(f"Changed default for HTCondor .log files to: {std_log}")

            if 'stderr' in config['htc-files']:
                std_err = config['htc-files']['stderr']
                logging.debug(f"Changed default for HTCondor .err files to: {std_err}")

            if 'stdout' in config['htc-files']:
                std_out = config['htc-files']['stdout']
                logging.debug(f"Changed default for HTCondor .out files to: {std_out}")

        # if show variables are set for further output
        if 'show-more' in sections:
            global show_list

            if 'show_list' in config['show-more']:
                for arg in " ".join(re.split(',| ', config['show-more']['show_list'])).split():
                    if arg in allowed_show_values:
                        show_list.append(arg)
                    else:
                        logging.debug("Don't know this ignore statement: "+ arg)
                logging.debug(f"Changed default ignore statements to: {show_list}")

        # what information should to be ignored
        if 'ignore' in sections:
            global ignore_list  # sources to ignore

            if 'ignore_list' in config['ignore']:
                for arg in " ".join(re.split(',| ', config['ignore']['ignore_list'])).split():
                    if arg in allowed_ignore_values:
                        ignore_list.append(arg)
                    else:
                        logging.debug("Don't know this show statement: "+ arg)
                logging.debug(f"Changed default show-more statements to: {ignore_list}")

        if 'thresholds' in sections:
            global low_usage_threshold, bad_usage_threshold
            if 'low_usage' in config['thresholds']:
                low_usage_threshold = float(config['thresholds']['low_usage'])
                logging.debug(f"Changed default low_usage to: {low_usage_threshold}")
            if 'bad_usage' in config['thresholds']:
                bad_usage_threshold = float(config['thresholds']['bad_usage'])
                logging.debug(f"Changed default bad_usage to: {bad_usage_threshold}")

        if "modes" in sections:
            global filter_mode, mode
            if 'filter_mode' in config['modes']:
                filter_mode = config['modes']['filter_mode'].lower() in accepted_states
                logging.debug(f"Changed filter_mode to: {filter_mode}")

            if 'mode' in config['modes']:
                mode_tmp = config['modes']['mode'].lower()
                if mode_tmp in allowed_modes.values():
                    mode = mode_tmp
                elif mode_tmp in allowed_modes.keys():
                    mode = allowed_modes.get(mode_tmp)
                else:
                    mode = None
                logging.debug(f"Changed mode to: {mode}")

        if "filter" in sections:
            global filter_keywords, filter_extended
            if 'filter_keywords' in config['filter']:
                filter_keywords = " ".join(re.split(',| ', config['filter']['filter_keywords'])).split()
                logging.debug(f"Changed default filter_keywords to: {filter_keywords}")
            if 'filter_extended' in config['filter']:
                filter_extended = config['filter']['filter_extended'].lower() in accepted_states
                logging.debug(f"Changed default filter_extended to: {filter_extended}")

        if 'features' in sections:
            global reverse_dns_lookup, to_csv  # extra parameters
            # the first thing to check should be if logging is turend on

            if 'reverse_dns_lookup' in config['features']:
                reverse_dns_lookup = config['features']['reverse_dns_lookup'].lower() in accepted_states
                logging.debug(f"Changed default reverse_dns_lookup to: {reverse_dns_lookup}")

            if 'to_csv' in config['features']:
                to_csv = config['features']['to_csv'].lower() in accepted_states
                logging.debug(f"Changed default to_csv to: {to_csv}")

        return True

    except KeyError as err:
        logging.exception(err)


# in order that plotille has nothing like a int converter,
# I have to set it up manually to show the y - label in number format
def _int_formatter(val, chars, delta, left=False):
    """
    Usage of this is shown here:
    https://github.com/tammoippen/plotille/issues/11

    :param val:
    :param chars:
    :param delta:
    :param left:
    :return:
    """
    align = '<' if left else ''
    return '{:{}{}d}'.format(int(val), align, chars)


def default(log_files: list_of_logs) -> log_inf_list:
    """
    Print the default output for a given list of log files

    This mode is just an easy view,
     on what the script is actually doing.

    :param log_files:
    :return: list of dicts
    """

    logging.info('Starting the default mode')

    list_of_dicts = list()
    current_path = os.getcwd()
    # go through all given logs and check for each if it is a directory or file and if std_log was missing or not
    for i, file in enumerate(log_files):

        # for the * operation it should skip std_err and std_out files
        if file.endswith(std_err) and not std_err.__eq__("") or file.endswith(std_out) and not std_out.__eq__(""):
            continue

        # else check if file is a valid file
        if os.path.isfile(file) or os.path.isfile(current_path + "/" + file):
            list_of_dicts.append(customize_log(file))

        # if that did not work try std_log at the end of that file
        elif not file.endswith(std_log):
            new_path = file + std_log
            # is this now a valid file ?
            if os.path.isfile(new_path) or os.path.isfile(current_path + "/" + new_path):
                list_of_dicts.append(customize_log(new_path))
            # no file or directory found, even after manipulating the string
            else:
                logging.error(f"No such file with that name or prefix: {file}")
                rprint(f"[red]No such file with that name or prefix: {file}[/red]")

        # The given .log file was not found
        else:
            logging.error(f"No such file: {file}")
            rprint(f"[red]No such file: {file}[/red]")

    if len(list_of_dicts) == 0:
        rprint("[yellow]Nothing found, please use \"man htcompact\" or \"htcompact -h\" for help[/yellow]", end="")

    return list_of_dicts


# Todo: explicitly Job event 000, 001, 005, 006 and 009 are filtered and all other job events that throw errors
#       until now there is not the need for other filters but it might be needed to extend this function
def analyse(log_files: list_of_logs) -> log_inf_list:

    logging.info('Starting the analyser mode')

    if len(log_files) == 0:
        return "No files to analyse"
    elif len(log_files) > 1 and not redirecting_output:
        print("More than one file is given, this mode is meant to be used for single job analysis.\n"
              "This will change nothing, but you should rather do it just for a file one by one")
        if not reading_stdin:
            x = input("Want to continue (y/n): ")
            if x != "y":
                rprint('[red]Process stopped[/red]')
                sys.exit(0)

    result_list = list()

    for file in log_files:
        result_dict = dict()

        try:
            job_events, job_raw_information = read_condor_log(file)
        except NameError as err:
            logging.exception(err)
            rprint("[red]The smart_output_logs method requires a " + std_log + " file as parameter[/red]")

        # initialise with empty values
        submitted_date, executed_date, terminated_date = None, None, None
        ram_histroy = list()
        occurred_errors = list()
        job_dict, res_dict = {"Execution details": [], "Values": []}, dict()

        logging.debug(f"Analysing the HTCondor log file: {file}")
        result_dict["file-description"] = f"{colors['green']}Job analysis of: {file}{colors['back_to_default']}"

        for job, job_inf in zip(job_events, job_raw_information):
            job_id = job[0]

            # job submitted
            if job_id == "000":
                submitted_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job[5])
                if match_host:
                    from_host = match_host[1]
                    # port = match_host[2]
                    job_dict["Execution details"].append("Submitted from host")
                    job_dict["Values"].append(from_host)

            # job executing
            elif job_id == "001":
                executed_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job[5])
                if match_host:
                    host = match_host[1]
                    # if reverse dns lookup, change the ip to cpu: last number
                    if reverse_dns_lookup:
                        host = gethostbyaddr(host)
                    port = match_host[2]

                    job_dict["Execution details"].extend(["Executing on Host", "Port"])
                    job_dict["Values"].extend([host, port])

                else:
                    logging.exception("Host and port haven't been matched correctly")
                    rprint("[red]Your log file has faulty values for the host ip,"
                           " make sure it's a valid IPv4[/red]")

            # job terminated
            elif job_id == "005":  # if the last job event is : Job terminated
                terminated_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                # make a fancy design for the job_information
                job_labels = []  # holds the labels
                job_information = []  # holds the related job information

                # check if termination state is normal or abnormal
                match_termination_state = re.match(r"[\t ]+\([0-9]+\) ((?:\w+ )*)", job_inf[0])
                job_labels.append("Termination State")  # append the termination state
                if match_termination_state:

                    # Normal termination
                    if "Normal termination" in match_termination_state[1]:
                        job_information.append(match_termination_state[1])
                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", job_inf[0])
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return value in the HTCondor log file: {file}")
                            rprint(f"[red]Not a valid return value in the HTCondor log file: {file}[/red]")
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)

                    # Abnormal Termination
                    elif "Abnormal termination" in match_termination_state[1]:
                        job_information.append(f"{colors['red']}Abnormal termination{colors['back_to_default']}")

                        match_signal = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", job_inf[0])
                        signal = match_signal[1] if match_signal else "None"
                        if signal == "None":
                            logging.debug(f"Not a valid return value in the HTCondor log file: {file}")
                            rprint(f"[red]Not a valid return value in the HTCondor log file: {file}[/red]")
                        else:
                            job_labels.append("Return Value")
                            job_information.append(signal)

                        termination_desc = list()
                        # append the reason for the Abnormal Termination
                        for desc in job_raw_information[-1][1:]:
                            if re.match(r"[\t ]+\(0\)", desc):
                                termination_desc.append(desc[5:])
                            else:
                                break

                        if not len(termination_desc) == 0:
                            job_labels.append("Execution details")
                            job_information.append("\n".join(termination_desc))
                        else:
                            logging.debug(f"No termination description for {file} found")

                    else:
                        job_information.append(f"{colors['yellow']}{match_termination_state[1]}{colors['back_to_default']}")
                        logging.debug(f"Don't know this Termination State: {match_termination_state[1]} yet")

                else:
                    job_information.append(f"{colors['red']}Not valid{colors['back_to_default']}")
                    rprint(f"[red]Termination error in HTCondor log file: {file}[/red]")

                job_dict["Execution details"].extend(job_labels)
                job_dict["Values"].extend(job_information)

                # these where the job information now focus on the used resources
                relevant_str = "\n".join(job_raw_information[-1])  # job information of the last job (Job terminated)

                # next part removes not useful lines
                if True:  # just for readability
                    # remove unnecessary lines
                    lines = relevant_str.splitlines()
                    # while not lines[0].startswith("\tPartitionable"):
                    while not re.match(r"[\t ]+Partitionable", lines[0]):
                        lines.remove(lines[0])

                    lines.remove(lines[0])
                    partitionable_res = lines
                    # done, partitionable_resources contain now only information about used resources

                # match all resources
                # ****************************************************************************************************
                match = re.match(r"[\t ]+Cpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+)", partitionable_res[0])
                if match:
                    cpu_usage, cpu_request, cpu_allocated = match[1], match[2], match[3]
                else:
                    logging.debug("Something went wrong reading the cpu information in: " + file)
                    raise_type_error("Something went wrong reading the cpu information")

                match = re.match(r"[\t ]+Disk \(KB\) *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[1])
                if match:
                    disk_usage, disk_request, disk_allocated = match[1], match[2], match[3]
                else:
                    logging.debug("Something went wrong reading the disk information in: " + file)
                    raise raise_type_error("Something went wrong reading the disk information")

                # check if the log has gpu information in between
                gpu_match_index = 2  # can be ignored if gpu is given memory has to access the next index
                gpu_found = False  # easier to check to add information in the dict
                match = re.match(r'[\t ]+Gpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+) "?(.*)"?',
                                 partitionable_res[gpu_match_index])
                if match:
                    gpu_usage, gpu_request, gpu_allocated, gpu_name = match[1], match[2], match[3], match[4]
                    gpu_match_index += 1
                    gpu_found = True

                match = re.match(r"[\t ]+Memory \(MB\)  *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[gpu_match_index])
                if match:
                    memory_usage, memory_request, memory_allocated = match[1], match[2], match[3]
                else:
                    logging.debug("Something went wrong reading the memory information in: " + file)
                    raise_type_error("Something went wrong reading the memory information")
                # ****************************************************************************************************

                # list of resources and their labels
                resource_labels = ["Cpu", "Disk (KB)", "Memory (MB)"]
                usage = [cpu_usage, disk_usage, memory_usage]
                requested = [cpu_request, disk_request, memory_request]
                allocated = [cpu_allocated, disk_allocated, memory_allocated]

                if gpu_found:
                    resource_labels.append("Gpus")
                    usage.append(gpu_usage)
                    requested.append(gpu_request)
                    allocated.append(gpu_allocated)

                # Error handling: change empty values to NaN in the first column
                for i in range(3):
                    if usage[i] == "":
                        usage[i] = np.nan
                    if requested[i] == "":
                        requested[i] = np.nan
                    if allocated[i] == "":
                        allocated[i] = np.nan

                # put the data in the dict
                resource_dict = {
                    "Resources": resource_labels,
                    "Usage": usage,
                    "Requested": requested,
                    "Allocated": allocated
                }

                if gpu_found:
                    resource_dict["Assigned"] = ["", "", "", gpu_name]

                res_dict = manage_thresholds(resource_dict)  # check for thresholds

            # job image size updated
            elif job_id == "006":
                date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                image_size = job[5]
                memory_usage = re.match("[\t ]+([0-9]+)", job_inf[0])[1]
                resident_set_size = re.match("[\t ]+([0-9]+)", job_inf[1])[1]
                ram_histroy.append([date, int(image_size), int(memory_usage), int(resident_set_size)])

            # job aborted
            elif job_id == "009":
                terminated_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                aborted_by_user = ((job_inf[0]).split(" ")[-1])[:-1]
                occurred_errors.append([job_id, terminated_date, "Terminated by user: "+aborted_by_user])

            else:
                # search for errors
                timestamp = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                error_strings = ["error", "err", "warning", "warn", "exception", "fatal"]
                for job_desc in job_inf:
                    for err in error_strings:
                        if err in job_desc.lower():
                            occurred_errors.append([job_id, timestamp,
                                                    re.sub(' \t', '', job_desc)])
                            break  # avoid duplicates

        # append execution details
        result_dict["execution-details"] = job_dict

        # managing the time information
        desc_list = list()
        value_list = list()
        waiting_time = None
        if submitted_date:
            desc_list.append("Submission date")
            value_list.append(submitted_date.strftime("%m/%d %H:%M:%S"))
        if executed_date:
            desc_list.append("Execution date")
            value_list.append(executed_date.strftime("%m/%d %H:%M:%S"))
            if submitted_date:
                waiting_time = executed_date - submitted_date
        if terminated_date:
            desc_list.append("Termination date")
            value_list.append(terminated_date.strftime("%m/%d %H:%M:%S"))
            if waiting_time:
                desc_list.append("Waiting time")
                value_list.append(waiting_time)
            if executed_date:
                runtime = terminated_date - executed_date
                desc_list.append("Execution runtime")
                value_list.append(runtime)
            if submitted_date:
                total_time = terminated_date - submitted_date
                desc_list.append("Total runtime")
                value_list.append(total_time)
        elif waiting_time:
            desc_list.append("Termination date")
            value_list.append("Process still running")
            desc_list.append("Waiting time")
            value_list.append(waiting_time)

        if len(desc_list) > 0:
            time_dict = {
                "Runtime details": desc_list,
                "Times and dates": value_list
            }
            result_dict["times"] = time_dict

        if not len(res_dict) == 0:
            result_dict["all-resources"] = res_dict

        # show HTCondor errors
        if len(occurred_errors) > 0:
            # Todo: is this covering all cases ?
            event_numbers = []
            time_list = []
            err_reason = []

            for err in occurred_errors:
                event_numbers.append(err[0])
                time_list.append(err[1].strftime("%m/%d %H:%M:%S"))
                # if the line is to long, split it
                if len(err[2]) > 50:
                    split = int(len(err[2])/2)
                    while err[2][split] != ' ' and split < len(err[2])-1:
                        split += 1
                    if split > len(err[2])-2:  # if no space was found, change back to normal
                        split = int(len(err[2]) / 2)
                    err_reason.append(err[2][0:split] + '\n' + err[2][split:])
                else:
                    err_reason.append(err[2])

            err_dict = {
                "Time": time_list,
                "Event Number": event_numbers,
                "Reason": err_reason
            }

            result_dict["errors"] = err_dict

        # managing the ram history
        if len(ram_histroy) > 0:
            if len(ram_histroy) > 1:
                np_ram = np.array(ram_histroy)
                ram = np_ram[:, 2]  # second column which is memory usage
                dates = np_ram[:, 0]  # first column

                fig = Figure()
                fig.width = 55
                fig.height = 15
                fig.set_x_limits(min_=min(dates))
                fig.set_y_limits(min_=min(ram))
                fig.y_label = "Usage"
                fig.x_label = "Time"

                # this will use the self written function _num_formatter, to convert the y-label to int values
                fig.register_label_formatter(float, _int_formatter)
                fig.plot(dates, ram, lc='green', label="Continuous Graph")  # Todo: implement a way to show thresholds
                fig.scatter(dates, ram, lc='red', label="Single Values")

                if redirecting_output:  # if redirected, the Legend is useless
                    result_dict["ram-history"] = fig.show()
                else:
                    result_dict["ram-history"] = fig.show(legend=True)
            else:
                single_date = ram_histroy[0][0].strftime("%m/%d %H:%M:%S")
                single_ram = ram_histroy[0][2]
                result_dict["ram-history"] = f"Single memory update found:\n" \
                    f"Memory usage on the {single_date} was updatet to {single_ram} MB"

        result_list.append(result_dict)

    return result_list


# Todo: what means sucessful executed ?
def summarize(log_files: list_of_logs) -> log_inf_list:
    """
    Summarises all used resources and the runtime in total and average

    Runs through the log files via the log_to_dict function

    :return:
    """

    logging.info('Starting the summarizer mode')

    valid_files = len(log_files)
    # no given files
    if valid_files == 0:
        return "No files to summarize"

    # allocated all diffrent datatypes, easier to handle
    result_dict = dict()

    aborted_files = 0
    still_running = 0
    other_exception = 0
    normal_runtime = datetime.timedelta()
    aborted_runtime = datetime.timedelta()
    host_nodes = dict()

    total_cpu_usage = float(0)
    total_disk_usage = int(0)
    total_memory_usage = int(0)

    total_cpu_requested = int(0)
    total_disk_requested = int(0)
    total_memory_requested = int(0)

    total_cpu_allocated = int(0)
    total_disk_allocated = int(0)
    total_memory_allocated = int(0)

    # if gpu given
    gpu_found = False
    total_gpu_usage = float(0)
    total_gpu_requested = int(0)
    total_gpu_allocated = int(0)
    list_of_gpu_names = list()

    for file in track(log_files, transient=True, description="Summarizing..."):
        try:
            log = log_to_dict(file)

            log_description, log_resources = log[0], log[1]
            # continue if Process is still running
            if log_description['Description'][0].__eq__("Process is"):
                still_running += 1
                rprint(f"[orange3]Process of {file} is still running, \n"
                       f"it will be ignored for this summation[/orange3]")
                continue

            elif log_description['Description'][0].__eq__("Process was"):
                aborted_files += 1
                rprint(f"[orange3]Process of {file} has been aborted, \n"
                       f"it will be ignored for this summation[/orange3]")
                continue
            elif len(log_resources) == 0:
                logging.error("if this even get's printed out, more work is needed")
                rprint(f"[orange3]Process of {file} is strange, \n"
                       f"don't know how to handle this yet[/orange3]")
                other_exception += 1
                continue

            normal_runtime += log_description['Values'][3]
            host = log_description['Values'][2]
            if host in host_nodes:
                host_nodes[host][0] += 1
                host_nodes[host][1] += log_description['Values'][3]
            else:
                host_nodes[host] = [1, log_description['Values'][3]]

            total_cpu_usage += float(log_resources['Usage'][0]) \
                if str(log_resources['Usage'][0]).lower() != 'nan' else 0
            total_disk_usage += int(log_resources['Usage'][1])
            total_memory_usage += int(log_resources['Usage'][2])

            total_cpu_requested += int(log_resources['Requested'][0])
            total_disk_requested += int(log_resources['Requested'][1])
            total_memory_requested += int(log_resources['Requested'][2])

            total_cpu_allocated += int(log_resources['Allocated'][0])
            total_disk_allocated += int(log_resources['Allocated'][1])
            total_memory_allocated += int(log_resources['Allocated'][2])

            if len(log_resources['Resources']) == 4:  # this means gpu information is given
                gpu_found = True
                total_gpu_usage += float(log_resources['Usage'][3])
                total_gpu_requested += int(log_resources['Requested'][3])
                if not "allocated-resources" in ignore_list:
                    total_gpu_allocated += int(log_resources['Allocated'][3])
                if log_resources['Assigned'][3] not in list_of_gpu_names:  # append new gpus
                    list_of_gpu_names.append(log_resources['Assigned'][3])  # Todo:

        # Error ocurres when Job was aborted
        except ValueError or KeyError as err:
            logging.exception(err)
            rprint(f"[red]Error with summarizing: {file}[/red]")
            continue
        except TypeError as err:
            logging.exception(err)
            rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
            sys.exit(3)

    n = valid_files - aborted_files - still_running - other_exception # calc diffrenc of successful executed jobs

    average_runtime = normal_runtime / n if n != 0 else normal_runtime
    average_runtime = datetime.timedelta(days=average_runtime.days, seconds=average_runtime.seconds)
    #total_runtime = normal_runtime + aborted_runtime

    exec_dict = {
        "Job types": ["Successful executed jobs"],
        "Occurrence": [n]
    }
    if aborted_files > 0:
        exec_dict["Job types"].append("Aborted jobs")
        exec_dict["Occurrence"].append(aborted_files)
    if still_running > 0:
        exec_dict["Job types"].append("Still running jobs")
        exec_dict["Occurrence"].append(still_running)
    if other_exception > 0:
        exec_dict["Job types"].append("Other exceptions")
        exec_dict["Occurrence"].append(other_exception)


    result_dict["execution-details"] = exec_dict

    # do not even try futher if the only files given have been aborted, are still running etc.
    if n == 0:
        return result_dict

    create_desc = "The following data only implies on sucessful executed jobs"
    if aborted_files > 0 or still_running > 0 or other_exception > 0:
        create_desc += f"\n{colors['light_grey']}" \
            f"Use the analysed-summary mode for more details about the other jobs" \
            f"{colors['back_to_default']}"

    result_dict["summation-description"] = create_desc

    time_desc_list = list()
    time_value_list = list()
    if normal_runtime != datetime.timedelta(0, 0, 0):
        time_desc_list.append("Total runtime of all \nsuccessful executed jobs")
        time_value_list.append(normal_runtime)
    if average_runtime:
        time_desc_list.append("Average runtime per job\n(only successful executed jobs)")
        time_value_list.append(average_runtime)

    time_dict = {
        "Times": time_desc_list,
        "Values": time_value_list
    }
    result_dict["times"] = time_dict

    if n != 0:  # do nothing, if all valid jobs were aborted
        # total_dict = {
        #     "Resources": ['Total Cpu', 'Total Disk (KB)', 'Total Memory (MB)'],
        #     "Usage": [str(round(total_cpu_usage, 2)),
        #               str(total_disk_usage),
        #               str(total_memory_usage)],  # necessary
        #     "Requested": [total_cpu_requested,
        #                   total_disk_requested,
        #                   total_memory_requested],
        #     "Allocated": [total_cpu_allocated,
        #                   total_disk_allocated,
        #                   total_memory_allocated]
        #
        # }

        average_dict = {
            "Resources": ['Average Cpu', 'Avergae Disk (KB)', 'Average Memory (MB)'],
            "Usage": [round(total_cpu_usage / n, 2),
                      round(total_disk_usage / n, 2),
                      round(total_memory_usage / n, 2)],  # necessary
            "Requested": [round(total_cpu_requested / n, 2),
                          round(total_disk_requested / n, 2),
                          round(total_memory_requested / n, 2)],
            "Allocated": [round(total_cpu_allocated / n, 2),
                          round(total_disk_allocated / n, 2),
                          round(total_memory_allocated / n, 2)]

        }

        average_dict = manage_thresholds(average_dict)

        # if gpu information was found
        if gpu_found:
            # gpu_total_dict = {
            #     "Resources": ['Total Gpu'],
            #     "Usage": [total_gpu_usage],  # necessary
            #     "Requested": [total_gpu_requested]
            #     "Allocated":
            # }
            # gpu_average_dict ={
            #     "Resources": ['Average Gpu'],
            #     "Usage": [round(total_gpu_usage / n, 2)],  # Todo: divide by gpu occurrence ????
            #     "Requested": [round(total_gpu_requested / n, 2)],
            #     "Allocated": [round(total_gpu_allocated / n, 2)]
            # }
            average_dict["Resources"].append('Average Gpu')
            average_dict["Usage"].append(round(total_gpu_usage / n, 2))
            average_dict["Requested"].append(round(total_gpu_requested / n, 2))
            average_dict["Allocated"].append(round(total_gpu_allocated / n, 2))
            average_dict["Assigned"] = ["", "", "", ", ".join(list_of_gpu_names)]
            # Todo: each gpu gets a single row

        result_dict["all-resources"] = average_dict

    # Todo: cpu nodes might change over lifetime of a job
    if len(host_nodes) > 0:

        executed_jobs = list()
        runtime_per_node = list()
        for val in host_nodes.values():
            executed_jobs.append(val[0])
            average_job_duration = val[1]/val[0]
            runtime_per_node.append(datetime.timedelta(average_job_duration.days, average_job_duration.seconds))

        cpu_dict = {
            "Host Nodes": list(host_nodes.keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        }

        result_dict["host-nodes"] = cpu_dict

    return [result_dict]


# Todo more specific control what to print out
# Todo ram histogram, BUT HOW
# Todo get files for which the specific termination state appeared ( if not nomral ???)
# Todo: separate Gpus from another
def analysed_summary(log_files: list_of_logs) -> log_inf_list:
    """
        analyse the summarized log files,
        this is meant to give the ultimate output
        about every single log event in average etc.

        Runs through the log files via the read_condor_log function

        :return: string
        """

    logging.info('Starting the analysed summary mode')

    valid_files = len(log_files)
    # no given files
    if valid_files == 0:
        return "No files for the analysed summary"

    # fill this dict with information by the execution type of the jobs
    all_files = dict()
    list_of_gpu_names = list()  # list of gpus found

    # Todo: rich logging handler
    for file in track(log_files, transient=True, description="Summarizing..."):

        try:
            job_events, job_raw_information = read_condor_log(file)
        except NameError as err:
            logging.exception(err)
            rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
            break

        # initialise with empty values
        event_number_order = []
        to_host = ""
        submitted_date, executed_date, terminated_date = None, None, None
        ram_histroy = list()
        occurred_errors = list()
        resource_dict = dict()

        # analyse
        for job, job_inf in zip(job_events, job_raw_information):
            event_number = job[0]
            event_number_order.append(event_number)

            # job submitted
            if event_number == "000":
                submitted_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job[5])
                if match_host:
                    from_host = match_host[1]
                    # port = match_host[2]

            # job executing
            elif event_number == "001":
                executed_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job[5])
                if match_host:
                    to_host = match_host[1]
                    # if resolve ip to hostname, change the ip to cpu: last number
                    if reverse_dns_lookup:
                        to_host = gethostbyaddr(to_host)
                    port = match_host[2]

                else:
                    logging.exception("Host and port haven't been matched correctly")
                    rprint("[red]your log file has faulty values for the host ip,"
                           " make sure it's a valid IPv4[/red]")

            # job terminated
            elif event_number == "005":  # if the last job event is : Job terminated
                terminated_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                # check if termination state is normal or abnormal
                match_termination_state = re.match(r"[\t ]+\([0-9]+\) ((?:\w+ )*)", job_inf[0])
                if match_termination_state:
                    termination_state = match_termination_state[1]
                    # Normal termination
                    if "Normal termination" in termination_state:
                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", job_inf[0])
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return value in the HTCondor log file: {file}")
                            rprint(f"[red]Not a valid return value in the HTCondor log file: {file}[/red]")

                    # Abnormal Termination
                    elif "Abnormal termination" in termination_state:

                        match_signal = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", job_inf[0])
                        signal = match_signal[1] if match_signal else "None"
                        if signal == "None":
                            logging.debug(f"Not a valid return value in the HTCondor log file: {file}")
                            rprint(f"[red]Not a valid return value in the HTCondor log file: {file}[/red]")

                    else:
                        logging.debug(f"Don't know this Termination State: {match_termination_state[1]} yet")

                else:
                    termination_state = "Not valid "
                    rprint(f"[red]Termination error in HTCondor log file: {file}[/red]")

                # these where the job information now focus on the used resources
                relevant_str = "\n".join(job_raw_information[-1])  # job information of the last job (Job terminated)

                # next part removes not useful lines
                if True:  # just for readability
                    # remove unnecessary lines
                    lines = relevant_str.splitlines()
                    # while not lines[0].startswith("\tPartitionable"):
                    while not re.match(r"[\t ]+Partitionable", lines[0]):
                        lines.remove(lines[0])

                    lines.remove(lines[0])
                    partitionable_res = lines
                    # done, partitionable_resources contain now only information about used resources

                # match all resources
                # ****************************************************************************************************
                match = re.match(r"[\t ]+Cpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+)", partitionable_res[0])
                if match:
                    cpu_usage, cpu_request, cpu_allocated = match[1], match[2], match[3]
                else:
                    logging.debug("Something went wrong reading the cpu information in: " + file)
                    raise_type_error("Something went wrong reading the cpu information")

                match = re.match(r"[\t ]+Disk \(KB\) *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[1])
                if match:
                    disk_usage, disk_request, disk_allocated = match[1], match[2], match[3]
                else:
                    logging.debug("Something went wrong reading the disk information in: " + file)
                    raise raise_type_error("Something went wrong reading the disk information")

                # check if the log has gpu information in between
                gpu_match_index = 2  # can be ignored if gpu is given memory has to access the next index
                gpu_found = False  # easier to check to add information in the dict
                match = re.match(r'[\t ]+Gpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+) "?(.*)"?',
                                 partitionable_res[gpu_match_index])
                if match:
                    gpu_usage, gpu_request, gpu_allocated, gpu_name = match[1], match[2], match[3], match[4]
                    gpu_match_index += 1
                    termination_state += "(GPU)"
                    gpu_found = True
                else:
                    termination_state += "(CPU)"

                match = re.match(r"[\t ]+Memory \(MB\)  *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[gpu_match_index])
                if match:
                    memory_usage, memory_request, memory_allocated = match[1], match[2], match[3]
                else:
                    logging.debug("Something went wrong reading the memory information in: " + file)
                    raise_type_error("Something went wrong reading the memory information")
                # ****************************************************************************************************

                # list of resources and their labels
                resource_labels = ["Cpu", "Disk (KB)", "Memory (MB)"]

                # Error handling: change empty values to NaN in the first column

                if cpu_usage == "":
                    cpu_usage = 0

                usage = [float(cpu_usage), int(disk_usage), int(memory_usage)]
                requested = [float(cpu_request), int(disk_request), int(memory_request)]
                allocated = [float(cpu_allocated), int(disk_allocated), int(memory_allocated)]

                if gpu_found:
                    resource_labels.append("Gpus")
                    usage.append(float(gpu_usage))
                    requested.append(float(gpu_request))
                    allocated.append(float(gpu_allocated))

                # create a resource_dict
                resource_dict = {
                    "Resources": resource_labels,
                    "Usage": np.array(usage, dtype=float),
                    "Requested": np.array(requested, dtype=float),
                    "Allocated": np.array(allocated, dtype=float)
                }

                # Todo split different gpus
                if gpu_found:
                    resource_dict["Assigned"] = ["", "", "", gpu_name]
                    if gpu_name not in list_of_gpu_names:  # append new gpus
                        list_of_gpu_names.append(gpu_name)

            # job image size updated
            elif event_number == "006":
                date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                image_size = job[5]
                memory_usage = re.match("[\t ]+([0-9]+)", job_inf[0])[1]
                resident_set_size = re.match("[\t ]+([0-9]+)", job_inf[1])[1]
                ram_histroy.append([date, int(image_size), int(memory_usage), int(resident_set_size)])

            # job aborted
            elif event_number == "009":
                terminated_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                aborted_by_user = ((job_inf[0]).split(" ")[-1])[:-1]
                occurred_errors.append([event_number, terminated_date, "aborted", "Terminated by user: "+aborted_by_user])

            else:
                # search for errors
                timestamp = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                error_strings = ["error", "err", "warning", "warn", "exception", "fatal"]
                for job_desc in job_inf:
                    for err in error_strings:
                        if err in job_desc.lower():
                            occurred_errors.append([event_number, timestamp, err,
                                                    re.sub(' \t', '', job_desc)])
                            break  # avoid duplicates

        # managing the time information
        waiting_time = datetime.timedelta()
        runtime = datetime.timedelta()
        total_time = datetime.timedelta()
        if executed_date and submitted_date:
            waiting_time = executed_date - submitted_date
        if terminated_date and executed_date:
            runtime = terminated_date - executed_date
        if terminated_date and submitted_date:
            total_time = terminated_date - submitted_date

        # still running ?
        if not terminated_date:
            today = datetime.datetime.now()
            today = today.replace(year=1900, microsecond=0)  # format the date
            if executed_date:
                runtime = today - executed_date
            if submitted_date:
                total_time = today - submitted_date

        # show HTCondor errors
        if len(occurred_errors) > 0:
            # Todo: is this covering all cases ?
            event_numbers = []
            time_list = []
            err_keyword = []
            err_reason = []

            for err in occurred_errors:
                event_numbers.append(err[0])
                time_list.append(err[1].strftime("%m/%d %H:%M:%S"))
                err_keyword.append(err[2].lower())
                # if the line is to long, split it
                if len(err[3]) > 50:
                    split = int(len(err[3]) / 2)
                    while err[3][split] != ' ' and split < len(err[2]) - 1:
                        split += 1
                    if split > len(err[3]) - 2:  # if no space was found, change back to normal
                        split = int(len(err[3]) / 2)
                    err_reason.append(err[3][0:split] + '\n' + err[3][split:])
                else:
                    err_reason.append(err[3])

            # err_dict = {
            #     "Time": time_list,
            #     "Event Number": event_numbers,
            #     "Error": err_keyword,
            #     "Reason": err_reason
            # }

        # if last termination was not 005, get last occurred error
        if event_number_order[-1] in ["002", "003", "004", "007", "009",
                                      "010", "018", "021", "022", "026", "029"]:
            termination_type = err_keyword[-1]
        elif event_number_order[-1].__eq__("005"):
            termination_type = termination_state
        else:
            termination_type = "running"

        try:
            if termination_type in all_files:
                # logging.debug(all_files[termination_type])
                all_files[termination_type][0] += 1  # count number
                all_files[termination_type][1] += waiting_time
                all_files[termination_type][2] += runtime
                all_files[termination_type][3] += total_time
                if not len(all_files[termination_type][4]) == 0:
                    # add usages

                    all_files[termination_type][4]["Usage"] += resource_dict["Usage"]
                    # add requested
                    all_files[termination_type][4]["Requested"] += resource_dict["Requested"]
                    # allocated
                    all_files[termination_type][4]["Allocated"] += resource_dict["Allocated"]

                # add cpu
                if to_host != "":
                    if to_host in all_files[termination_type][5].keys():
                        all_files[termination_type][5][to_host][0] += 1
                        all_files[termination_type][5][to_host][1] += total_time
                    else:
                        all_files[termination_type][5][to_host] = [1, total_time]
                elif 'Aborted before submission' in all_files[termination_type][5].keys():
                        all_files[termination_type][5]['Aborted before submission'][0] += 1
                        all_files[termination_type][5]['Aborted before submission'][1] += total_time
                else:
                    count_host_nodes = dict()
                    count_host_nodes['Aborted before submission'] = [1, total_time]
                    all_files[termination_type][5] = count_host_nodes

            # else new entry
            else:
                # if host exists
                if to_host != "":
                    count_host_nodes = dict()
                    count_host_nodes[to_host] = [1, total_time]
                else:
                    count_host_nodes = dict()
                    count_host_nodes['Aborted before submission'] = [1, total_time]
                all_files[termination_type] = [1, waiting_time, runtime, total_time, resource_dict, count_host_nodes]

        # Todo: test if valid here
        # Error ocurres when Job was aborted
        except ValueError or KeyError as err:
            logging.exception(err)
            rprint(f"[red]Error with summarizing: {file}[/red]")
            continue
        except TypeError as err:
            logging.exception(err)
            rprint(f"[red] {err}[/red]")
            sys.exit(3)

    # Now put everything together
    result_list = list()
    for term_state in all_files:
        term_info = all_files[term_state]
        result_dict = dict()

        # differentiate between terminated and running processes
        if term_state.__ne__("running"):
            result_dict["file-description"] = f"{colors['blue']}" \
                                              f"All files summarized with the Termination State: {term_state}" \
                                              f"{colors['back_to_default']}"
        else:
            result_dict["file-description"] = f"{colors['blue']}" \
                                              f"All files summarized, that are currently running:" \
                                              f"{colors['back_to_default']}"

        n = int(term_info[0])
        desc_dict = {
            "Description": ["Occurence"],
            "Values": [n]
        }

        times = np.array([term_info[1], term_info[2], term_info[3]])
        av_times = times/n
        format_av_times = [datetime.timedelta(days=time.days, seconds=time.seconds) for time in av_times]

        time_dict = {
            "Times": ["Waiting Time", "Runtime", "Total"],
            "Total": times,
            "Average": format_av_times
        }

        result_dict["execution-details"] = desc_dict

        result_dict["times"] = time_dict

        if not len(term_info[4]) == 0:
            total_resources_dict = term_info[4]
            avg_dict = {
                'Resources': ['Average Cpu', ' Average Requested ', 'Average Allocated'],
                'Usage': np.round(total_resources_dict['Usage'] / term_info[0], 2).tolist(),
                'Requested': np.round(total_resources_dict['Requested'] / term_info[0], 2).tolist(),
                'Allocated': np.round(total_resources_dict['Allocated'] / term_info[0], 2).tolist()
            }
            if 'Assigned' in total_resources_dict.keys():
                avg_dict['Resources'].append('Gpu')
                avg_dict['Assigned'] = ['', '', '', ", ".join(list_of_gpu_names)]

            avg_dict = manage_thresholds(avg_dict)
            result_dict["all-resources"] = avg_dict

        executed_jobs = list()
        runtime_per_node = list()
        for val in term_info[5].values():
            executed_jobs.append(val[0])
            average_job_duration = val[1] / val[0]
            runtime_per_node.append(datetime.timedelta(average_job_duration.days, average_job_duration.seconds))

        host_nodes_dict = {
            "Host Nodes": list(term_info[5].keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        }

        result_dict["host-nodes"] = host_nodes_dict

        result_list.append(result_dict)

    return result_list


# search in the files for the keywords
# Todo: show which keywords have been found and which not
# Todo: AND option, ONLY option
def filter_for(log_files: list_of_logs, keywords: list, extend=False) -> log_inf_list:
    """
    Filter for a list of keywords, which can be extended
    and print out every file which matches the pattern (not case sensitive)
    The filtered files can be analysed summarise, etc afterwards,
    else this function will return None

    :param log_files:
    :param keywords:
    :param extend:
    :return:
        list with dicts depending on the used mode, to forward the filtered files,

        None if no forwarding is set
    """
    logging.info('Starting the filter mode')

    # if the keywords are given as a string, try to create a list
    if type(keywords) == list:
        keyword_list = keywords
    elif type(keywords) == str:
        keyword_list = " ".join(re.split(',| ', keywords)).split()  # remove spaces and commas from string
    else:
        logging.debug(f"Filter mode only accepts a string or list with keywords, not {keywords}")
        raise_type_error("Expecting a list or a string")

    # if extend is set, keywords like err will also look for keywords like warn exception, aborted, etc.
    if extend:  # apply some rules
        # the error list
        err_list = ["err", "warn", "exception", "aborted", "abortion", "abnormal", "fatal"]

        # remove keyword if already in err_list
        for i in range(len(keyword_list)):
            if (keyword_list[i]).lower() in err_list:
                keyword_list.remove(keyword_list[i])

        keyword_list.extend(err_list)  # extend search

        rprint(f"[green]Keyword List was extended, now search for these keywords:[/green]", keyword_list)
    else:
        rprint(f"[green]Search for these keywords:[/green]", keyword_list)

    if len(keyword_list) == 1 and keyword_list[0] == "":
        logging.debug("Empty filter, don't know what to do")
        return f"{colors['yellow']}Don't know what to do with an empty filter,\n" \
            f"if you activate the filter mode in the config file, \n" \
            f"please add a [filter] section with the filter_keywords = your_filter"

    logging.debug(f"These are the keywords to look for: {keyword_list}")

    # now search
    found_at_least_one = False
    found_logs = []
    for file in log_files:
        found = False
        with open(file, "r") as read_file:
            for line in read_file:
                for keyword in keyword_list:
                    if re.search(keyword.lower(), line.lower()):
                        if not found_at_least_one:
                            print("Matches:")
                        # Todo: save numbers of occurence for each keyword, if more than one is given
                        # Todo: can be commenetd, or maybe should be hidden, if more than like 50 files are found
                        rprint(f"[grey74]{keyword} in:\t{file}[/grey74] ")
                        found = True
                        found_at_least_one = True
                        break
                if found:
                    found_logs.append(file)
                    break

    return_dicts = None
    if not found_at_least_one:
        rprint(f"[red]Unable to find these keywords:[/red]", keyword_list)
        rprint(f"[red]maybe try again with similar expressions[/red]")

    elif mode is not None:
        print(f"Total count: {len(found_logs)}")
        if mode.__eq__("default"):
            return_dicts = default(found_logs)
        elif mode.__eq__("analysed-summary"):
            rprint("[magenta]Try to give an analysed summary for these files[/magenta]")
            return_dicts = analysed_summary(found_logs)
        elif mode.__eq__("summarize"):
            rprint("[magenta]Try to summarize these files[/magenta]")
            return_dicts = summarize(found_logs)
        elif mode.__eq__("analyse"):
            rprint("[magenta]Try to analyse these files[/magenta]")
            return_dicts = analyse(found_logs)
    elif not reading_stdin and not redirecting_output:  # if not reading from stdin or redirected
        rprint("[blue]Want do do more?[/blue]")
        x = input("default(d), summarize(s), analyse(a), analysed summary(as), exit(e): ")
        if x == "d":
            return_dicts = default(found_logs)
        elif x == "s":
            return_dicts = summarize(found_logs)
        elif x == "a":
            return_dicts = analyse(found_logs)
        elif x == "as":
            return_dicts = analysed_summary(found_logs)
        elif x == "e":
            sys.exit(0)
        else:
            print('Not a valid argument, quitting ...')
            sys.exit(0)

    return return_dicts


def validate_given_logs(file_list: list_of_logs) -> list_of_logs:
    """
    This method is supposed to take the given log files (by config or command line argument)
    and tries to determine if these are valid log files, ignoring the std_log specification, if std_log = "".

    It will run through given directories and check every single file, if accessible, for the
    HTCondor log file standard, valid log files should start with lines like:

    000 (5989.000.000) 03/30 15:48:47 Job submitted from host: <10.0.8.10:9618?addrs=10.0.8.10-9618&noUDP&sock=3775629_0774_3>

    if done, it will change the global files list and store only valid log files.
    The user will be remind, what files were accepted as "valid".

    The user will also be informed if a given file was not found.

    Todo: In addition there should be an option --force, that makes the script stop, if the file was not found or marked as valid

    Todo: user should be able to get the option to decide, when the same files appear more than once
    -> my guess: yes or no question, if nothing is given in under 10 seconds, it should go with no
    -> this should prevent, that the script is stucked, if the user is for example running it over night

    """
    valid_files = list()
    total = len(file_list)

    logging.info('Validate given log files')
    with Progress(transient=True) as progress:

        task = progress.add_task("Validating...", total=total, start=False)

        for arg in file_list:

            path = os.getcwd()  # current working directory , should be condor job summarizer script
            logs_path = path + "/" + arg  # absolute path

            working_dir_path = ""
            working_file_path = ""

            if os.path.isdir(arg):
                working_dir_path = arg
            elif os.path.isdir(logs_path):
                working_dir_path = logs_path

            elif os.path.isfile(arg):
                working_file_path = arg
            elif os.path.isfile(logs_path):
                working_file_path = logs_path
            # check if only the id was given and resolve it with the std_log specification
            elif os.path.isfile(arg+std_log):
                working_file_path = arg+std_log
            elif os.path.isfile(logs_path+std_log):
                working_file_path = logs_path+std_log

            # if path is a directory
            if working_dir_path.__ne__(""):
                rprint(f"[light_coral]Try to find valid log files in the given directory:[/light_coral]\n"
                       f"{working_dir_path}")
                # run through all files and separate the files into log/error and output files
                valid_dir_files = list()
                file_dir = os.listdir(working_dir_path)
                total += len(file_dir) - 1
                for file in file_dir:
                    progress.update(task, total=total, advance=1)

                    # ---- if std_log is set ignore other log files with a different suffix ----
                    if std_log.__ne__("") and not file.endswith(std_log):
                        logging.debug("Ignoring this file, " + file + ", because std_log is set to: "+ std_log)
                        continue

                    # ignore hidden files
                    if file.startswith("."):
                        continue
                    # if it's not a sub folder
                    if working_dir_path.endswith('/'):
                        file_path = working_dir_path + file
                    else:
                        file_path = working_dir_path + '/' + file

                    if os.path.isfile(file_path):
                        with open(file_path, "r") as read_file:
                            # Todo: better specification with re
                            if os.path.getsize(file_path) == 0:  # file is empty
                                continue

                            if re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)", read_file.readlines()[0]):
                                # logging.debug(f"{read_file.name} is a valid HTCondor log file")
                                valid_dir_files.append(file_path)

                    else:
                        logging.debug(f"Found a subfolder: {working_dir_path}/{file}, it will be ignored")
                        progress.console.print(f"[yellow]Found a subfolder: {working_dir_path}/{file},"
                                               f" it will be ignored[/yellow]")

                valid_files.extend(valid_dir_files)
                rprint(f"[green]Found {len(valid_dir_files)} valid log files out of "
                       f"{len(file_dir)} files[/green]\n")
            # else if path "might" be a valid HTCondor file
            elif working_file_path.__ne__(""):
                progress.update(task, advance=1)

                # ---- if std_log is set ignore other log files with a different suffix ----
                if std_log.__ne__("") and not working_file_path.endswith(std_log):
                    logging.debug("Ignoring this file, " +
                                  working_file_path + ", because std_log is set to: " + std_log)
                    continue

                if std_err.__ne__("") and working_file_path.endswith(std_err) or \
                        std_out.__ne__("") and working_file_path.endswith(std_out):
                    logging.debug(f"Only log files accepted, ignored this file: {working_file_path}")
                    continue

                with open(working_file_path, "r") as read_file:

                    if os.path.getsize(working_file_path) == 0:  # file is empty
                        progress.console.print(f"[red]How dare you, the file is empty: {read_file.name} :([/red]")

                    elif re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)", read_file.readlines()[0]):
                        logging.debug(f"{read_file.name} is a valid HTCondor log file")
                        valid_files.append(working_file_path)
                    else:
                        logging.debug(f"The given file {read_file.name} is not a valid HTCondor log file")
                        progress.console.print(f"[yellow]The given file {read_file.name} "
                                               f"is not a valid HTCondor log file[/yellow]",)
            else:
                logging.error(f"The given file: {arg} does not exist")
                rprint(f"[red]The given file: {arg} does not exist[/red]")

    return valid_files


def setup_logging_tool(log_file=sys.argv[0]+'.log'):
    """
        Set up the logging device,
        to generate a log file, with --generate-log-file
        or to print more descriptive output with the verbose mode to stdout

        both modes are compatible together
    :return:
    """

    # disable the loggeing tool by default
    logging.getLogger().disabled = True

    # I don't know why a root handler is already set,
    # but we have to remove him in order
    # to get just the output of our own handler
    if len(logging.root.handlers) == 1:
        default_handler = logging.root.handlers[0]
        logging.root.removeHandler(default_handler)

    # if logging tool is set to use
    if generate_log_file:
        # activate logger if not already activated
        logging.getLogger().disabled = False
        # logging.basicConfig(filename=log_file,
        #                     level=logging.DEBUG,
        #                     format=logging_format)

        logging_file_format = '%(asctime)s - [%(funcName)s:%(lineno)d] %(levelname)s : %(message)s'
        file_formatter = logging.Formatter(logging_file_format)

        handler = RotatingFileHandler(log_file, maxBytes=1000000, backupCount=1)
        handler.setLevel(logging.DEBUG)
        handler.setFormatter(file_formatter)

        log = logging.getLogger()
        log.setLevel(logging.DEBUG)
        log.addHandler(handler)

    if verbose_mode:
        # activate logger if not already activated
        logging.getLogger().disabled = False
        # logging.basicConfig(stream=sys.stdout,
        #                     level=logging.DEBUG,
        #                     format=logging_stdout_format)

        logging_stdout_format = '%(asctime)s - %(levelname)s: %(message)s'
        stdout_formatter = logging.Formatter(logging_stdout_format)

        stdout_handler = logging.StreamHandler(sys.stdout)
        #stdout_handler = rlog.RichHandler(logging.DEBUG, console=sys.stdout)
        stdout_handler.setLevel(logging.DEBUG)
        stdout_handler.setFormatter(stdout_formatter)
        log = logging.getLogger()
        log.setLevel(logging.DEBUG)
        log.addHandler(stdout_handler)


# Todo: a function to format the output
def customize_results(files: list_of_logs):

    output_string = ""

    if filter_mode:
        results = filter_for(files, filter_keywords, filter_extended)
    elif mode.__eq__("default"):
        results = default(files)  # force default with -d
    elif mode.__eq__("analysed-summary"):
        results = analysed_summary(files)  # analysed summary ?
    elif mode.__eq__("summarize"):
        results = summarize(files)  # summarize information
    elif mode.__eq__("analyse"):
        results = analyse(files)  # analyse the given files
    else:
        results = default(files)  # anyways try to print default output

    # This can happen, when for example the filter mode is not forwarded
    if results is None:
        sys.exit(0)

    work_with = results
    # convert result to list, if given as dict
    if type(results) == dict:
        work_with = [results]

    # check for ignore values
    for i in range(len(work_with)):
        mystery = work_with[i]

        if "execution-details" in mystery:
            if "execution-details" in ignore_list:
                del mystery["execution-details"]
            else:
                mystery["execution-details"] = tabulate(mystery["execution-details"],
                                                        headers='keys', tablefmt=table_format)

        if "times" in mystery:
            if "times" in ignore_list:
                del mystery["times"]
            else:
                mystery["times"] = tabulate(mystery["times"], showindex=False, headers='keys', tablefmt=table_format)

        if "all-resources" in mystery:
            resources = mystery["all-resources"]
            if "all-resources" in ignore_list:
                del mystery["all-resources"]
            else:
                if "used-resources" in ignore_list:
                    del mystery["all-resources"]["Usage"]
                if "requested-resources" in ignore_list:
                    del mystery["all-resources"]["Requested"]
                if "allocated-resources" in ignore_list:
                    del mystery["all-resources"]["Allocated"]

                mystery["all-resources"] = tabulate(resources, showindex=False, headers='keys',
                                                    tablefmt=table_format)

        if "errors" in mystery:
            if "errors" in ignore_list:
                del mystery["errors"]
            else:
                mystery["errors"] = "Occurred errors: \n" +\
                                    tabulate(mystery["errors"], showindex=False, headers='keys', tablefmt='grid')

        if "host-nodes" in mystery:
            if "host-nodes" in ignore_list:
                del mystery["host-nodes"]
            else:
                mystery["host-nodes"] = tabulate(mystery["host-nodes"], showindex=False,
                                                 headers='keys', tablefmt=table_format)

        # Todo: show-more section

        for _i in mystery:
            if mystery[_i] is not None:
                output_string += mystery[_i]
            else:
                logging.debug("This musst be fixed, NoneType found.")
                rprint("[red]NoneType object found, this should not happen[/red]")

            if mystery[_i] == mystery[list(mystery.keys())[-1]]:
                output_string += "\n"
            else:
                output_string += "\n\n"

        output_string += "\n"

    return output_string

    # these might be added in future:
    # Todo: gpus, cpu, warnings


def run(commandline_args):
    """
    Run this script

    This searches first for a given config file.
    After that, given parameters in the terminal will be interpreted, so they have a higher priority
    given files will be validated and the logging tool will be managed
    After that the user input will be used to process the output to the terminal,
    which will contain information about the given log files

    :return:
    """
    try:

        initialize()  # initialize global parameters

        check_for_redirection()  # set the color sequenzes to ""

        # if exit parameters are given that will interrupt this script, like --help,
        # catch them here so the config won't be unnecessary loaded
        manage_prioritized_params(commandline_args)

        # if not --no-config is set:
        # interpret the first file, that can be interpreted as a config file and remove it, other given config files,
        # (you will see when you try, files will be interpreted as HTCondor log files)
        # so it's not possible to give multiple config files
        found_conf = None
        if not no_config:
            found_conf, commandline_args = find_config(commandline_args)

        setup_logging_tool()  # set up the logging device

        logging.debug("-------Start of htcompact scipt-------")

        if verbose_mode:
            logging.info('Verbose mode turned on')

        if reading_stdin:
            logging.debug("Reading from stdin")
        if redirecting_output:
            logging.debug("Output is getting redirected")

        if found_conf is not None:
            load_config(found_conf)

        # after that, interpret the command line arguments, these will have a higher priority, than the config setup
        # and by that arguments set in the config file, will be ignored
        manage_params(commandline_args)

        global files
        valid_files = validate_given_logs(files)  # validate the files, make a list of all valid config files

        if len(valid_files) == 0:
            rprint("[red]No valid HTCondor log files found[/red]")
            sys.exit(2)

        output_str = customize_results(valid_files)

        print(output_str)  # write it to the console

        logging.debug("-------End of htcompact script-------")

        sys.exit(0)

    except TypeError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        sys.exit(3)

    except KeyboardInterrupt:
        logging.info("Script was interrupted by the user")
        print("Script was interrupted")
        sys.exit(4)


if __name__ == "__main__":
    """
    This is the main function, which runs the script, if not imported as a module

    :return:
    """
    run(sys.argv[1:])



