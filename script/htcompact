#!/usr/bin/env python3
import re
import sys
import os
import getopt
import datetime
import logging
from logging.handlers import RotatingFileHandler

import socket
import configparser
import numpy as np
import htcondor
from htcondor import JobEventType as jet
from rich import print as rprint
from rich import logging as rlog
from rich.progress import track, Progress
from tabulate import tabulate
from plotille import Figure

from typing import List

log_inf_list = List[dict]
list_of_logs = List[str]
date_time = datetime.datetime
timedelta = datetime.timedelta

"""

This script is basically reading information from HTCondor log files and storing them into
dictionaries. By that its actually pretty simple to analyse the data.

The script makes the information compact and easy to under stand,
that's the reason for the name htcompact

Single logs can be read quite easily,
but also it's possible to summarize a whole directory with logs
to see for ex. the average runtime and usage of all the logs

"""

# Exit Codes
"""
 Normal Termination: 0
 Wrong Options or Arguments: 1
 No given files: 2
 TypeError: 3
 Keyboard interruption: 4
"""


def initialize():
    """
    This method initializes all global variables
    :return:
    """
    global accepted_states, files, option_shorts, option_longs, \
        ignore_list, allowed_ignore_values, no_config, reverse_dns_lookup, \
        store_dns_lookups, verbose_mode, \
        to_csv, generate_log_file, filter_mode, filter_keywords, filter_extended, \
        colors, tolerated_usage_threshold, bad_usage_threshold, std_log, std_err, std_out, \
        default_configfile, table_format, reading_stdin, redirecting_output, \
        show_list, allowed_show_values, mode, allowed_modes

    # global parameters, used for dynamical output of information
    accepted_states = ["true", "yes", "y", "enable", "enabled", "0"]
    files = list()
    option_shorts = "hsavm:"
    option_longs = ["help", "version", "verbose", "mode=",
                    "std-log=", "std-err=", "std-out=",
                    "ignore=", "show-more=", "no-config", "to-csv",
                    "generate-log-file", "filter=", "extend", "print-event-table",
                    "reverse-dns-lookup", "table-format="]

    # show more information, mostly given by HTCompact .err and .out files
    mode = None
    allowed_modes = {"a": "analyse",
                     "s": "summarize",
                     "as": "analysed-summary",
                     "d": "default"}

    show_list = list()
    allowed_show_values = ["std-err", "std-out"]

    # ignore information
    ignore_list = list()
    allowed_ignore_values = ["execution-details", "times", "host-nodes",
                             "used-resources", "requested-resources",
                             "allocated-resources", "all-resources",
                             "errors"]

    # if set do not use a config file, even if one was found
    no_config = False

    # Features:
    reverse_dns_lookup = False
    store_dns_lookups = dict()
    to_csv = False

    # logging tool
    generate_log_file = False
    verbose_mode = False

    # filter mode
    filter_mode = False
    filter_keywords = list()
    filter_extended = False

    # escape sequences for colors
    colors = {
        'red': "\033[0;31m",
        'green': "\033[0;32m",
        'yellow': "\033[0;33m",
        'magenta': "\033[0;35m",
        'cyan': "\033[0;36m",
        'blue': "\033[0;34m",
        'light_grey': "\033[37m",
        'back_to_default': "\033[0;39m"
    }

    # thresholds for bad and low usage of resources
    tolerated_usage_threshold = 0.1
    bad_usage_threshold = 0.25

    # global variables with default values for err/log/out files
    std_log = ""
    std_err = ".err"
    std_out = ".out"

    # global defaults
    default_configfile = "htcompact.conf"
    table_format = "pretty"  # ascii by default

    reading_stdin = False
    redirecting_output = False


def check_for_redirection():
    """
    This method should be activated first, when output is generated
    it does nothing, if output is printed to the terminal.
    If output gets redirected with > or | or other redirection tools, ignore escape sequences
    by setting them to ""
    :return:
    """
    # if output gets redirected with > or | or other redirection tools, ignore escape sequences
    global reading_stdin, redirecting_output, stdin_input  # it is easier to save this globaly, because if whe change

    reading_stdin = not sys.stdin.isatty()
    redirecting_output = not sys.stdout.isatty()

    if reading_stdin:
        stdin_input = sys.stdin.readlines()

    if redirecting_output:
        global colors
        colors = {
            'red': "",
            'green': "",
            'yellow': "",
            'magenta': "",
            'cyan': "",
            'blue': "",
            'light_grey': "",
            'back_to_default': ""
        }


def remove_files_from_args(args: list, short_opts: str, long_opts: list):
    """
        I want to make it easier for the user to insert many files at once.
        The script should detect option arguments and files

        It will filter the given files and remove them from the argument list

        !!!Exactly that is this method doing!!! (because getopt has no such function)

    :param args: is supposed to be a list, in any case it should be the sys.argv[1:] list
    :param short_opts: the short getopt options
    :param long_opts: the long getopt options
    :return: list of getopt arguments
    """
    if type(args) != list:
        raise_value_error("Expecting a list as argument")

    new_args = args.copy()
    generate_files = list()
    index = 0
    # run through the command line arguments,
    # skip the getopt argument and interpret everything else as a file
    while True:
        if index >= len(args):
            break

        arg = args[index]

        is_long_arg = False

        if arg.startswith("-"):  # for short_args
            arg = arg[1:]
            if arg.startswith("-"):  # for long_args
                arg = arg[1:]
                is_long_arg = True

             # skip the argument if getopt arguments are setable
            if not is_long_arg:
                for i, s_arg in enumerate(short_opts):
                    if s_arg == arg and i < len(short_opts)-1 and short_opts[i+1] == ":":
                        index += 1
                        break
            else:
                for l_arg in long_opts:
                    if l_arg.startswith(arg) and l_arg.endswith("="):
                        index += 1
                        break

            # if arg + ":" in short_opts:
            #     index += 1
            # elif arg + "=" in long_opts:
            #     index += 1
        else:
            generate_files.append(arg)
            new_args.remove(arg)

        index += 1

    # change files, if found
    if len(generate_files) > 0:
        global files
        files = generate_files

    # return valid arguments
    return new_args


def manage_prioritized_params(args: list):
    if type(args) != list:
        raise_value_error("Expecting a list as argument")

    global option_shorts, option_longs
    global no_config, generate_log_file, verbose_mode

    # if for whatever reasons files were given
    better_args = remove_files_from_args(args, option_shorts, option_longs)

    try:
        opts, args = getopt.getopt(better_args, option_shorts, option_longs)

        for opt, arg in opts:

            # system exit params
            if opt in ["-h", "--help"]:
                print(small_help())
                sys.exit(0)
            if opt.__eq__("--version"):
                print(f"Version: v1.1.0")
                sys.exit(0)
            if opt.__eq__("--print-event-table"):
                print(get_event_information())
                sys.exit(0)

            # prioritized management params,
            if opt in ["-v", "--verbose"]:
                verbose_mode = True
            elif opt.__eq__("--no-config"):
                no_config = True
            elif opt.__eq__("--generate-log-file"):
                generate_log_file = True

    # print error messages
    except Exception as err:
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        rprint("[dark_olive_green1]Use '--help' or 'man htcompact' for help [/dark_olive_green1]")
        sys.exit(1)


def manage_params(args: list):
    """
    Interprets the given command line arguments and changes the global variables in this scrips

    """
    global files  # list of files and directories
    global mode  # what mode are we on
    global std_log, std_err, std_out  # all default values for the HTCondor files
    global show_list  # show more
    global ignore_list  # ignore information variables
    global to_csv
    global filter_mode, filter_keywords, filter_extended  # search features
    global table_format  # table_format can be changed
    global reverse_dns_lookup  # if set host ip's will be looked up in dns server

    global option_shorts, option_longs

    summarizer_mode = False
    analyser_mode = False

    all_args = args

    # listen to stdin and add these files
    if reading_stdin:  # else the script will be waiting fro stdin
        logging.debug("Listening to arguments from stdin")
        for line in stdin_input:
            all_args.append(line.rstrip('\n'))

    better_args = remove_files_from_args(all_args, option_shorts, option_longs)

    try:
        opts, args = getopt.getopt(better_args, option_shorts, option_longs)

        for opt, arg in opts:

            # catch unusual but not wrong parameters starting with -
            if arg.startswith("-"):
                rprint("[yellow]The argument for {0} is {1}, "
                       "is that wanted?[/yellow]".format(opt, arg))
                logging.warning("The argument for {0} is {1}, "
                                "is that wanted?".format(opt, arg))

            elif opt in ["-m", "--mode"]:

                if arg in allowed_modes.keys():
                    mode = allowed_modes.get(arg)
                elif arg in allowed_modes.values():
                    mode = arg
                else:
                    raise_value_error("Invalid argument '" + arg + "'\n"
                                                                   "Valid arguments:\n" +
                                      ", ".join("'{}' or '{}'".format(key, value)
                                                for key, value in allowed_modes.items()))

                logging.debug("Starting the: " + mode + " mode")

            elif opt.__eq__("-s"):
                summarizer_mode = True
            elif opt.__eq__("-a"):
                analyser_mode = True
            elif opt in ["--filter"]:
                filter_mode = True
                filter_keywords = " ".join(re.split(',| ', arg)).split()
            elif opt in ["--extend"]:
                filter_extended = True
            # all HTCondor files, given by the user if they are not saved in .log/.err/.out files
            elif opt.__eq__("--std-log"):
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "." + arg
                std_log = arg
            elif opt.__eq__("--std-err"):
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "." + arg
                std_err = arg
            elif opt.__eq__("--std-out"):
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "." + arg
                std_out = arg

            # show more specific information
            elif opt.__eq__("--show-more"):
                keywords = " ".join(re.split(',| ', arg)).split()
                for keyword in keywords:
                    if keyword in allowed_show_values:
                        show_list.append(keyword)
                    else:
                        raise_value_error("Invalid argument '" + keyword + "'\n"
                                                                           "Valid arguments:\n" + ", ".join(
                            allowed_show_values))
                logging.debug("Show these information: " + ", ".join(show_list))

            # all variables to ignore unwanted information
            elif opt.__eq__("--ignore"):
                keywords = " ".join(re.split(',| ', arg)).split()
                for keyword in keywords:
                    if keyword in allowed_ignore_values:
                        ignore_list.append(keyword)
                    else:
                        raise_value_error("Invalid argument '" + keyword + "'\n"
                                                                           "Valid arguments:\n" + ", ".join(
                            allowed_ignore_values))
                logging.debug("Ignore these information: " + ", ".join(ignore_list))

            elif opt.__eq__("--to-csv"):
                to_csv = True
                rprint("[red]--to-csv not handled yet[/red]")
                sys.exit(0)

            elif opt.__eq__("--table-format"):
                types = "plain,simple,github,grid,fancy_grid,pipe," \
                        "orgtbl,rst,mediawiki,html,latex,latex_raw," \
                        "latex_booktabs,tsv,pretty"
                # only valid arguments
                if arg in types.split(","):
                    table_format = arg
                else:
                    logging.debug("The given table format doesn't exist")

            elif opt.__eq__("--reverse-dns-lookup"):
                reverse_dns_lookup = True

            # these are already managed in manage_prioritized_params
            # need to be caught, cause of exception cases
            elif opt.__eq__("--no-config"):
                pass
            elif opt.__eq__("--generate-log-file"):
                pass
            elif opt in ["-v", "--verbose"]:
                pass

            else:
                rprint("[red]Option not handled yet[/red]")
                sys.exit(0)
    # print error messages
    except Exception as err:
        logging.exception(err)  # write a detailed description in the stdout.log file
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        rprint("[dark_olive_green1]Use '--help' or 'man htcompact' for help [/dark_olive_green1]")
        sys.exit(1)

    if summarizer_mode and analyser_mode:
        mode = "analysed-summary"
    elif summarizer_mode:
        mode = "summarize"
    elif analyser_mode:
        mode = "analyse"

    if len(files) == 0:
        logging.debug("No files given")
        rprint("[red]No files given[/red]")
        sys.exit(2)


def small_help() -> str:
    """
Usage:
  htcompact [file ...] [-hsav] [-m mode] [--help] [--version] [--verbose]
            [--mode mode] [--std-log suffix] [--std-out suffix]
            [--std-err suffix] [--ignore keywords] [--show-more keywords]
            [--no-config] [--to-csv] [--generate-log-file]
            [--filter keywords [--extend]] [--print-event-table]
            [--reverse-dns-lookup] [--table-format tablefmt]

----------------------------Main features:---------------------------------

 --mode summary
         summarizes all given HTCondor log files and return a result
         regarding the averange usages and runtimes, this only makes
         sense, if multiple log files are given

 --mode analyse
        analyses a specific file, for occurred errors, ram history ->
        histogram, execution status, runtime and much more

 --mode analysed-summary
        A combination of both modes, which gives a more detailed overview

 More detailed descriptions and help on other options with:
 'man htcompact'
    """
    # returns this docstring
    return small_help.__doc__


def get_event_information(event_id="") -> str:
    events = [
        {
            "Event Number": "000",
            "Event Name": "Job submitted",
            "Event Description": "This event occurs when a user submits a job."
                                 " It is the first event you will see for a job, and it should only occur once.",
        },
        {
            "Event Number": "001",
            "Event Name": "Job executing",
            "Event Description": "This shows up when a job is running. It might occur more than once.",
        },
        {
            "Event Number": "002",
            "Event Name": "Error in executable",
            "Event Description": "The job could not be run because the executable was bad.",
        },
        {
            "Event Number": "003",
            "Event Name": "Job was checkpointed",
            "Event Description": "The job's complete state was written to a checkpoint file."
                                 " This might happen without the job being removed from a machine,"
                                 " because the checkpointing can happen periodically.",
        },
        {
            "Event Number": "004",
            "Event Name": "Job evicted from machine",
            "Event Description": "A job was removed from a machine before it finished,"
                                 " usually for a policy reason."
                                 " Perhaps an interactive user has claimed the computer,"
                                 " or perhaps another job is higher priority.",
        },
        {
            "Event Number": "005",
            "Event Name": "Job terminated",
            "Event Description": "The job has completed.",
        },
        {
            "Event Number": "006",
            "Event Name": "Image size of job updated",
            "Event Description": "An informational event,"
                                 " to update the amount of memory that the job is using while running."
                                 " It does not reflect the state of the job.",
        },
        {
            "Event Number": "007",
            "Event Name": "Shadow exception",
            "Event Description": "The condor_shadow, a program on the submit computer"
                                 " that watches over the job and performs some services for the job,"
                                 " failed for some catastrophic reason."
                                 " The job will leave the machine and go back into the queue.",
        },
        {
            "Event Number": "008",
            "Event Name": "Generic log event",
            "Event Description": "Not used.",
        },
        {
            "Event Number": "009",
            "Event Name": "Job aborted",
            "Event Description": "The user canceled the job.",
        },
        {
            "Event Number": "010",
            "Event Name": "Job was suspended",
            "Event Description": "The job is still on the computer,"
                                 " but it is no longer executing. This is usually for a policy reason,"
                                 " such as an interactive user using the computer.",
        },
        {
            "Event Number": "011",
            "Event Name": "Job was unsuspended",
            "Event Description": "The job has resumed execution, after being suspended earlier.",
        },
        {
            "Event Number": "012",
            "Event Name": "Job was held",
            "Event Description": "The job has transitioned to the hold state."
                                 " This might happen if the user applies the condor_hold command to the job.",
        },
        {
            "Event Number": "013",
            "Event Name": "Job was released",
            "Event Description": "The job was in the hold state and is to be re-run.",
        },
        {
            "Event Number": "014",
            "Event Name": "Parallel node executed",
            "Event Description": "A parallel universe program is running on a node.",
        },
        {
            "Event Number": "015",
            "Event Name": "Parallel node terminated",
            "Event Description": "A parallel universe program has completed on a node.",
        },
        {
            "Event Number": "016",
            "Event Name": "POST script terminated",
            "Event Description": "A node in a DAGMan work flow has a script that should be run after a job."
                                 " The script is run on the submit host."
                                 " This event signals that the post script has completed.",
        },
        {
            "Event Number": "017",
            "Event Name": "Job submitted to Globus",
            "Event Description": "A grid job has been delegated to Globus (version 2, 3, or 4)."
                                 " This event is no longer used.",
        },
        {
            "Event Number": "018",
            "Event Name": "Globus submit failed",
            "Event Description": "The attempt to delegate a job to Globus failed.",
        },
        {
            "Event Number": "019",
            "Event Name": "Globus resource up",
            "Event Description": "The Globus resource that a job wants to run on was unavailable,"
                                 " but is now available. This event is no longer used.",
        },
        {
            "Event Number": "020",
            "Event Name": "Detected Down Globus Resource",
            "Event Description": "The Globus resource that a job wants to run on has become unavailable."
                                 " This event is no longer used.",
        },
        {
            "Event Number": "021",
            "Event Name": "Remote error",
            "Event Description": "The condor_starter (which monitors the job on the execution machine) has failed.",
        },
        {
            "Event Number": "022",
            "Event Name": "Remote system call socket lost",
            "Event Description": "The condor_shadow and condor_starter "
                                 "(which communicate while the job runs) have lost contact.",
        },
        {
            "Event Number": "023",
            "Event Name": "Remote system call socket reestablished",
            "Event Description": "The condor_shadow and condor_starter "
                                 "(which communicate while the job runs)"
                                 " have been able to resume contact before the job lease expired.",
        },
        {
            "Event Number": "024",
            "Event Name": "Remote system call reconnect failure",
            "Event Description": "The condor_shadow and condor_starter (which communicate while the job runs)"
                                 " were unable to resume contact before the job lease expired.",
        },
        {
            "Event Number": "025",
            "Event Name": "Grid Resource Back Up",
            "Event Description": "A grid resource that was previously unavailable is now available.",
        },
        {
            "Event Number": "026",
            "Event Name": "Detected Down Grid Resource",
            "Event Description": "The grid resource that a job is to run on is unavailable.",
        },
        {
            "Event Number": "027",
            "Event Name": "Job submitted to grid resource",
            "Event Description": "A job has been submitted, and is under the auspices of the grid resource.",
        },
        {
            "Event Number": "028",
            "Event Name": "Job ad information event triggered.",
            "Event Description": "Extra job ClassAd attributes are noted."
                                 " This event is written as a supplement to other events"
                                 " when the configuration parameter"
                                 " EVENT_LOG_JOB_AD_INFORMATION_ATTRS is set."
        },
        {
            "Event Number": "029",
            "Event Name": "The job's remote status is unknown",
            "Event Description": "No updates of the job's remote status have been received for 15 minutes.",
        },
        {
            "Event Number": "030",
            "Event Name": "The job's remote status is known again",
            "Event Description": "An update has been received for a job"
                                 " whose remote status was previous logged as unknown.",
        },
        {
            "Event Number": "031",
            "Event Name": "Job stage in",
            "Event Description": "A grid universe job is doing the stage in of input files.",
        },
        {
            "Event Number": "032",
            "Event Name": "Job stage out",
            "Event Description": "A grid universe job is doing the stage out of output files."
        },
        {
            "Event Number": "033",
            "Event Name": "Job ClassAd attribute update",
            "Event Description": "A Job ClassAd attribute is changed"
                                 " due to action by the condor_schedd daemon. "
                                 "This includes changes by condor_prio."
        },
        {
            "Event Number": "034",
            "Event Name": "Pre Skip event",
            "Event Description": "For DAGMan, this event is logged"
                                 " if a PRE SCRIPT exits with the defined PRE_SKIP value"
                                 " in the DAG input file. This makes it possible for DAGMan"
                                 " to do recovery in a workflow that has such an event,"
                                 " as it would otherwise not have any event for the DAGMan node"
                                 " to which the script belongs, and in recovery,"
                                 " DAGMan's internal tables would become corrupted."
        }
    ]

    out_str = ""

    # print the whole table
    if event_id == "":
        for event in events:
            for key, value in event.items():
                out_str += str(key) + ": " + str(value) + "\n"
            out_str += "\n"
        return out_str

    # print a single event
    event_id = int(event_id)
    if 0 <= event_id <= 34:
        for key, value in events[event_id].items():
            out_str += str(key)+ ": " + str(value)
        return out_str
    else:
        return "This event number does not exist, Valid event numbers range from 0 to 34"


def raise_value_error(message: str) -> ValueError:
    raise ValueError(message)


def raise_type_error(message: str) -> TypeError:
    raise TypeError(message)


# Todo: time differences over end of year
def gen_time_dict(
        submission_date: date_time,
        execution_date: date_time = None,
        termination_date: date_time = None
) -> (timedelta, timedelta, timedelta):
    """
    Takes in three dates, at least one must be given,
    return the datetime.timedelta objects,
    Depending on the given arguments,
    this function will try to find out the time differences between the events.
    If not all three values are given, it will return None

    Example:
    Submissio

    :param submission_date: Job Sumbission date from the user
    :param execution_date: The date when the job actually started executing
    :param termination_date: The date when the job was finished
    :return: (waiting_time, runtime, total_time)

    """
    waiting_time = None
    runtime = None
    total_time = None
    today = datetime.datetime.now()
    today = today.replace(microsecond=0)  # remove unnecessary microseconds

    time_desc = list()
    time_vals = list()
    running_over_neyear = False

    # calculate the time difference to last year,
    # if the date is higher that today of running jobs
    # this means the execution started before newyear
    if termination_date is None:
        if submission_date and submission_date > today:
            running_over_neyear = True
            submission_date = submission_date.replace(year=submission_date.year-1)
        if execution_date and execution_date > today:
            running_over_neyear = True
            execution_date = execution_date.replace(year=execution_date.year-1)

    if execution_date and submission_date:
        execution_date = execution_date
        # new year ?
        if submission_date > execution_date:
            running_over_neyear = True
            submission_date = submission_date.replace(year=submission_date.year - 1)
        waiting_time = execution_date - submission_date
    if termination_date:
        if waiting_time:
            pass
        if execution_date:
            # new year ?
            if execution_date > termination_date:
                running_over_neyear = True
                execution_date = execution_date.replace(year=execution_date.year - 1)
            runtime = termination_date - execution_date
        if submission_date:
            # new year ?
            if submission_date > termination_date:
                running_over_neyear = True
                submission_date = submission_date.replace(year=submission_date.year - 1)
            total_time = termination_date - submission_date
    # Process still running
    elif waiting_time:
        runtime = today - execution_date
    # Still waiting for execution
    elif submission_date:
        waiting_time = today - submission_date

    # now after collecting all available values try to produce a dict
    # if new year was hitted by one of them, show the year as well
    if running_over_neyear:
        if submission_date:
            time_desc.append("Submission date")
            time_vals.append(submission_date)
        if execution_date:
            time_desc.append("Execution date")
            time_vals.append(execution_date)
        if termination_date:
            time_desc.append("Termination date")
            time_vals.append(termination_date)
    else:
        if submission_date:
            time_desc.append("Submission date")
            time_vals.append(submission_date.strftime("%m/%d %H:%M:%S"))
        if execution_date:
            time_desc.append("Execution date")
            time_vals.append(execution_date.strftime("%m/%d %H:%M:%S"))
        if termination_date:
            time_desc.append("Termination date")
            time_vals.append(termination_date.strftime("%m/%d %H:%M:%S"))

    if waiting_time:
        time_desc.append("Waiting time")
        time_vals.append(waiting_time)
    if runtime:
        time_desc.append("Execution runtime")
        time_vals.append(runtime)
    if total_time:
        time_desc.append("Total runtime")
        time_vals.append(total_time)

    time_dict = {
        "Dates and times": time_desc,
        "Values": time_vals
    }

    return time_dict


def log_to_dict(file: str, sec: int = 0) -> (dict, dict, dict, dict, dict):
    """
            Read the log file with the use of the htcondor module and filter useful information.
            Structur it in a way, that later two different dictionaries will be retuned.

        :type file: str
        :param file: HTCondor log file
        :param sec: seconds to wait for new events
        :return: job_dict, res_dict, time_dict, ram_history, errors

        Consider that the return values can be None or empty dictionarys
    """
    job_events = list()
    res_dict = dict()
    time_dict = {
        "Submission date": None,
        "Execution date": None,
        "Termination date": None
    }
    ram_history = list()
    occurred_errors = list()

    has_terminated = False
    invalid_file = False

    try:
        jel = htcondor.JobEventLog(file)
        # Read all currently-available events waiting for 'sec' seconds for the next event.
        for event in jel.events(sec):
            event_type_number = event.get('EventTypeNumber')
            # convert time to datetime object
            date = datetime.datetime.strptime(event.get('EventTime'), "%Y-%m-%dT%H:%M:%S")
            if event.type == jet.SUBMIT:
                time_dict["Submission date"] = date

                match_from_host = re.match(r"<(.+):[0-9]+\?(.*)>", event.get('SubmitHost'))
                if match_from_host:
                    submitted_host = match_from_host[1]
                    job_events.append(('Submitted from', submitted_host))
                # ERROR
                else:
                    invalid_file = True
                    reason = f"Can't read user address"
                    occurred_errors.append([event_type_number, "Now", "invalid user address", reason])
                    job_events.append(('Submitted from', "invalid user"))
                    raise_value_error("Submission Host is wrong in file: " + file)

            if event.type == jet.EXECUTE:
                time_dict["Execution date"] = date

                match_to_host = re.match(r"<(.+):[0-9]+\?(.*)>", event.get('ExecuteHost'))
                if match_to_host:
                    execution_host = match_to_host[1]
                    if reverse_dns_lookup:  # resolve ip to dns if set
                        execution_host = gethostbyaddr(execution_host)

                    job_events.append(('Executing on', execution_host))
                # ERROR
                else:
                    invalid_file = True
                    reason = f"Can't read host address"
                    occurred_errors.append([event_type_number, "Now", "invalid host address", reason])
                    job_events.append(('Executing on', "invalid host"))
                    raise_value_error("Execution host address is wrong in file: " + file)

            if event.type == jet.IMAGE_SIZE:
                size_update = event.get('Size')
                memory_usage = event.get('MemoryUsage')
                resident_set_size = event.get('ResidentSetSize')
                ram_history.append((date, size_update, memory_usage, resident_set_size))  # append to ram_history list

            if event.type == jet.JOB_TERMINATED:
                has_terminated = True
                time_dict["Termination date"] = date

                # get all resources, replace by np.nan if value is None
                cpu_usage = event.get('CpusUsage') if event.get('CpusUsage') is not None else np.nan
                cpu_requested = event.get('RequestCpus') if event.get('RequestCpus') is not None else np.nan
                cpu_allocated = event.get('Cpus') if event.get('Cpus') is not None else np.nan
                disk_usage = event.get('DiskUsage') if event.get('DiskUsage') is not None else np.nan
                disk_requested = event.get('RequestDisk') if event.get('RequestDisk') is not None else np.nan
                disk_allocated = event.get("Disk") if event.get('Disk') is not None else np.nan
                memory_usage = event.get('MemoryUsage') if event.get('MemoryUSage') is not None else np.nan
                memory_requested = event.get('RequestMemory') if event.get('RequestMemory') is not None else np.nan
                memory_allocated = event.get('Memory') if event.get('Memory') is not None else np.nan

                # put the data in the dict
                res_dict = {
                    "Resources": ["Cpu", "Disk", "Memory"],
                    "Usage": np.array([cpu_usage, disk_usage, memory_usage], dtype=float),
                    "Requested": np.array([cpu_requested, disk_requested, memory_requested], dtype=float),
                    "Allocated": np.array([cpu_allocated, disk_allocated, memory_allocated], dtype=float)
                }
                normal_termination = event.get('TerminatedNormally')
                # differentiate between normal and abnormal termination
                if normal_termination:
                    job_events.insert(0, ("Termination State", colors['green'] + "Normal" + colors['back_to_default']))
                    return_value = event.get('ReturnValue')
                    job_events.append(("Return Value", return_value))
                else:
                    job_events.insert(0, ("Termination State", colors['red'] + "Abnormal" + colors['back_to_default']))
                    signal = event.get('TerminatedBySignal')
                    job_events.append(("Terminated by Signal", signal))

            if event.type == jet.JOB_ABORTED:
                has_terminated = True
                time_dict["Termination date"] = date

                reason = event.get('Reason')
                occurred_errors.append([event_type_number, date.strftime("%m/%d %H:%M:%S"), "Aborted", reason])
                job_events.insert(0, ("Process was", colors['red'] + "Aborted" + colors['back_to_default']))

            # now filter all known errors
            if event.type == jet.JOB_HELD:
                reason = event.get('HoldReason')
                occurred_errors.append([event_type_number, date.strftime("%m/%d %H:%M:%S"), "JOB_HELD", reason])

            if event.type == jet.SHADOW_EXCEPTION:
                reason = event.get('Message')
                occurred_errors.append((event_type_number, date.strftime("%m/%d %H:%M:%S"), "SHADOW_EXCEPTION", reason))
        else:
            # End of the file
            pass

    except OSError as err:
        invalid_file = True
        if err.args[0] == "ULOG_RD_ERROR":
            rprint(f"[red]{err}: Not a valid htcondor logfile: {file}[/red]")
            reason = "Error while reading log file. " \
                     "This happens if the file was manipulated or contains gpu usage."
            occurred_errors.append(["None", "Now", "ULOG_RD_ERROR", reason])
        else:
            rprint(f"[red]Not able to open the file: {file}[/red]")
    except ValueError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    except TypeError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")

    # generate a better time dict
    time_values = list(time_dict.values())
    better_time_dict = gen_time_dict(*time_values)

    # Job still running and file valid
    if not invalid_file and not has_terminated:
        if "Total runtime" in better_time_dict["Dates and times"]:
            rprint("[red]This is not supposed to happen, check your code[/red]")
        elif "Execution runtime" in better_time_dict["Dates and times"]:
            state = "Executing"
        elif "Waiting time" in better_time_dict["Dates and times"]:
            state = "Waiting"
        else:
            pass
        job_events.insert(0, ("Process is", colors["blue"] + state + colors["back_to_default"]))
    # file not fully readable
    elif invalid_file:
        better_time_dict = dict()  # times should not be returned, because they will make no sense
        job_events.insert(0, ("Error", colors["red"] + "Error while reading" + colors["back_to_default"]))

    job_events_dict = dict()
    error_dict = dict()
    ram_history_dict = dict()
    # convert job_events to a nice and simple dictionary
    if len(job_events) > 0:
        desc, val = zip(*job_events)
        job_events_dict = {
            "Execution details": desc,
            "Values": val
        }

    # convert errors into a dictionary
    if len(occurred_errors) > 0:
        event_numbers, time_list, errors, reasons = zip(*occurred_errors)
        reasons = list(reasons)
        for i in range(len(reasons)):
            tmp_reason = reasons[i]
            if len(tmp_reason) > 50:
                split = int(len(tmp_reason) / 2)
                while tmp_reason[split] != ' ' and split < len(tmp_reason) - 1:
                    split += 1
                if split > len(tmp_reason) - 2:  # if no space was found, change back to normal
                    split = int(len(tmp_reason) / 2)
                reasons[i] = tmp_reason[0:split] + '\n' + tmp_reason[split:]
        error_dict = {
            "Event Number": list(event_numbers),
            "Time": list(time_list),
            "Error": list(errors),
            "Reason": list(reasons)
        }
    # convert ram_history to a dictionary
    if len(ram_history) > 0:
        time_list, img_size, mem_usage, res_set_size = zip(*ram_history)
        ram_history_dict = {
            "Dates": list(time_list),
            "Image size updates": list(img_size),
            "Memory usages": list(mem_usage),
            "Resident Set Sizes": list(res_set_size)
        }

    return job_events_dict, res_dict, better_time_dict, ram_history_dict, error_dict


# just read .err files content and return it as a string
def read_condor_error(file: str) -> str:
    """
    Reads a HTCondor .err file and returns its content

    :param file: a HTCondor .err file

    :raises: :class:'NameError': The param file was no .err file
             :class:'FileNotFoundError': if open does not work

    :return: file content

    """
    if not file.endswith(std_err):
        raise NameError("The read_condor_error method is only for " + std_err + " files")

    try:
        err_file = open(file)
    except FileNotFoundError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    else:
        return "".join(err_file.readlines())


# just read .out files content and return it as a string
def read_condor_output(file: str) -> str:
    """
        Reads a HTCondor .out file and returns its content

        :param file: a HTCondor .out file

        :raises: :class:'NameError': The param file was no .out file
                 :class:'FileNotFoundError': if open does not work

        :return: file content

        """
    if not file.endswith(std_out):
        raise NameError("The read_condor_output method is only for " + std_out + " files")
    try:
        out_file = open(file)
    except FileNotFoundError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
    else:
        return "".join(out_file.readlines())


def gethostbyaddr(ip):
    """
        this function is supposed to filter a given ip for it's representative domain name like google.com
        :return: resolved domain name, else give back the ip
    """
    try:
        if ip in list(store_dns_lookups.keys()):
            return store_dns_lookups[ip]
        # else lookup
        reversed_dns = socket.gethostbyaddr(ip)
        logging.debug('Lookup successful ' + ip + ' resolved as: ' + reversed_dns[0])
        # store
        store_dns_lookups[ip] = reversed_dns[0]
        # return
        return reversed_dns[0]
    except Exception:
        logging.debug('Not able to resolve the IP: ' + ip)
        # also store
        store_dns_lookups[ip] = ip
        return ip


def manage_thresholds(resources: dict) -> dict:
    """

        The important part is that the keywords "Usage", "Requested" exists
        and that at least 3 values are given: cpu, disk, memory

    :param resources:
    :return:
    """

    resources.update(Usage=list(resources["Usage"]))  # change to list, to avoid numpy type errors
    for i in range(len(resources['Resources'])):
        # thresholds used vs. requested
        if float(resources['Requested'][i]) != 0:

            deviation = float(resources['Usage'][i]) / float(resources['Requested'][i])

            # color red if more than bad_usage_thresholds % away from the requested value
            if deviation >= 1+bad_usage_threshold or deviation <= 1-bad_usage_threshold:
                resources['Usage'][i] = colors['red'] + str(resources['Usage'][i]) + colors['back_to_default']

            # color yellow if more than low_usage_threhold % away from requested value
            elif deviation >= 1+tolerated_usage_threshold or deviation <= 1-tolerated_usage_threshold:
                resources['Usage'][i] = colors['yellow'] + str(resources['Usage'][i]) + colors['back_to_default']
            # else it's okay, color green
            else:
                resources['Usage'][i] = colors['green'] + str(resources['Usage'][i]) + colors['back_to_default']

    return resources


def show_htcondor_stderr(file: str) -> str:
    """

    :param file: a HTCondor .err file
    :return: the content of the given file as a string
    """

    # accept files without the std_err suffix
    if std_err.__ne__("") and file[-len(std_err):].__eq__(std_err):
        job_spec_id = file[:-len(std_err)]
    elif std_err.__ne__(""):
        job_spec_id = file
    else:
        raise_value_error("Missing specification for the std_err")

    output_string = ""
    try:

        error_content = read_condor_error(job_spec_id + std_err)
        for line in error_content.split("\n"):
            if 'std-err' in show_list:
                if "err" in line.lower():
                    output_string += colors['red'] + line + colors['back_to_default'] + "\n"
                elif "warn" in line.lower():
                    output_string += colors['yellow'] + line + colors['back_to_default'] + "\n"

    except NameError as err:
        logging.exception(err)
        rprint("[red]The smart_output_error method requires a " + std_err + " file as parameter[/red]")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)" + std_err, relevant[1])
        rprint(f"[yellow]There is no related {std_err} file: {relevant[1]} in the directory:\n[/yellow]"
               f"[cyan]'{os.path.abspath(relevant[0])}'\n with the prefix: {match[1]}[/cyan]")
    except TypeError as err:
        logging.exception(err)
        # we continue here no exit needed
    finally:
        return output_string


def show_htcondor_stdout(file: str) -> str:
    """
    :param file: a HTCondor .out file
    :return: the content of that file as a string
    """

    # accept files without the std_out suffix
    if std_out.__ne__("") and file[-len(std_out):].__eq__(std_out):
        job_spec_id = file[:-len(std_out)]
    elif std_out.__ne__(""):
        job_spec_id = file
    else:
        raise_value_error("Missing specification for the std_out file")

    output_string = ""
    try:

        output_content = read_condor_output(job_spec_id + std_out)
        output_string += output_content
    except NameError as err:
        logging.exception(err)
        rprint("[red]The smart_output_output method requires a " + std_out + " file as parameter[/red]")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)" + std_out, relevant[1])
        rprint(f"[yellow]There is no related {std_out} file: {relevant[1]} in the directory:\n[/yellow]"
               f"[cyan]'{os.path.abspath(relevant[0])}'\n with the prefix: {match[1]}[/cyan]")
    except TypeError as err:
        logging.exception(err)
        # we continue here no exit needed
    finally:

        return output_string


def customize_log(file: str) -> dict:
    """
    reads a given HTCondor .log file with the log_to_dict() function

    :param file:    a HTCondor .log file
    :param header:  Shows the header of the columns
    :param index:   Shows the index of the rows

    :return:        dict

    """
    # accept files without the std_log suffix
    if std_log.__ne__("") and file[-len(std_log):].__eq__(std_log):
        job_spec_id = file[:-len(std_log)]
    else:
        job_spec_id = os.path.splitext(file)[0]

    result_dict = dict()

    try:

        htcondor_log = log_to_dict(file)

        job_dict = htcondor_log[0]
        res_dict = htcondor_log[1]
        times = htcondor_log[2]

        if to_csv:
            pass
        else:
            result_dict[
                "file-description"] = f"{colors['green']}The job procedure of : {file}{colors['back_to_default']}"

            result_dict["execution-details"] = job_dict

            result_dict["times"] = times
            if not len(res_dict) == 0:  # make sure res_df is not None

                res_dict = manage_thresholds(res_dict)

                result_dict["all-resources"] = res_dict

        if 'std-err' in show_list:
            result_dict['stderr'] = show_htcondor_stderr(job_spec_id + std_err)
        if 'std-out' in show_list:
            result_dict['stdout'] = show_htcondor_stdout(job_spec_id + std_out)

    except NameError as err:
        logging.exception(err)
        rprint(f"[red]The smart_output_logs method requires a {std_log} file as parameter[/red]")
    except ValueError as err:
        logging.exception(err)
        rprint(f"[red]{err}[/red]")
    except TypeError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        sys.exit(3)
    except Exception as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")

    return result_dict


def find_config(args: list) -> (str, list):
    """

        Try to find a config file in the given arguments.
        Go through the hierarchy in the order:
        - current_working_directory/file
        - current_project_folder/config/file
        - ~/.config/{script_name}/{file}"
        - /etc/{file}

        ONE config will be loaded if set: generate_logging_file

        -> else try to find default config file "htcompact.conf"
         in the same order excluding the current working directory,
         because this might lead to misbehaviour, when the user changes the directory.

        :return: file if found, else None, args without config, if found
    """

    script_name = sys.argv[0]
    if script_name.endswith(".py"):
        script_name = sys.argv[0][:-3]  # scriptname without .py
    config = configparser.ConfigParser()
    # search for the given file, first directly, then in /etc and then in ~/.config/htcompact/
    found_config = False

    arguments = args
    arguments.append(default_configfile)

    found_config_file = None
    found_default_in_cwd = False
    # run through every given argument and try to determine if it's a valid config file
    for i, file in enumerate(arguments):
        # try to find the given file in the current directory, in /etc or in ~/.config/htcompact/
        try:
            # search in current working directory
            if os.path.isfile(file) and config.read(file):
                # only use the file if given, means the default config should be ignored,
                # even if it's in cwd, this will exclude misbehaviour, when cwd is changed
                if not (file.__eq__(default_configfile) and i == len(arguments) - 1):
                    found_config = True
                    arguments.remove(file)
                else:
                    found_default_in_cwd = True

                found_config_file = file

            # try to find the config file in the current environment hierarchy
            elif os.path.isfile(f"{sys.prefix}/config/{file}") and config.read(f"{sys.prefix}/config/{file}"):
                found_config = True
                arguments.remove(file)
                found_config_file = f"{sys.prefix}/config/{file}"  # remember the file

            # try to find config file in ~/.config/script_name/
            elif os.path.isfile(f"~/.config/{script_name}/{file}") \
                    and config.read(f"~/.config/{script_name}/{file}"):
                found_config = True
                arguments.remove(file)
                found_config_file = f"~/.config/{script_name}/{file}"  # remember the file

            # try to find config file in /etc
            elif os.path.isfile(f"/etc/{file}") and config.read(f"/etc/{file}"):
                found_config = True
                arguments.remove(file)
                found_config_file = f"/etc/{file}"  # remember the file

            else:
                logging.debug(f"{file} not found or is not a valid config file")

            # do not search any longer if found
            if found_config:
                break

        # File has no readable format for the configparser, probably because it's not a config file
        except configparser.MissingSectionHeaderError:
            continue
        except configparser.DuplicateOptionError as err:
            rprint(f"[red]{err}[/red]")

    try:
        arguments.remove(default_configfile)  # remove again, if other config file was found
    except ValueError:
        pass

    # prepare output if not and break if not found
    if not found_config and not found_default_in_cwd:
        rprint(f"[yellow]No valid config file found in: \n"
               f"- {os.getcwd()},\n"
               f"- {sys.prefix}/config,\n"
               f"- ~/.config/htcompact/,\n"
               f"- /etc/ \n"
               f"-> using default settings[/yellow]")
        return None, args
    elif found_default_in_cwd:
        rprint(f"[yellow]Found default {default_configfile} in the current working directory.\n"
               f"It will be ignore, to use this, make sure to pass it as an argument like:\n"
               f"htcompact {default_configfile} [logs] [options]\n"
               f"-> using default settings[/yellow]")
        return None, args

    config.read(found_config_file)
    sections = config.sections()
    # check if logging is tuned on
    if 'features' in sections:
        if 'generate_log_file' in config['features']:
            global generate_log_file
            if not generate_log_file:  # if already set, do NOT overwrite
                generate_log_file = config['features']['generate_log_file'].lower() in accepted_states
                logging.getLogger().disabled = not generate_log_file

    return found_config_file, arguments


# search for config file ( UNIX BASED )
def load_config(config_file: str):
    config = configparser.ConfigParser()

    try:

        config.read(config_file)
        # all sections in the config file
        sections = config.sections()

        # now try filter the config file for all available parameters

        rprint(f"[blue]Load config from file: {config_file}[/blue]")

        # all documents like files, etc.
        if 'documents' in sections:
            global files

            if 'files' in config['documents']:
                files = config['documents']['files'].split(" ")
                logging.debug(f"Changed document files to {files}")

        # all formats like the table format
        if 'formats' in sections:
            global table_format
            if 'table_format' in config['formats']:
                table_format = config['formats']['table_format']
                logging.debug(f"Changed default table_format to: {table_format}")

        # all basic HTCondor file endings like .log, .err, .out
        if 'htc-files' in sections:
            global std_log, std_err, std_out

            if 'stdlog' in config['htc-files']:
                std_log = config['htc-files']['stdlog']
                logging.debug(f"Changed default for HTCondor .log files to: {std_log}")

            if 'stderr' in config['htc-files']:
                std_err = config['htc-files']['stderr']
                logging.debug(f"Changed default for HTCondor .err files to: {std_err}")

            if 'stdout' in config['htc-files']:
                std_out = config['htc-files']['stdout']
                logging.debug(f"Changed default for HTCondor .out files to: {std_out}")

        # if show variables are set for further output
        if 'show-more' in sections:
            global show_list

            if 'show_list' in config['show-more']:
                for arg in " ".join(re.split(',| ', config['show-more']['show_list'])).split():
                    if arg in allowed_show_values:
                        show_list.append(arg)
                    else:
                        logging.debug("Don't know this ignore statement: " + arg)
                logging.debug(f"Changed default ignore statements to: {show_list}")

        # what information should to be ignored
        if 'ignore' in sections:
            global ignore_list  # sources to ignore

            if 'ignore_list' in config['ignore']:
                for arg in " ".join(re.split(',| ', config['ignore']['ignore_list'])).split():
                    if arg in allowed_ignore_values:
                        ignore_list.append(arg)
                    else:
                        logging.debug("Don't know this show statement: " + arg)
                logging.debug(f"Changed default show-more statements to: {ignore_list}")

        if 'thresholds' in sections:
            global tolerated_usage_threshold, bad_usage_threshold
            if 'tolerated_usage' in config['thresholds']:
                tolerated_usage_threshold = float(config['thresholds']['tolerated_usage'])
                logging.debug(f"Changed default tolerated_usage to: {tolerated_usage_threshold}")
            if 'bad_usage' in config['thresholds']:
                bad_usage_threshold = float(config['thresholds']['bad_usage'])
                logging.debug(f"Changed default bad_usage to: {bad_usage_threshold}")

        if "modes" in sections:
            global filter_mode, mode
            if 'filter_mode' in config['modes']:
                filter_mode = config['modes']['filter_mode'].lower() in accepted_states
                logging.debug(f"Changed filter_mode to: {filter_mode}")

            if 'mode' in config['modes']:
                mode_tmp = config['modes']['mode'].lower()
                if mode_tmp in allowed_modes.values():
                    mode = mode_tmp
                elif mode_tmp in allowed_modes.keys():
                    mode = allowed_modes.get(mode_tmp)
                else:
                    mode = None
                logging.debug(f"Changed mode to: {mode}")

        if "filter" in sections:
            global filter_keywords, filter_extended
            if 'filter_keywords' in config['filter']:
                filter_keywords = " ".join(re.split(',| ', config['filter']['filter_keywords'])).split()
                logging.debug(f"Changed default filter_keywords to: {filter_keywords}")
            if 'filter_extended' in config['filter']:
                filter_extended = config['filter']['filter_extended'].lower() in accepted_states
                logging.debug(f"Changed default filter_extended to: {filter_extended}")

        if 'features' in sections:
            global reverse_dns_lookup, to_csv  # extra parameters
            # the first thing to check should be if logging is turend on

            if 'reverse_dns_lookup' in config['features']:
                reverse_dns_lookup = config['features']['reverse_dns_lookup'].lower() in accepted_states
                logging.debug(f"Changed default reverse_dns_lookup to: {reverse_dns_lookup}")

            if 'to_csv' in config['features']:
                to_csv = config['features']['to_csv'].lower() in accepted_states
                logging.debug(f"Changed default to_csv to: {to_csv}")

        return True

    except KeyError as err:
        logging.exception(err)


# in order that plotille has nothing like a int converter,
# I have to set it up manually to show the y - label in number format
def _int_formatter(val, chars, delta, left=False):
    """
    Usage of this is shown here:
    https://github.com/tammoippen/plotille/issues/11

    :param val:
    :param chars:
    :param delta:
    :param left:
    :return:
    """
    align = '<' if left else ''
    return '{:{}{}d}'.format(int(val), align, chars)


def default(log_files: list_of_logs) -> log_inf_list:
    """
    Print the default output for a given list of log files

    This mode is just an easy view,
     on what the script is actually doing.

    :param log_files:
    :return: list of dicts
    """

    logging.info('Starting the default mode')

    list_of_dicts = list()
    current_path = os.getcwd()
    # go through all given logs and check for each if it is a directory or file and if std_log was missing or not
    for file in track(log_files, transient=True, description="Processing..."):

        # for the * operation it should skip std_err and std_out files
        if file.endswith(std_err) and not std_err.__eq__("") or file.endswith(std_out) and not std_out.__eq__(""):
            continue

        # else check if file is a valid file
        if os.path.isfile(file) or os.path.isfile(current_path + "/" + file):
            list_of_dicts.append(customize_log(file))

        # if that did not work try std_log at the end of that file
        elif not file.endswith(std_log):
            new_path = file + std_log
            # is this now a valid file ?
            if os.path.isfile(new_path) or os.path.isfile(current_path + "/" + new_path):
                list_of_dicts.append(customize_log(new_path))
            # no file or directory found, even after manipulating the string
            else:
                logging.error(f"No such file with that name or prefix: {file}")
                rprint(f"[red]No such file with that name or prefix: {file}[/red]")

        # The given .log file was not found
        else:
            logging.error(f"No such file: {file}")
            rprint(f"[red]No such file: {file}[/red]")

    if len(list_of_dicts) == 0:
        rprint("[yellow]Nothing found, please use \"man htcompact\" or \"htcompact -h\" for help[/yellow]", end="")

    return list_of_dicts


def analyse(log_files: list_of_logs) -> log_inf_list:
    logging.info('Starting the analyser mode')

    if len(log_files) == 0:
        return "No files to analyse"
    elif len(log_files) > 1 and not redirecting_output:
        print("More than one file is given, this mode is meant to be used for single job analysis.\n"
              "This will change nothing, but you should rather do it just for a file one by one")
        if not reading_stdin:
            x = input("Want to continue (y/n): ")
            if x != "y":
                rprint('[red]Process stopped[/red]')
                sys.exit(0)

    result_list = list()

    for file in track(log_files, transient=True, description="Analysing..."):
        result_dict = dict()

        logging.debug(f"Analysing the HTCondor log file: {file}")
        result_dict["file-description"] = f"{colors['green']}Job analysis of: {file}{colors['back_to_default']}"

        job_dict, res_dict, time_dict, ram_history, occurred_errors = log_to_dict(file)

        if len(job_dict) != 0:
            result_dict["execution-details"] = job_dict

        if len(time_dict) > 0:
            result_dict["times"] = time_dict

        if len(res_dict) > 0:
            result_dict["all-resources"] = manage_thresholds(res_dict)

        # show HTCondor errors
        if len(occurred_errors) > 0:
            result_dict["errors"] = occurred_errors

        # managing the ram history
        if len(ram_history) > 0:
            ram = np.array(ram_history.get('Image size updates'))
            dates = np.array(ram_history.get('Dates'))

            if len(ram) > 1:

                fig = Figure()
                fig.width = 55
                fig.height = 15
                fig.set_x_limits(min_=min(dates))
                min_ram = int(min(ram))  # raises an error if not casted to int
                fig.set_y_limits(min_=min_ram)
                fig.y_label = "Usage"
                fig.x_label = "Time"

                # this will use the self written function _num_formatter, to convert the y-label to int values
                fig.register_label_formatter(float, _int_formatter)
                fig.plot(dates, ram, lc='green', label="Continuous Graph")
                fig.scatter(dates, ram, lc='red', label="Single Values")

                if redirecting_output:  # if redirected, the Legend is useless
                    result_dict["ram-history"] = fig.show()
                else:
                    result_dict["ram-history"] = fig.show(legend=True)
            else:
                result_dict["ram-history"] = f"Single memory update found:\n" \
                    f"Memory usage on the {dates[0]} was updatet to {ram[0]} MB"

        result_list.append(result_dict)

    return result_list


def summarize(log_files: list_of_logs) -> log_inf_list:
    """
    Summarises all used resources and the runtime in total and average

    Runs through the log files via the log_to_dict function

    :return:
    """

    logging.info('Starting the summarizer mode')

    valid_files = len(log_files)
    # no given files
    if valid_files == 0:
        return "No files to summarize"

    # allocated all diffrent datatypes, easier to handle
    result_dict = dict()

    aborted_files = 0
    still_running = 0
    error_reading_files = 0
    other_exception = 0
    normal_runtime = datetime.timedelta()
    host_nodes = dict()

    total_usages = np.array([0, 0, 0], dtype=float)
    total_requested = np.array([0, 0, 0], dtype=float)
    total_allocated = np.array([0, 0, 0], dtype=float)

    # if gpu given
    gpu_found = False

    for file in track(log_files, transient=True, description="Summarizing..."):
        try:
            job_dict, res_dict, time_dict, _, _ = log_to_dict(file)

            # continue if Process is still running
            if job_dict['Execution details'][0].__eq__("Process is"):
                still_running += 1
                rprint(f"[orange3]Process of {file} is still running, \n"
                       f"it will be ignored for this summation[/orange3]")
                continue

            elif job_dict['Execution details'][0].__eq__("Process was"):
                aborted_files += 1
                rprint(f"[orange3]Process of {file} has been aborted, \n"
                       f"it will be ignored for this summation[/orange3]")
                continue
            elif job_dict['Execution details'][0].__eq__("Error"):
                error_reading_files += 1
                rprint(f"[orange3]Process of {file} not fully readable,\n"
                       f"cause it's not matching the htcondor specs,\n"
                       f"it will be ignored for this summation[/orange3]")
                continue
            elif len(job_dict) == 0:
                logging.error("if this even get's printed out, more work is needed")
                rprint(f"[orange3]Process of {file} is strange, \n"
                       f"don't know how to handle this yet[/orange3]")
                other_exception += 1
                continue

            if "Total runtime" in time_dict["Dates and times"]:
                normal_runtime += time_dict['Values'][3]
            host = job_dict['Values'][2]
            if host in host_nodes:
                host_nodes[host][0] += 1
                host_nodes[host][1] += time_dict['Values'][3]
            else:
                host_nodes[host] = [1, time_dict['Values'][3]]

            total_usages += np.nan_to_num(res_dict["Usage"])
            total_requested += np.nan_to_num(res_dict["Requested"])
            total_allocated += np.nan_to_num(res_dict["Allocated"])

        # Error ocurres when Job was aborted
        except ValueError or KeyError as err:
            logging.exception(err)
            rprint(f"[red]Error with summarizing: {file}[/red]")
            continue
        except TypeError as err:
            logging.exception(err)
            rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
            sys.exit(3)

    # calc difference of successful executed jobs
    n = valid_files - aborted_files - still_running - other_exception - error_reading_files

    average_runtime = normal_runtime / n if n != 0 else normal_runtime
    average_runtime = datetime.timedelta(days=average_runtime.days, seconds=average_runtime.seconds)

    exec_dict = {
        "Job types": ["normal executed jobs"],
        "Occurrence": [n]
    }
    if aborted_files > 0:
        exec_dict["Job types"].append("Aborted jobs")
        exec_dict["Occurrence"].append(aborted_files)
    if still_running > 0:
        exec_dict["Job types"].append("Still running jobs")
        exec_dict["Occurrence"].append(still_running)
    if error_reading_files > 0:
        exec_dict["Job types"].append("Error while reading")
        exec_dict["Occurrence"].append(error_reading_files)
    if other_exception > 0:
        exec_dict["Job types"].append("Other exceptions")
        exec_dict["Occurrence"].append(other_exception)

    result_dict["execution-details"] = exec_dict

    # do not even try futher if the only files given have been aborted, are still running etc.
    if n == 0:
        return result_dict

    create_desc = "The following data only implies on sucessful executed jobs"
    if aborted_files > 0 or still_running > 0 or other_exception > 0:
        create_desc += f"\n{colors['light_grey']}" \
            f"Use the analysed-summary mode for more details about the other jobs" \
            f"{colors['back_to_default']}"

    result_dict["summation-description"] = create_desc

    time_desc_list = list()
    time_value_list = list()
    if normal_runtime != datetime.timedelta(0, 0, 0):
        time_desc_list.append("Total runtime")
        time_value_list.append(normal_runtime)
    if average_runtime:
        time_desc_list.append("Average runtime")
        time_value_list.append(average_runtime)

    result_dict["times"] = {
        "Times": time_desc_list,
        "Values": time_value_list
    }

    if n != 0:  # do nothing, if all valid jobs were aborted

        average_dict = {
            "Resources": ['Average Cpu', 'Average Disk (KB)', 'Average Memory (MB)'],
            "Usage": np.round(total_usages / n, 4),
            "Requested": np.round(total_requested / n, 2),
            "Allocated": np.round(total_allocated / n, 2)

        }

        average_dict = manage_thresholds(average_dict)

        result_dict["all-resources"] = average_dict

    if len(host_nodes) > 0:

        executed_jobs = list()
        runtime_per_node = list()
        for val in host_nodes.values():
            executed_jobs.append(val[0])
            average_job_duration = val[1] / val[0]
            runtime_per_node.append(datetime.timedelta(average_job_duration.days, average_job_duration.seconds))

        cpu_dict = {
            "Host Nodes": list(host_nodes.keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        }

        result_dict["host-nodes"] = cpu_dict

    return [result_dict]


def analysed_summary(log_files: list_of_logs) -> log_inf_list:
    """
        analyse the summarized log files,
        this is meant to give the ultimate output
        about every single log event in average etc.

        Runs through the log files via the log_to_dict function

        :return: string
        """

    logging.info('Starting the analysed summary mode')

    valid_files = len(log_files)
    # no given files
    if valid_files == 0:
        return "No files for the analysed summary"

    # fill this dict with information by the execution type of the jobs
    all_files = dict()
    list_of_gpu_names = list()  # list of gpus found
    occurrence_dict = dict()

    for file in track(log_files, transient=True, description="Summarizing..."):

        job_dict, res_dict, time_dict, ram_history, occurred_errors = log_to_dict(file)

        if len(occurred_errors) > 0:
            create_file_list = list()
            for i in range(len(occurred_errors["Event Number"])):
                create_file_list.append(file)
            occurred_errors['File'] = create_file_list

        refactor_job_dict = dict(zip(job_dict["Execution details"], job_dict["Values"]))
        job_keys = list(refactor_job_dict.keys())
        if "Executing on" in job_keys:
            to_host = refactor_job_dict["Executing on"]

        termination_type = job_dict["Values"][0]

        # if time dict exists
        time_keys = list()
        if time_dict:
            refactor_time_dict = dict(zip(time_dict["Dates and times"], time_dict["Values"]))
            time_keys = list(refactor_time_dict.keys())
        if "Waiting time" in time_keys:
            waiting_time = refactor_time_dict["Waiting time"]
        else:
            waiting_time = datetime.timedelta()
        if "Execution runtime" in time_keys:
            runtime = refactor_time_dict["Execution runtime"]
        else:
            runtime = datetime.timedelta()
        if "Total runtime" in time_keys:
            total_time = refactor_time_dict["Total runtime"]
        else:
            total_time = datetime.timedelta()

        try:
            if termination_type in all_files:
                # logging.debug(all_files[termination_type])
                all_files[termination_type][0] += 1  # count number
                all_files[termination_type][1] += waiting_time
                all_files[termination_type][2] += runtime
                all_files[termination_type][3] += total_time

                # add errors
                if len(occurred_errors) > 0:
                    for key in occurred_errors.keys():
                        all_files[termination_type][6][key].extend(occurred_errors[key])

                if not len(all_files[termination_type][4]) == 0:
                    # add usages

                    all_files[termination_type][4]["Usage"] += np.nan_to_num(res_dict["Usage"])
                    # add requested
                    all_files[termination_type][4]["Requested"] += np.nan_to_num(res_dict["Requested"])
                    # allocated
                    all_files[termination_type][4]["Allocated"] += np.nan_to_num(res_dict["Allocated"])

                # add cpu
                if to_host != "":
                    # cpu known ???
                    if to_host in all_files[termination_type][5].keys():
                        all_files[termination_type][5][to_host][0] += 1
                        all_files[termination_type][5][to_host][1] += total_time
                    else:
                        all_files[termination_type][5][to_host] = [1, total_time]
                elif "Submitted from" in job_dict["Execution details"]:
                    # other waiting jobs ???
                    if 'Waiting for execution' in all_files[termination_type][5].keys():
                        all_files[termination_type][5]['Waiting for execution'][0] += 1
                        all_files[termination_type][5]['Waiting for execution'][1] += total_time
                    else:
                        count_host_nodes = dict()
                        count_host_nodes['Waiting for execution'] = [1, total_time]
                        all_files[termination_type][5] = count_host_nodes
                else:
                    # other aborted before submission jobs ???
                    if 'Aborted before submission' in all_files[termination_type][5].keys():
                        all_files[termination_type][5]['Aborted before submission'][0] += 1
                        all_files[termination_type][5]['Aborted before submission'][1] += total_time
                    else:
                        count_host_nodes = dict()
                        count_host_nodes['Aborted before submission'] = [1, total_time]
                        all_files[termination_type][5] = count_host_nodes

            # else new entry
            else:
                # if host exists
                if "Executing on" in job_dict["Execution details"]:
                    # to_host = job_dict["Values"][2]
                    count_host_nodes = dict()
                    count_host_nodes[to_host] = [1, total_time]
                # else if still waiting
                elif "Submitted from" in job_dict["Execution details"]:
                    count_host_nodes = dict()
                    count_host_nodes['Waiting for execution'] = [1, total_time]
                # else aborted before submission ?
                else:
                    count_host_nodes = dict()
                    count_host_nodes['Aborted before submission'] = [1, total_time]

                # convert nan values to 0
                if len(res_dict) > 0:
                    res_dict["Usage"] = np.nan_to_num(res_dict["Usage"])
                    res_dict["Requested"] = np.nan_to_num(res_dict["Requested"])
                    res_dict["Allocated"] = np.nan_to_num(res_dict["Allocated"])

                all_files[termination_type] = [1, waiting_time, runtime, total_time,
                                               res_dict, count_host_nodes, occurred_errors]

        # Error ocurres when Job was aborted
        except ValueError or KeyError as err:
            logging.exception(err)
            rprint(f"[red]Error with summarizing: {file}[/red]")
            continue
        except TypeError as err:
            logging.exception(err)
            rprint(f"[red] {err}[/red]")
            sys.exit(3)

    # Now put everything together
    result_list = list()
    for term_state in all_files:
        term_info = all_files[term_state]
        result_dict = dict()

        # differentiate between terminated and running processes
        if "Error while reading" in term_state:
            result_dict["file-description"] = f"" \
                f"##################################################\n" \
                f"## All files, that caused an {colors['red']}error while reading{colors['back_to_default']}\n" \
                f"##################################################"
        elif not term_state in ["Waiting", "Executing"]:
            result_dict["file-description"] = f"" \
                f"##################################################\n" \
                f"## All files with the termination state: {term_state}\n" \
                f"##################################################"
        else:
            result_dict["file-description"] = f"" \
                f"###########################################\n" \
                f"## All files, that are currently {term_state}\n" \
                f"###########################################"

        n = int(term_info[0])
        occurrence_dict[term_state] = str(n)

        times = np.array([term_info[1], term_info[2], term_info[3]])
        av_times = times / n
        format_av_times = [datetime.timedelta(days=time.days, seconds=time.seconds) for time in av_times]

        time_dict = {
            "Times": ["Waiting time", "Runtime", "Total"],
            "Average": format_av_times,
            "Total": times
        }

        result_dict["times"] = time_dict

        if not len(term_info[4]) == 0:
            total_resources_dict = term_info[4]
            avg_dict = {
                'Resources': ['Average Cpu', ' Average Disk (KB)', 'Average Allocated'],
                'Usage': np.round(np.array(total_resources_dict['Usage']) / term_info[0], 2).tolist(),
                'Requested': np.round(np.array(total_resources_dict['Requested']) / term_info[0], 2).tolist(),
                'Allocated': np.round(np.array(total_resources_dict['Allocated']) / term_info[0], 2).tolist()
            }
            if 'Assigned' in total_resources_dict.keys():
                avg_dict['Resources'].append('Gpu')
                avg_dict['Assigned'] = ['', '', '', ", ".join(list_of_gpu_names)]

            avg_dict = manage_thresholds(avg_dict)
            result_dict["all-resources"] = avg_dict

        executed_jobs = list()
        runtime_per_node = list()
        for val in term_info[5].values():
            executed_jobs.append(val[0])
            average_job_duration = val[1] / val[0]
            runtime_per_node.append(datetime.timedelta(average_job_duration.days, average_job_duration.seconds))

        host_nodes_dict = {
            "Host Nodes": list(term_info[5].keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        }

        result_dict["host-nodes"] = host_nodes_dict

        if len(term_info[6]) > 0:
            temp_err = term_info[6]
            del temp_err["Reason"]  # remove reason, cause thats just too much information
            result_dict["errors"] = temp_err

        result_list.append(result_dict)

    new_occ = {
        "Termination type": list(occurrence_dict.keys()),
        "Appearance": list(occurrence_dict.values())
    }
    result_list.insert(0, {"execution-details": new_occ})

    return result_list


# search in the files for the keywords
def filter_for(log_files: list_of_logs, keywords: list, extend=False) -> log_inf_list:
    """
    Filter for a list of keywords, which can be extended
    and print out every file which matches the pattern (not case sensitive)
    The filtered files can be analysed summarise, etc afterwards,
    else this function will return None

    :param log_files:
    :param keywords:
    :param extend:
    :return:
        list with dicts depending on the used mode, to forward the filtered files,

        None if no forwarding is set
    """
    logging.info('Starting the filter mode')

    # if the keywords are given as a string, try to create a list
    if type(keywords) == list:
        keyword_list = keywords
    elif type(keywords) == str:
        keyword_list = " ".join(re.split(',| ', keywords)).split()  # remove spaces and commas from string
    else:
        logging.debug(f"Filter mode only accepts a string or list with keywords, not {keywords}")
        raise_type_error("Expecting a list or a string")

    # if extend is set, keywords like err will also look for keywords like warn exception, aborted, etc.
    if extend:  # apply some rules
        # the error list
        err_list = ["err", "warn", "exception", "aborted", "abortion", "abnormal", "fatal"]

        # remove keyword if already in err_list
        for i in range(len(keyword_list)):
            if (keyword_list[i]).lower() in err_list:
                keyword_list.remove(keyword_list[i])

        keyword_list.extend(err_list)  # extend search

        rprint(f"[green]Keyword List was extended, now search for these keywords:[/green]", keyword_list)
    else:
        rprint(f"[green]Search for these keywords:[/green]", keyword_list)

    if len(keyword_list) == 1 and keyword_list[0] == "":
        logging.debug("Empty filter, don't know what to do")
        return f"{colors['yellow']}Don't know what to do with an empty filter,\n" \
            f"if you activate the filter mode in the config file, \n" \
            f"please add a [filter] section with the filter_keywords = your_filter"

    logging.debug(f"These are the keywords to look for: {keyword_list}")

    # now search
    found_at_least_one = False
    found_logs = []
    for file in track(log_files, transient=True, description="Filtering..."):
        found = False
        with open(file, "r") as read_file:
            for line in read_file:
                for keyword in keyword_list:
                    if re.search(keyword.lower(), line.lower()):
                        if not found_at_least_one:
                            print("Matches:")
                        rprint(f"[grey74]{keyword} in:\t{file}[/grey74] ")
                        found = True
                        found_at_least_one = True
                        break
                if found:
                    found_logs.append(file)
                    break

    return_dicts = None
    if not found_at_least_one:
        rprint(f"[red]Unable to find these keywords:[/red]", keyword_list)
        rprint(f"[red]maybe try again with similar expressions[/red]")

    elif mode is not None:
        print(f"Total count: {len(found_logs)}")
        if mode.__eq__("default"):
            return_dicts = default(found_logs)
        elif mode.__eq__("analysed-summary"):
            rprint("[magenta]Try to give an analysed summary for these files[/magenta]")
            return_dicts = analysed_summary(found_logs)
        elif mode.__eq__("summarize"):
            rprint("[magenta]Try to summarize these files[/magenta]")
            return_dicts = summarize(found_logs)
        elif mode.__eq__("analyse"):
            rprint("[magenta]Try to analyse these files[/magenta]")
            return_dicts = analyse(found_logs)
    elif not reading_stdin and not redirecting_output:  # if not reading from stdin or redirected
        rprint("[blue]Want do do more?[/blue]")
        x = input("default(d), summarize(s), analyse(a), analysed summary(as), exit(e): ")
        if x == "d":
            return_dicts = default(found_logs)
        elif x == "s":
            return_dicts = summarize(found_logs)
        elif x == "a":
            return_dicts = analyse(found_logs)
        elif x == "as":
            return_dicts = analysed_summary(found_logs)
        elif x == "e":
            sys.exit(0)
        else:
            print('Not a valid argument, quitting ...')
            sys.exit(0)

    return return_dicts


def validate_given_logs(file_list: list_of_logs) -> list_of_logs:
    """
    This method is supposed to take the given log files (by config or command line argument)
    and tries to determine if these are valid log files, ignoring the std_log specification, if std_log = "".

    It will run through given directories and check every single file, if accessible, for the
    HTCondor log file standard, valid log files should start with lines like:

    000 (5989.000.000) 03/30 15:48:47 Job submitted from host: <10.0.8.10:9618?addrs=10.0.8.10-9618&noUDP&sock=3775629_0774_3>

    if done, it will change the global files list and store only valid log files.
    The user will be remind, what files were accepted as "valid".

    The user will also be informed if a given file was not found.

    -> my guess: yes or no question, if nothing is given in under 10 seconds, it should go with no
    -> this should prevent, that the script is stucked, if the user is for example running it over night

    """
    valid_files = list()
    total = len(file_list)

    logging.info('Validate given log files')
    with Progress(transient=True) as progress:

        task = progress.add_task("Validating...", total=total, start=False)

        for arg in file_list:

            path = os.getcwd()  # current working directory , should be condor job summarizer script
            logs_path = path + "/" + arg  # absolute path

            working_dir_path = ""
            working_file_path = ""

            if os.path.isdir(arg):
                working_dir_path = arg
            elif os.path.isdir(logs_path):
                working_dir_path = logs_path

            elif os.path.isfile(arg):
                working_file_path = arg
            elif os.path.isfile(logs_path):
                working_file_path = logs_path
            # check if only the id was given and resolve it with the std_log specification
            elif os.path.isfile(arg + std_log):
                working_file_path = arg + std_log
            elif os.path.isfile(logs_path + std_log):
                working_file_path = logs_path + std_log

            # if path is a directory
            if working_dir_path.__ne__(""):
                rprint(f"[light_coral]Search: {working_dir_path} for valid log files[/light_coral]")
                # run through all files and separate the files into log/error and output files
                valid_dir_files = list()
                file_dir = os.listdir(working_dir_path)
                total += len(file_dir) - 1
                for file in file_dir:
                    progress.update(task, total=total, advance=1)

                    # ---- if std_log is set ignore other log files with a different suffix ----
                    if std_log.__ne__("") and not file.endswith(std_log):
                        logging.debug("Ignoring this file, " + file + ", because std_log is set to: " + std_log)
                        continue

                    # ignore hidden files
                    if file.startswith("."):
                        continue
                    # if it's not a sub folder
                    if working_dir_path.endswith('/'):
                        file_path = working_dir_path + file
                    else:
                        file_path = working_dir_path + '/' + file

                    if os.path.isfile(file_path):
                        with open(file_path, "r") as read_file:
                            if os.path.getsize(file_path) == 0:  # file is empty
                                continue

                            if re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)", read_file.readlines()[0]):
                                # logging.debug(f"{read_file.name} is a valid HTCondor log file")
                                valid_dir_files.append(file_path)

                    else:
                        logging.debug(f"Found a subfolder: {working_dir_path}/{file}, it will be ignored")
                        progress.console.print(f"[yellow]Found a subfolder: {working_dir_path}/{file},"
                                               f" it will be ignored[/yellow]")

                valid_files.extend(valid_dir_files)
                rprint(f"[green]Found {len(valid_dir_files)} valid log files out of "
                       f"{len(file_dir)} files[/green]\n")
            # else if path "might" be a valid HTCondor file
            elif working_file_path.__ne__(""):
                progress.update(task, advance=1)

                # ---- if std_log is set ignore other log files with a different suffix ----
                if std_log.__ne__("") and not working_file_path.endswith(std_log):
                    logging.debug("Ignoring this file, " +
                                  working_file_path + ", because std_log is set to: " + std_log)
                    continue

                if std_err.__ne__("") and working_file_path.endswith(std_err) or \
                        std_out.__ne__("") and working_file_path.endswith(std_out):
                    logging.debug(f"Only log files accepted, ignored this file: {working_file_path}")
                    continue

                with open(working_file_path, "r") as read_file:

                    if os.path.getsize(working_file_path) == 0:  # file is empty
                        progress.console.print(f"[red]How dare you, the file is empty: {read_file.name} :([/red]")

                    elif re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)", read_file.readlines()[0]):
                        logging.debug(f"{read_file.name} is a valid HTCondor log file")
                        valid_files.append(working_file_path)
                    else:
                        logging.debug(f"The given file {read_file.name} is not a valid HTCondor log file")
                        progress.console.print(f"[yellow]The given file {read_file.name} "
                                               f"is not a valid HTCondor log file[/yellow]", )
            else:
                logging.error(f"The given file: {arg} does not exist")
                rprint(f"[red]The given file: {arg} does not exist[/red]")

    return valid_files


def setup_logging_tool(log_file=sys.argv[0] + '.log'):
    """
        Set up the logging device,
        to generate a log file, with --generate-log-file
        or to print more descriptive output with the verbose mode to stdout

        both modes are compatible together
    :return:
    """

    # disable the loggeing tool by default
    logging.getLogger().disabled = True

    # I don't know why a root handler is already set,
    # but we have to remove him in order
    # to get just the output of our own handler
    if len(logging.root.handlers) == 1:
        default_handler = logging.root.handlers[0]
        logging.root.removeHandler(default_handler)

    # if logging tool is set to use
    if generate_log_file:
        # activate logger if not already activated
        logging.getLogger().disabled = False

        # more specific view into the script itself
        logging_file_format = '%(asctime)s - [%(funcName)s:%(lineno)d] %(levelname)s : %(message)s'
        file_formatter = logging.Formatter(logging_file_format)

        handler = RotatingFileHandler(log_file, maxBytes=1000000, backupCount=1)
        handler.setLevel(logging.DEBUG)
        handler.setFormatter(file_formatter)

        log = logging.getLogger()
        log.setLevel(logging.DEBUG)
        log.addHandler(handler)

    if verbose_mode:
        # activate logger if not already activated
        logging.getLogger().disabled = False

        logging_stdout_format = '%(asctime)s - %(levelname)s: %(message)s'
        stdout_formatter = logging.Formatter(logging_stdout_format)

        stdout_handler = logging.StreamHandler(sys.stdout)
        stdout_handler.setLevel(logging.DEBUG)
        stdout_handler.setFormatter(stdout_formatter)
        log = logging.getLogger()
        log.setLevel(logging.DEBUG)
        log.addHandler(stdout_handler)


def customize_results(files: list_of_logs):
    output_string = ""

    if filter_mode:
        results = filter_for(files, filter_keywords, filter_extended)
    elif mode.__eq__("default"):
        results = default(files)  # force default with -d
    elif mode.__eq__("analysed-summary"):
        results = analysed_summary(files)  # analysed summary ?
    elif mode.__eq__("summarize"):
        results = summarize(files)  # summarize information
    elif mode.__eq__("analyse"):
        results = analyse(files)  # analyse the given files
    else:
        results = default(files)  # anyways try to print default output

    # This can happen, when for example the filter mode is not forwarded
    if results is None:
        sys.exit(0)

    work_with = results
    # convert result to list, if given as dict
    if type(results) == dict:
        work_with = [results]

    # check for ignore values
    for i in range(len(work_with)):
        mystery = work_with[i]

        if "execution-details" in mystery:
            if "execution-details" in ignore_list:
                del mystery["execution-details"]
            else:
                mystery["execution-details"] = tabulate(mystery["execution-details"],
                                                        headers='keys', tablefmt=table_format)

        if "times" in mystery:
            if "times" in ignore_list:
                del mystery["times"]
            else:
                mystery["times"] = tabulate(mystery["times"], showindex=False, headers='keys', tablefmt=table_format)

        if "all-resources" in mystery:
            resources = mystery["all-resources"]
            if "all-resources" in ignore_list:
                del mystery["all-resources"]
            else:
                if "used-resources" in ignore_list:
                    del mystery["all-resources"]["Usage"]
                if "requested-resources" in ignore_list:
                    del mystery["all-resources"]["Requested"]
                if "allocated-resources" in ignore_list:
                    del mystery["all-resources"]["Allocated"]

                mystery["all-resources"] = tabulate(resources, showindex=False, headers='keys',
                                                    tablefmt=table_format)

        if "errors" in mystery:
            if "errors" in ignore_list:
                del mystery["errors"]
            else:
                mystery["errors"] = "Occurred errors: \n" + \
                                    tabulate(mystery["errors"], showindex=False, headers='keys', tablefmt='grid')

        if "host-nodes" in mystery:
            if "host-nodes" in ignore_list:
                del mystery["host-nodes"]
            else:
                mystery["host-nodes"] = tabulate(mystery["host-nodes"], showindex=False,
                                                 headers='keys', tablefmt=table_format)

        # TODO: show-more section

        for _i in mystery:
            if mystery[_i] is not None:
                output_string += mystery[_i]
            else:
                logging.debug("This musst be fixed, NoneType found.")
                rprint("[red]NoneType object found, this should not happen[/red]")

            if mystery[_i] == mystery[list(mystery.keys())[-1]]:
                output_string += "\n"
            else:
                output_string += "\n\n"

        output_string += "\n"

    return output_string


def run(commandline_args):
    """
    Run this script

    This searches first for a given config file.
    After that, given parameters in the terminal will be interpreted, so they have a higher priority
    given files will be validated and the logging tool will be managed
    After that the user input will be used to process the output to the terminal,
    which will contain information about the given log files

    :return:
    """
    try:
        start = datetime.datetime.now()  # startdate

        initialize()  # initialize global parameters

        check_for_redirection()  # set the color sequenzes to ""

        # if exit parameters are given that will interrupt this script, like --help,
        # catch them here so the config won't be unnecessary loaded
        manage_prioritized_params(commandline_args)

        # if not --no-config is set:
        # interpret the first file, that can be interpreted as a config file and remove it, other given config files,
        # (you will see when you try, files will be interpreted as HTCondor log files)
        # so it's not possible to give multiple config files
        found_conf = None
        if not no_config:
            found_conf, commandline_args = find_config(commandline_args)

        setup_logging_tool()  # set up the logging device

        logging.debug("-------Start of htcompact scipt-------")

        if verbose_mode:
            logging.info('Verbose mode turned on')

        if reading_stdin:
            logging.debug("Reading from stdin")
        if redirecting_output:
            logging.debug("Output is getting redirected")

        if found_conf is not None:
            load_config(found_conf)

        # after that, interpret the command line arguments, these will have a higher priority, than the config setup
        # and by that arguments set in the config file, will be ignored
        manage_params(commandline_args)

        global files
        valid_files = validate_given_logs(files)  # validate the files, make a list of all valid config files

        if len(valid_files) == 0:
            rprint("[red]No valid HTCondor log files found[/red]")
            sys.exit(2)

        output_str = customize_results(valid_files)

        print(output_str)  # write it to the console

        end = datetime.datetime.now()  # enddate

        logging.debug(f"Runtime: {end-start}")  # runtime of this script

        logging.debug("-------End of htcompact script-------")

        sys.exit(0)

    except TypeError as err:
        logging.exception(err)
        rprint(f"[red]{err.__class__.__name__}: {err}[/red]")
        sys.exit(3)

    except KeyboardInterrupt:
        logging.info("Script was interrupted by the user")
        print("Script was interrupted")
        sys.exit(4)


if __name__ == "__main__":
    """
    This is the main function, which runs the script, if not imported as a module

    :return:
    """
    run(sys.argv[1:])



