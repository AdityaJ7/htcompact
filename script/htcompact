#!/usr/bin/env python3
import re
import sys
import os
import getopt
import datetime
import logging

import configparser
import pandas as pd
import numpy as np
from tabulate import tabulate


"""

This script is basically reading information from HTCondor log files and storing them into
pandas DataFrames. By that its actually pretty simple to analyse the data.

The script makes the information compact and easy to under stand, that's the reason for the name htcompact

Single logs can be read quite easily, but also it's possible to summarise a wholÃ¶e directory with logs 
to see for ex. the avarage runtime and usage of all the logs

"""

# Todo: logging should be declared in setup and command line arguments
# make default changes to logging tool
log_disabled = False
logging.getLogger().disabled = log_disabled  # no more logging
if not log_disabled:
    logging.basicConfig(filename="stdout.log", level=logging.DEBUG,
                        format='%(asctime)s - [%(funcName)s:%(lineno)d] %(levelname)s : %(message)s')

# global parameters, used for dynamical output of information
accepted_states = ["true", "yes", "y", "ja", "j", "enable", "enabled", "wahr", "0"]
files = list()
border_str = ""

# variables for given parameters
show_output = False
show_warnings = False
ignore_allocated_resources = False
show_errors = False
ignore_job_information = False
ignore_resources = False

# Features:
resolve_ip_to_hostname = False
reverse_dns_lookup = False  # Todo: implement in function (filter_for_host)
summarise = False
to_csv = False
indexing = True # only for csv structure

# escape sequences for colors
red = "\033[0;31m"
green = "\033[0;32m"
yellow = "\033[0;33m"
cyan = "\033[0;36m"
back_to_default = "\033[0;39m"

# thresholds for bad and low usage of resources
low_usage = 0.75
bad_usage = 1.2

# global variables with default values for err/log/out files
std_log = ""
std_err = ""
std_out = ""

# global variables for tabulate
table_format = "pretty"  # ascii by default

# Todos:
# Todo: did a lot of test, but it needs more
# Todo: background colors etc. for terminal usage
# Todo: filter erros better, less priority
# Todo: realise the further specs on: https://jugit.fz-juelich.de/inm7/infrastructure/scripts/-/issues/1
# a redirection in the terminal via > should ignore escape sequences


# may not be needed in future but will be used for now
def remove_files_from_args(args, short_opts, long_opts):
    """
        I want to make it easier for the user to insert many files at once.
        The script should detect option arguments and files

        It will filter the given files and remove them from the argument list

        !!!Exactly that is this method doing!!! (because getopt has no such function)

    :param args: is supposed to be a list, in any case it should be the sys.argv[1:] list
    :param short_opts: the short getopt options
    :param long_opts: the long getopt options
    :return: list of getopt arguments
    """
    new_args = args.copy()
    generate_files = list()
    index = 0
    # run through the command line arguments, skip the getopt argument and interprete everything else as a file
    while True:
        if index >= len(args):
            break

        arg = args[index]

        if arg.startswith("-"):  # for short_args
            arg = arg[1:]
            if arg.startswith("-"):  # for long_args
                arg = arg[1:]

            # skip the argument if getopt arguments are setable
            if arg+":" in short_opts:
                index += 1
            elif arg+"=" in long_opts:
                index += 1
        else:
            generate_files.append(arg)
            new_args.remove(arg)

        index += 1

    # change files, if found
    if len(generate_files) > 0:
        global files
        files = generate_files

    # return valid arguments
    return new_args


# Todo: thresholds by command line
def manage_params():
    """
    Interprets the given command line arguments and changes the global variables in this scrips

    """
    global files  # list of files and directories
    global std_log, std_err, std_out  # all default values for the HTCondor files
    global show_output, show_warnings, ignore_allocated_resources  # show more information variables
    global show_errors, ignore_resources, ignore_job_information  # ignore information variables
    global to_csv, summarise, indexing  # features
    global table_format  # table_format can be changed
    global resolve_ip_to_hostname  # if set hots ip will changed to a cpu-number

    # if output gets redirected with > or | or other redirection tools, ignore escape sequences
    if not sys.stdout.isatty():
        global red, green, yellow, cyan, back_to_default
        red = ""
        green = ""
        yellow = ""
        cyan = ""
        back_to_default = ""
        logging.debug("Output is getting redirected, all escape sequences were set to \"\"")

    option_shorts = "hs"
    option_longs = ["help", "std-log=", "std-err=", "std-out=",
                    "show-errors", "show-output", "show-warnings",
                    "ignore-resources", "ignore-job-information", "ignore-allocated-resources",
                    "to-csv", "indexing=", "summarise",
                    "resolve-ip", "table-format="]

    better_args = remove_files_from_args(sys.argv[1:], option_shorts, option_longs)

    try:
        opts, args = getopt.getopt(better_args, option_shorts, option_longs)

        for opt, arg in opts:

            # catch unusual but not wrong parameters starting with -
            if arg.startswith("-"):
                print(yellow+"The argument for {0} is {1}, is that wanted?".format(opt, arg)+back_to_default)
                logging.warning("The argument for {0} is {1}, is that wanted?".format(opt, arg))

            if opt in ["-h", "--help"]:
                print(help_me())
                sys.exit(0)
            elif opt in ["-s", "--summarise"]:
                summarise = True
                logging.debug("Summariser mode turned on")
            # all HTCondor files, given by the user if they are not saved in .log/.err/.out files
            elif opt == "--std-log":
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_log = arg
            elif opt == "--std-err":
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_err = arg
            elif opt == "--std-out":
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_out = arg
            # all variables, to show more specific information
            elif opt.__eq__("--show-output"):
                show_output = True
            elif opt.__eq__("--show-warnings"):
                show_warnings = True
            elif opt.__eq__("--ignore-allocated-resources"):
                ignore_allocated_resources = True

            # all variables to ignore unwanted information
            elif opt.__eq__("--show-errors"):
                show_errors = True
            elif opt.__eq__("--ignore-resources"):
                ignore_resources = True
            elif opt.__eq__("--ignore-job-information"):
                ignore_job_information = True

            # all tabulate variables
            elif opt.__eq__("--to-csv"):
                to_csv = True
            elif opt.__eq__("--indexing"):
                indexing = (arg.lower() in accepted_states)
                logging.debug("Indexing set to: {0}".format(indexing))

            elif opt.__eq__("--table-format"):
                types = "plain,simple,github,grid,fancy_grid,pipe," \
                        "orgtbl,rst,mediawiki,html,latex,latex_raw," \
                        "latex_booktabs,tsv,pretty"
                # only valid arguments
                if arg in types.split(","):
                    table_format = arg
                else:
                    logging.debug("The given table format doesn't exist")

            elif opt.__eq__("--resolve-ip"):
                resolve_ip_to_hostname = True

            else:
                print(help_me())
                sys.exit(0)
    # print error messages
    except Exception as err:
        logging.exception(err)  # write a detailed description in the stdout.log file
        print((red+"{0}: {1}"+back_to_default).format(err.__class__.__name__, err))
        print(help_me())
        sys.exit(1)

    if len(files) == 0:
        print(help_me())
        sys.exit(2)


def help_me():
    """
    Usage: htcompact (log_files|config_file) [ Arguments ]

    Arguments:                  [-h|--help, -s|--summarise
                                --std-log=, --std-err=, --std-out=
                                --show-output, --show-warnings, --show-allocated
                                --ignore-errors, --ignore-resources
                                --to-csv, indexing=,
                                --resolve-ip
                                table-format=]

    [-h|--help]                 to show this dialog

    ----------------------------standard HTCondor files:------------------------

    [--std-log=log_file]        describe the format of the HTCondor log files
                                for example if log files look like: 452_0.log
                                then set --std-log=.log  | default is .log

    [--std-err=error_file]      describe the format of the HTCondor err files
                                for example if error files look like: 452_0.err
                                then set --std-err=.err  | default is .err

    [--std-out=output_file]     describe the format of the HTCondor output files
                                for example if output files look like: 452_0.out
                                then set --std-out=.out  | default is .out

    ----------------------------show more information:--------------------------

    [--show-output]             shows job related output if the related output
                                file is in the same directory as the log file

    [--show-warnings]           shows warnings that occurred inside the HTCondor
                                error file

    [--show-allocated]          shows allocated resources related to the used
                                and requested resources

    ----------------------------ignore information:-----------------------------

    [--ignore-errors]           ignores all occurring erros inside the
                                HTCondor error files

    [--ignore-resources]        ignores all resources, show-allocated will have
                                no effect, if this argument is given

    ----------------------------csv related settings:---------------------------

    [--to-csv]                  Has to be implemented

    [--indexing=(True|False)]   no effect yet

    ----------------------------features:---------------------------------------

    [-s|--summarise]            Takes all given logs and summarises resources,
                                runtime and other informations in total and
                                average

    [--resolve-ip]              if set ips like: 10.0.9.4:9618, will be reduced
                                to cpu 4, which only applies to our cluster in
                                the FZJ

    ----------------------------output settings:--------------------------------

    [--table-format=format]     the table format for the output,
                                if not res-to-csv or job-to-csv is given

                                valid arguments are:
                                [plain, simple, github, grid, fancy_grid, pipe,
                                orgtbl, rst, mediawiki, html, latex, latex_raw,
                                latex_booktabs, tsv, pretty]

                                default: pretty

    ----------------------------config setup:----------------------------------

    furthermore all these variables|settings can be set inside a config file
    See the #Todo: link to config explanation

    A basic config file htcsetup.conf will be installed with this script.
    The script is also checking for other config files in other places:

    \033[0;32m"/etc" and "~/.config/htcompact/"\033[0;39m

    with different priorities from 1 (high) to 4 (low):
    Priority[1] find config_file directly
    Priority[2] search for config_file in /etc
    Priority[3] search for config_file in ~/.config/htcompact
    Priority[4] take the "htcsetup.conf" from the Project


    """
    # returns this docstring
    return help_me.__doc__


# reads all information, but returns them in two lists
def read_condor_logs(file):
    """

    reads a given HTCondor std_log file and separates the information in two lists,
    for further and easier access

    :param file:    a HTCondor std_log file

    :raises: :class:'FileNotFoundError': if open does not work

    :return:    (job_event, job_event_information)
                a list of job_events and a list of job_event-relevant information
                these are related by the same index like:
                job_event[1]-> relevant information: job_event_information[1]
    """

    log_job_events = list()  # saves the job_events
    job_event_information = list()  # saves the information for each job event, if there are any
    temp_job_event_list = list()  # just a temporary list, gets inserted into job_event_information

    with open(file) as log_file:
        # for every line in the log_file
        for line in log_file:

            line = line.rstrip("\n")  # remove newlines
            match_log\
                = re.match(r"([0-9]{3})"  # matches event number
                           r" (\([0-9]+.[0-9]+.[0-9]{3}\))"  # matches clusterid and process id
                           r" ([0-9]{2}/[0-9]{2})"  # matches date
                           r" ([0-9]{2}:[0-9]{2}:[0-9]{2})"  # matches time
                           r"((?: \w+)*)."  # matches the job event name
                           r"(.*)", line)  # matches further information
            if match_log:
                job_event_number = match_log[1]  # job event number, can be found in job_description.txt
                clusterid_procid_inf = match_log[2]  # same numbers, that make the name of the file
                date = match_log[3]  # filter the date in the form Month/Day in numbers
                time = match_log[4]  # filter the time
                job_event_name = match_log[5]  # filter the job event name
                job_relevant_inf = match_log[6]  # filter the job relevant information
                log_job_events.append([job_event_number, clusterid_procid_inf,
                                       date, time, job_event_name, job_relevant_inf])
                # print(job_event_number, clusterid_procid_inf, date, time, job_event_name, job_relevant_inf)
            else:
                # end of job event
                if "..." in line:
                    job_event_information.append(temp_job_event_list)
                    temp_job_event_list = list()  # clear list
                    continue
                # else
                #if re.match("[\t ]+", line):
                else:
                    temp_job_event_list.append(line)
    return log_job_events, job_event_information


def raiseValueError(message):
    raise ValueError(message)


# Todo: store job event numbers ( see job_descriptions.txt)
# Todo: Read HTCondor Errors that occurred
def log_to_dataframe(file):
    try:
        job_events, job_raw_information = read_condor_logs(file)

        submitted_date = datetime.datetime.strptime(job_events[0][2] + " " + job_events[0][3], "%m/%d %H:%M:%S")

        if job_events[-1][0].__eq__("005"):  # if the last job event is : Job terminated

            # calculate the runtime for the job
            terminating_date = datetime.datetime.strptime(job_events[-1][2] + " " + job_events[-1][3], "%m/%d %H:%M:%S")
            runtime = terminating_date - submitted_date  # calculation of the time runtime

            match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job_events[1][5])
            if match_host:
                host = match_host[1]
                # if resolve ip to hostname, change the ip to cpu: last number
                if resolve_ip_to_hostname:
                    host = "cpu node: " + host.split('.')[-1]
                port = match_host[2]
            else:
                logging.exception("Host and port haven't been matched correctly")
                print(
                    red + "your log file has faulty values for the host ip make sure it's a valid IPv4" + back_to_default)

            # make a fancy design for the job_information
            job_labels = ["Executing on Host", "Port", "Runtime"]  # holds the labels
            job_information = [host, port, runtime]  # holds the related job information

            # filter the termination state ...
            if True:
                # check if termination state is normal
                termination_state_inf = job_raw_information[-1][0]
                match_termination_state = re.match(r"[\t ]+\([0-9]+\) ((?:\w+ )*)", termination_state_inf)
                if match_termination_state:
                    job_labels.append("Termination State")

                    if "Normal termination" in match_termination_state[1]:
                        job_information.append(match_termination_state[1])
                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", termination_state_inf)
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)
                    elif "Abnormal termination" in match_termination_state[1]:
                        job_information.append(red+match_termination_state[1]+back_to_default)

                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", termination_state_inf)
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)

                        termination_desc = list()
                        # append the reason for the Abnormal Termination
                        for desc in job_raw_information[-1][1:]:
                            if re.match(r"[\t ]+\(0\)",desc):
                                termination_desc.append(desc[5:])
                            else:
                                break

                        if not len(termination_desc) == 0:
                            job_labels.append("Description")
                            job_information.append("\n".join(termination_desc))
                        else:
                            logging.debug(f"No termination description for {file} found")

                    else:
                        logging.debug(f"Don't know this Termination State: {match_termination_state[1]} yet")

                else:
                    print(red + f"Termination error in HTCondor log file: {file}" + back_to_default)

            # now put everything together in a table
            job_df = pd.DataFrame({
                "Description": job_labels,
                "Values": job_information
            })

            # these where the job information now focus on the used resources

            relevant_str = "\n".join(job_raw_information[-1])  # job information of the last job (Job terminated)

            # next part removes not useful lines
            if True:  # just for readability
                # remove unnecessary lines
                lines = relevant_str.splitlines()
                # while not lines[0].startswith("\tPartitionable"):
                while not re.match(r"[\t ]+Partitionable", lines[0]):
                    lines.remove(lines[0])

                lines.remove(lines[0])
                partitionable_res = lines
                # done, partitionable_resources contain now only information about used resources

            # match all resources
            # ****************************************************************************************************
            match = re.match(r"[\t ]+Cpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+)", partitionable_res[0])
            if match:
                cpu_usage, cpu_request, cpu_allocated = match[1], match[2], match[3]
            else:
                raiseValueError("Something went wrong reading the cpu information")

            match = re.match(r"[\t ]+Disk \(KB\) *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[1])
            if match:
                disk_usage, disk_request, disk_allocated = match[1], match[2], match[3]
            else:
                raise raiseValueError("Something went wrong reading the disk information")

            # check if the log has gpu information in between
            gpu_match_index = 2  # can be ignored if gpu is given memory has to access the next index
            gpu_found = False  # easier to check to add information in the DataFrame
            match = re.match(r"[\t ]+Gpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+) \"(.*)\"",
                             partitionable_res[gpu_match_index])
            if match:
                gpu_usage, gpu_request, gpu_allocated, gpu_name = match[1], match[2], match[3], match[4]
                gpu_match_index += 1
                gpu_found = True

            match = re.match(r"[\t ]+Memory \(MB\)  *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[gpu_match_index])
            if match:
                memory_usage, memory_request, memory_allocated = match[1], match[2], match[3]
            else:
                raiseValueError("Something went wrong reading the memory information")
            # ****************************************************************************************************

            # list of resources and their labels
            resource_labels = ["Cpu", "Disk", "Memory"]
            usage = [cpu_usage, disk_usage, memory_usage]
            requested = [cpu_request, disk_request, memory_request]
            allocated = [cpu_allocated, disk_allocated, memory_allocated]

            if gpu_found:
                resource_labels.append("Gpus")
                usage.append(gpu_usage)
                requested.append(gpu_request)
                allocated.append(gpu_allocated)

            # Error handling: change empty values to NaN in the first column
            for i in range(3):
                if usage[i] == "":
                    usage[i] = np.nan
                if requested[i] == "":
                    requested[i] = np.nan
                if allocated[i] == "":
                    allocated[i] = np.nan

            # put the data in the DataFrame
            res_df = pd.DataFrame({
                "Rescources": resource_labels,
                "Usage": usage,
                "Requested": requested,
                # "Allocated": allocated
            })

            # if the user wants allocated resources then add it to the DataFrame as well
            temp_index = 3
            if not ignore_allocated_resources:
                res_df.insert(temp_index, "Allocated", allocated)
                temp_index += 1
            if gpu_found:
                res_df.insert(temp_index, "Assigned", ["", "", "", gpu_name])

        # Todo: more information, maybe why ?
        elif job_events[-1][0].__eq__("009"):  # job aborted

            # calculate the runtime for the job
            if job_events[-1].__ne__(job_events[0]):
                terminating_date = datetime.datetime.strptime(job_events[-1][2] + " " + job_events[-1][3], "%m/%d %H:%M:%S")
                runtime = terminating_date - submitted_date  # calculation of the time runtime
            else:
                runtime = datetime.timedelta(0)

            user = ((job_raw_information[-1][0]).split(" ")[-1])[:-1]
            job_df = pd.DataFrame({
                "Description": ['Aborted by:', 'Runtime'],
                "Values": [user, runtime]
            })
            res_df = None
            logging.debug(f"{file}: Job arboted by {user}")

    except NameError as err:
        logging.exception(err)
        print("The smart_output_logs method requires a " + std_log + " file as parameter")
    except FileNotFoundError or ValueError or IndexError as err:
        logging.exception(err)
        print(red + str(err) + back_to_default)
    # finally
    else:
        return job_df, res_df

def smart_output_logs(file):
    """
    reads a given HTCondor .log file with the read_condor_logs() function

    :param file:    a HTCondor .log file
    :param header:  Shows the header of the columns
    :param index:   Shows the index of the rows

    :return:        (output_string)
                    an output string that shows information like:
                    The job procedure of : ../logs/454_199.log
                    +-------------------+--------------------+
                    | Executing on Host |      10.0.9.1      |
                    |       Port        |        9618        |
                    |      Runtime      |      1:12:20       |
                    | Termination State | Normal termination |
                    |   Return Value    |         0          |
                    +-------------------+--------------------+
                    +--------+-------+-----------+-----------+
                    |        | Usage | Requested | Allocated |
                    +--------+-------+-----------+-----------+
                    |  Cpu   | 0.30  |     1     |     1     |
                    |  Disk  |  200  |    200    |  3770656  |
                    | Memory |   3   |     1     |    128    |
                    +--------+-------+-----------+-----------+

    """
    try:

        job_df, res_df = log_to_dataframe(file)

        output_string = ""

        # Todo: csv style one line
        if to_csv:
            pass
        else:
            global border_str
            output_string += green + "The job procedure of : " + file + back_to_default + "\n"
            border_str = "-" * len(output_string) + "\n"

            if not isinstance(res_df, type(None)):  # make sure res_df is not None

                # reuested vs used disk RAM
                if float(res_df.iloc[1, 1]) / float(res_df.iloc[1, 2]) > bad_usage:
                    res_df.iloc[1, 1] = red+res_df.iloc[1, 1]+back_to_default  # changed color to red
                elif float(res_df.iloc[1, 1]) / float(res_df.iloc[1, 2]) < low_usage:
                    res_df.iloc[1, 1] = yellow+res_df.iloc[1, 1]+back_to_default  # changed color to yellow

                # requested vs used Memory
                if float(res_df.iloc[2, 1]) / float(res_df.iloc[2, 2]) > bad_usage:
                    res_df.iloc[2, 1] = red+res_df.iloc[2, 1]+back_to_default  # changed color to red
                elif float(res_df.iloc[2, 1]) / float(res_df.iloc[2, 2]) < low_usage:
                    res_df.iloc[2, 1] = yellow+res_df.iloc[2, 1]+back_to_default  # changed color to yellow

            if not ignore_job_information:
                output_string += tabulate(job_df, tablefmt=table_format, showindex=False) + "\n"
            if not ignore_resources:
                output_string += tabulate(res_df, headers='keys', tablefmt=table_format, showindex=False) + "\n"

    except NameError as err:
        logging.exception(err)
        print("The smart_output_logs method requires a "+std_log+" file as parameter")
    except Exception as err:
        logging.exception(err)
        print(red+str(err)+back_to_default)
    # finally
    else:
        return output_string


# Todo: maybe less information
# just read .err files content and return it as a string
def read_condor_error(file):
    """
    Reads a HTCondor .err file and returns its content

    :param file: a HTCondor .err file

    :raises: :class:'NameError': The param file was no .err file
             :class:'FileNotFoundError': if open does not work

    :return: file content

    """
    if not file.endswith(std_err):
        raise NameError("The read_condor_error method is only for "+std_err+" files")

    err_file = open(file)
    return "".join(err_file.readlines())


# just read .out files content and return it as a string
def read_condor_output(file):
    """
        Reads a HTCondor .out file and returns its content

        :param file: a HTCondor .out file

        :raises: :class:'NameError': The param file was no .out file
                 :class:'FileNotFoundError': if open does not work

        :return: file content

        """
    if not file.endswith(std_out):
        raise NameError("The read_condor_output method is only for "+std_out+" files")

    out_file = open(file)
    return "".join(out_file.readlines())


# Todo:
def filter_for_host(ip):
    """
    this function is supposed to filter a given ip for it's representive url like juseless.inm7.de:cpu1
    :return:
    """


def smart_output_error(file):
    """

    :param file: a HTCondor .err file
    :return: the content of the given file as a string
    """

    # Todo:
    # - errors from htcondor are more important (std.err output)
    # - are there errors ? true or false -> maybe print those in any kind of fromat
    # - maybe check for file size !!!
    # - is the file size the same ? what is normal / different errors (feature -> for  -> later)

    output_string = ""
    try:
        error_content = read_condor_error(file)
        for line in error_content.split("\n"):
            if "err" in line.lower() and show_errors:
                output_string += red + line + back_to_default + "\n"
            elif "warn" in line.lower() and show_warnings:
                output_string += yellow + line + back_to_default + "\n"

    except NameError as err:
        logging.exception(err)
        print("The smart_output_error method requires a "+std_err+" file as parameter")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)"+std_err, relevant[1])
        print(yellow + "There is no related {0} file: {1} in the directory:{2}\n'{3}'\n"
              " with the prefix: {4}{5}"
              .format(std_err, relevant[1], cyan, os.path.abspath(relevant[0]), match[1], back_to_default))
    except TypeError as err:
        logging.exception(err)
        print(red+str(err)+back_to_default)
    finally:
        return output_string


def smart_output_output(file):
    """
    Todo: filter the output ?
    :param file: a HTCondor .out file
    :return: the content of that file as a string
    """

    output_string = ""
    try:
        output_string = read_condor_output(file)
    except NameError as err:
        logging.exception(err)
        print("The smart_output_output method requires a "+std_out+" file as parameter")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)"+std_out, relevant[1])
        print(yellow + "There is no related {0} file: {1} in the directory:{2}\n'{3}'\n"
                       " with the prefix: {4}{5}"
              .format(std_out, relevant[1], cyan, os.path.abspath(relevant[0]), match[1], back_to_default))

    finally:
        return output_string


def smart_manage_all(file):
    """
    Combine all informations, that the user wants for a given job_spec_id like 398_31.
    -the first layer represents the HTCondor .log file
    -the second layer represents the HTCondor .err file
    -the third layer represents the HTCondor .out file

    information will be put together regarding the global booleans set by the user.
    job_spec_id, is the ClusterID_ProcessID of a job executed by HTCondor like job_4323_21 or 231_0

    :param file: file name without endling like job4323_1, job4323_1.<err|out|log> will cut the end off
    :return: a string that combines HTCondor log/err and out files and returns it
    """
    try:
        if std_log.__ne__(""):
            job_spec_id = file[:-len(std_log)]
        else:
            job_spec_id = file

        output_string = smart_output_logs(job_spec_id + std_log)  # normal smart_output of log files

        if show_errors:  # show errors ?
            output_string += smart_output_error(job_spec_id + std_err)

        if show_output:  # show output content ?
            output_string += smart_output_output(job_spec_id + std_out)
    except Exception as err:
        logging.exception(err)
        print(red+str(err)+back_to_default)
        sys.exit(1)
    else:
        return output_string


def read_through_logs_dir(directory):
    """
    Runs through the given directory and return all logs related content
    via the smart_output_logs methods

    :param directory: a directory were the *log/err/out* files should be stored

    :return: an output string, that returns all outputs
    """
    path = os.getcwd()  # current working directory , should be condor job summariser script
    logs_path = path + "/" + directory

    if os.path.isdir(directory):
        working_path = directory
    elif os.path.isdir(logs_path):
        working_path = logs_path
    else:
        raise NameError("The given directory is either no directory or the path was wrong")

    output_string = ""
    # list are better to separate corresponding files
    list_of_logs = list()
    list_of_errors = list()
    list_of_outputs = list()
    # run through all files and separate the files into log/error and output files
    for file in os.listdir(working_path):
        if file.endswith(std_log):
            list_of_logs.append(file)
        elif file.endswith(std_err):
            list_of_errors.append(file)
        elif file.endswith(std_out):
            list_of_outputs.append(file)

    list_of_logs.sort()
    # now find every log
    for file in list_of_logs:
        job_id = file[:-len(std_log)]
        file_path = directory + "/" + job_id

        output_string += smart_manage_all(file_path)
        # separate logs if file is not the last occurring file in the files list
        if not file == list_of_logs[-1] or not directory == files[-1]:
            output_string += "\n" + border_str + "\n"

    return output_string


# Todo: change for no specified type of log error and output files
def output_given_logs():
    output_string = ""
    current_path = os.getcwd()
    # go through all given logs and check for each if it is a directory or file and if std_log was missing or not
    for i, file in enumerate(files):

        # for the * operation it should skip std_err and std_out files
        if file.endswith(std_err) and not std_err.__eq__("") or file.endswith(std_out) and not std_out.__eq__(""):
            continue
        # check if file is a directory and run through the whole directory
        if os.path.isdir(file) or os.path.isdir(current_path + "/" + file):
            output_string += read_through_logs_dir(file)
        # else check if file is a valid file
        elif os.path.isfile(file) or os.path.isfile(current_path + "/" + file):
            output_string += smart_manage_all(file)

        # if that did not work try std_log at the end of that file
        elif not file.endswith(std_log):
            new_path = file + std_log
            # is this now a valid file ?
            if os.path.isfile(new_path) or os.path.isfile(current_path + "/" + new_path):
                output_string += smart_manage_all(new_path)
            # no file or directory found, even after manipulating the string
            else:
                logging.error("No such file with that name or prefix: {0}".format(file))
                output_string += red+"No such file with that name or prefix: {0}".format(file)+back_to_default
        # The given .log file was not found
        else:
            output_string += red + "No such file: {0}".format(file) + back_to_default
            logging.error(f"No such file: {file}")

        # don't change output if csv style is wanted
        # Todo:
        if to_csv:
            pass
        # because read_through_dir is already separating and after the last occurring file
        # no separation is needed
        elif not os.path.isdir(file) and not os.path.isdir(current_path + "/" + file) and not i == len(files)-1:
            output_string += "\n" + border_str + "\n"  # if empty it still contains a newline

    if output_string == "":
        print(yellow+"Nothing found to analyse, please use htcompact --help for help"+back_to_default)

    return output_string


# search for config file ( UNIX BASED )
# Todo: Test
#       Change some varibales
def load_config():
    """

    Reads config file and changes global parameter by its configuration

    :return: False if not found, True if found and
    """

    script_name = sys.argv[0]
    if script_name.endswith(".py"):
        script_name = sys.argv[0][:-3]  # scriptname without .py
    config = configparser.ConfigParser()
    # search for the given file, first directly, then in /etc and then in ~/.config/htcompact/
    found_config = False

    print(cyan, end="")

    if len(sys.argv) > 1:
        # run through every given argument and try to determine if it's a valid config file
        arguments = sys.argv[1:]
        arguments.append("htcsetup.conf")  # default config, should be checked after no one was found **
        # Todo: maybe search in the current directory for config files
        for file in arguments:
            # try to find the given file in the current directory, in /etc or in ~/.config/htcompact/
            try:
                if os.path.isfile(file) and config.read(file):
                    logging.debug(f"Load config from the file: {file}")
                    print(f"Load config from the file: {file}")
                    found_config = True
                    arguments.remove(file)
                    logging.debug("Removed {0} from arguments".format(file))
                # try to find config file in /etc
                elif os.path.isfile(f"/etc/{file}") and config.read(f"/etc/{file}"):
                    logging.debug(f"Load config from: /etc/{file}")
                    print(f"Load config from: /etc/{file}")
                    found_config = True
                    arguments.remove(file)
                    logging.debug("Removed {0} from arguments".format(file))
                # try to find config file in ~/.config/script_name/
                elif os.path.isfile(f"~/.config/{script_name}/{file}") \
                        and config.read(f"~/.config/{script_name}/{file}"):
                    logging.debug(f"Load config from: ~/.config/{script_name}/{file}")
                    print(f"Load config from: ~/.config/{script_name}/{file}")
                    found_config = True
                    arguments.remove(file)
                    logging.debug("Removed {0} from arguments".format(file))
                else:
                    logging.debug(f"{file} not a file or directory")

                if found_config:
                    break

            # File has no readable format for the configparser, probably because it's not a config file
            except configparser.MissingSectionHeaderError:
                logging.debug(f"{file} is not a valid config file")

        try:
            arguments.remove("htcsetup.conf")  # remove again, if other config file was found **
        except ValueError:
            pass
        print(back_to_default, end="")

        sys.argv[1:] = arguments

    # break if not found
    if not found_config:
        print(f"No valid config file given and no default "
              f"htcsetup.conf file found in: ({os.getcwd()}, ~/.config/htcompact/, /etc/) ")
        return False

    try:
        # all sections in the config file
        sections = config.sections()

        # now try filter the config file for all available parameters

        # all documents like files, etc.
        if 'documents' in sections:
            global files

            if 'files' in config['documents']:
                files = config['documents']['files'].split(" ")
                logging.debug(f"Changed document files to {files}")

        # all formats like the table format
        if 'formats' in sections:
            global table_format
            if 'table_format' in config['formats']:
                table_format = config['formats']['table_format']
                logging.debug(f"Changed default table_format to: {table_format}")

        # all basic HTCondor file endings like .log, .err, .out
        if 'htc-files' in sections:
            global std_log, std_err, std_out

            if 'stdlog' in config['htc-files']:
                std_log = config['htc-files']['stdlog']
                logging.debug(f"Changed default for HTCondor .log files to: {std_log}")

            if 'stderr' in config['htc-files']:
                std_err = config['htc-files']['stderr']
                logging.debug(f"Changed default for HTCondor .err files to: {std_err}")

            if 'stdout' in config['htc-files']:
                std_out = config['htc-files']['stdout']
                logging.debug(f"Changed default for HTCondor .out files to: {std_out}")

        # if show variables are set for further output
        if 'show-more' in sections:
            global show_output, show_warnings, show_errors  # sources to show

            if 'show_errors' in config['show-more']:
                show_errors = config['show-more']['show_errors'].lower() in accepted_states
                logging.debug(f"Changed default show_errors to: {show_errors}")

            if 'show_output' in config['show-more']:
                show_output = config['show-more']['show_output'].lower() in accepted_states
                logging.debug(f"Changed default show_output to: {show_output}")

            if 'show_warnings' in config['show-more']:
                show_warnings = config['show-more']['show_warnings'].lower() in accepted_states
                logging.debug(f"Changed default show_warnings to: {show_warnings}")

        # what information should to be ignored
        if 'ignore' in sections:
            global ignore_resources, ignore_job_information, ignore_allocated_resources  # sources to ignore

            if 'ignore_resources' in config['ignore']:
                ignore_resources = config['ignore']['ignore_resources'].lower() in accepted_states
                logging.debug(f"Changed default ignore_resources to: {ignore_resources}")
            if 'ignore_job_information' in config['ignore']:
                ignore_job_information = config['ignore']['ignore_job_information'].lower() in accepted_states
                logging.debug(f"Changed default ignore_job_information to: {ignore_job_information}")
            if 'ignore_allocated_resources' in config['ignore']:
                ignore_allocated_resources = config['ignore']['ignore_allocated_resources'].lower() \
                                             in accepted_states
                logging.debug(f"Changed default ignore_allocated_res to: {ignore_allocated_resources}")

        if 'thresholds' in sections:
            global low_usage, bad_usage
            if 'low_usage' in config['thresholds']:
                low_usage = float(config['thresholds']['low_usage'])
                logging.debug(f"Changed default low_usage to: {low_usage}")
            if 'bad_usage' in config['thresholds']:
                bad_usage = float(config['thresholds']['bad_usage'])
                logging.debug(f"Changed default bad_usage to: {bad_usage}")

        if 'csv' in sections:
            global to_csv, indexing
            if 'to_csv' in config['csv']:
                to_csv = config['csv']['to_csv'].lower() in accepted_states
                logging.debug(f"Changed default to_csv to: {to_csv}")
            if 'indexing' in config['csv']:
                indexing = config['csv']['indexing'].lower() in accepted_states
                logging.debug(f"Changed default indexing to: {indexing}")

        # Todo: reverse DNS-Lookup etc.
        if 'features' in sections:
            global resolve_ip_to_hostname, reverse_dns_lookup, summarise  # extra parameters
            if 'resolve_ip_to_hostname' in config['features']:
                resolve_ip_to_hostname = config['features']['resolve_ip_to_hostname'].lower() in accepted_states
                logging.debug(f"Changed default resolve_ip_to_hostname to: {resolve_ip_to_hostname}")
            if 'summarise' in config['features']:
                summarise = config['features']['summarise'].lower() in accepted_states
                logging.debug(f"Changed default summarise to: {summarise}")

        return True

    except KeyError as err:
        logging.exception(err)
        return False


# Todo: analyse a particular log file, with ALL kind of information, not just the defaults
def analyse():
    pass


# Todo: test with many files
def summarise_given_logs():
    """
    Summarises all used resources and the runtime in total and average

    Runs threw the log files via the log_to_dataframe function

    :return:
    """

    valid_files = len(files)
    # no given files
    if valid_files == 0:
        return "No files to summarise"

    # allocated all diffrent datatypes, easier to handle
    output_string = ""

    aborted_files = 0
    normal_runtime = datetime.timedelta()
    aborted_runtime = datetime.timedelta()
    cpu_nodes = dict()

    total_cpu_usage = float(0)
    total_disk_usage = int(0)
    total_memory_usage = int(0)

    total_cpu_requested = int(0)
    total_disk_requested = int(0)
    total_memory_requested = int(0)

    total_cpu_allocated = int(0)
    total_disk_allocated = int(0)
    total_memory_allocated = int(0)

    # if gpu given
    gpu_found = False
    total_gpu_usage = float(0)
    total_gpu_requested = int(0)
    total_gpu_allocated = int(0)
    list_of_gpu_names = list()

    percent_mark = 10  # calculate the percentage of the running summarisation
    start_status = 50  # value that decides, when it's worth showing a status of the progress

    for i, file in enumerate(files):
        try:
            log = log_to_dataframe(file)

            log_description, log_resources = log[0], log[1]

            # if the Job was aborted, it still might have a runtime
            if log_description.at[0, 'Description'].__eq__("Aborted by:"):
                # print(f"The job described in {file} was aborted")
                aborted_runtime += log_description.at[1, 'Values']
                aborted_files += 1
                continue

            normal_runtime += log_description.at[2, 'Values']
            cpu = log_description.at[0, 'Values']
            if cpu in cpu_nodes:
                cpu_nodes[cpu][0] += 1
                cpu_nodes[cpu][1] += log_description.at[2, 'Values']
            else:
                cpu_nodes[cpu] = [1, log_description.at[2, 'Values']]

            total_cpu_usage += float(log_resources.at[0, 'Usage']) \
                if str(log_resources.at[0, 'Usage']).lower() != 'nan' else 0
            total_disk_usage += int(log_resources.at[1, 'Usage'])
            total_memory_usage += int(log_resources.at[2, 'Usage'])

            total_cpu_requested += int(log_resources.at[0, 'Requested'])
            total_disk_requested += int(log_resources.at[1, 'Requested'])
            total_memory_requested += int(log_resources.at[2, 'Requested'])

            if not ignore_allocated_resources:
                total_cpu_allocated += int(log_resources.at[0, 'Allocated'])
                total_disk_allocated += int(log_resources.at[1, 'Allocated'])
                total_memory_allocated += int(log_resources.at[2, 'Allocated'])

            status = round(((i+1)/valid_files)*100)
            if int(status/percent_mark) > 0 and valid_files > start_status:
                percent_mark += 10
                print(f"Status: {status}% of all files summarised")

            if 3 in log_resources.index:  # this means gpu information is given
                gpu_found = True
                total_gpu_usage += float(log_resources.at[3, 'Usage'])
                total_gpu_requested += int(log_resources.at[3, 'Requested'])
                if not ignore_allocated_resources:
                    total_gpu_allocated += int(log_resources.at[3, 'Allocated'])
                if log_resources.at[3, 'Assigned'] not in list_of_gpu_names:  # append new gpus
                    list_of_gpu_names.append(log_resources.at[3, 'Assigned'])

        # Error ocurres when Job was aborted
        except ValueError or KeyError as err:
            logging.exception(err)
            print(f"Summarisation error with {file}")
            continue

    n = valid_files - aborted_files  # calc diffrenc of successful executed jobs

    average_runtime = normal_runtime / n if n != 0 else normal_runtime
    average_runtime = datetime.timedelta(days=average_runtime.days, seconds=average_runtime.seconds)
    total_runtime = normal_runtime + aborted_runtime

    output_string += "-"*75 + "\n"

    output_string += f"{valid_files} valid HTCondor job files found\n"
    # it is more obvious, to see None when there was no time to detect
    if aborted_files > 0:
        output_string += f"{aborted_files} of the jobs were aborted\n\n"

    time_dict = dict()

    if normal_runtime != datetime.timedelta(0, 0, 0):
        time_dict["Total runtime of all successful jobs"] = normal_runtime
    if aborted_runtime != datetime.timedelta(0, 0, 0):
        # some jobs were aborted, diffrentiate between normal runtime, aborted runtime and total runtime
        time_dict["Total runtime of all aborted jobs"] = aborted_runtime
        time_dict["Total runtime"] = total_runtime
    if average_runtime:
        time_dict["Average runtime per job\n(only successful executed jobs)"] = average_runtime

    runtime_df = pd.DataFrame(time_dict.items())
    output_string += tabulate(runtime_df, showindex=False, tablefmt='simple') + "\n\n"

    if n != 0:  # do nothing, if all valid jobs were aborted
        df_total = pd.DataFrame({
            "Resources": ['Total Cpu', 'Total Disk (KB)', 'Total Memory (MB)'],
            "Usage": [str(round(total_cpu_usage, 2)),
                      str(total_disk_usage),
                      str(total_memory_usage)],  # necessary
            "Requested": [total_cpu_requested,
                          total_disk_requested,
                          total_memory_requested]

        })
        df_average = pd.DataFrame({
            "Resources": ['Average Cpu', 'Avergae Disk (KB)', 'Average Memory (MB)'],
            "Usage": [round(total_cpu_usage / n, 2),
                      round(total_disk_usage / n, 2),
                      round(total_memory_usage / n, 2)],  # necessary
            "Requested": [round(total_cpu_requested / n, 2),
                          round(total_disk_requested / n, 2),
                          round(total_memory_requested / n, 2)]

        })
        insert_index = 3  # for the row at the right place

        if not ignore_allocated_resources:
            df_total.insert(insert_index, "Allocated", [total_cpu_allocated,
                                             total_disk_allocated,
                                             total_memory_allocated])
            df_average.insert(insert_index, "Allocated", [round(total_cpu_allocated / n, 2),
                                               round(total_disk_allocated / n, 2),
                                               round(total_memory_allocated / n, 2)])

        # if gpu information was found
        if gpu_found:
            df_gpu_total = pd.DataFrame({
                "Resources": ['Total Gpu'],
                "Usage": [total_gpu_usage],  # necessary
                "Requested": [total_gpu_requested]
            })
            df_gpu_average = pd.DataFrame({
                "Resources": ['Average Gpu'],
                "Usage": [round(total_gpu_usage / n, 2)],  # Todo: divide by gpu occurrence ????
                "Requested": [round(total_gpu_requested / n, 2)]
            })
            if not ignore_allocated_resources:
                df_gpu_total.insert(insert_index, "Allocated", [total_gpu_allocated])
                df_gpu_average.insert(insert_index, "Allocated", [round(total_gpu_allocated / n, 2)])
                insert_index += 1

            df_total = df_total.append(df_gpu_total)
            df_average = df_average.append(df_gpu_average)

            df_total.insert(insert_index, "Assigned", ["", "", "", ", ".join(list_of_gpu_names)])
            df_average.insert(insert_index, "Assigned", ["", "", "", ", ".join(list_of_gpu_names)])
            insert_index += 1

        # Todo: add later
        #output_string += "Total used resources:\n"
        #output_string += tabulate(df_total, showindex=False, headers='keys', tablefmt=table_format) + "\n\n"

        output_string += "The following data only applies to successful executed jobs\n\n"
        output_string += "Used resources in average:\n"
        output_string += tabulate(df_average, showindex=False, headers='keys', tablefmt=table_format) + "\n\n"

    # Todo: cpu nodes might change over lifetime of a job
    if len(cpu_nodes) > 0:

        executed_jobs = list()
        runtime_per_node = list()
        for val in cpu_nodes.values():
            executed_jobs.append(val[0])
            average_job_duration = val[1]/val[0]
            runtime_per_node.append(datetime.timedelta(average_job_duration.days,average_job_duration.seconds))

        df_cpu = pd.DataFrame({
            "Cpu Nodes": list(cpu_nodes.keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        })
        output_string += tabulate(df_cpu, showindex=False, headers='keys', tablefmt=table_format) + "\n"
    output_string += "-"*75
    return output_string


# Todo: how many valid files and how many not valid and what type etc JUST EVERYTHING
def validate_given_logs(file_list):
    """
    This method is supposed to take the given log files (by config or command line argument)
    and tries to determine if these are valid log files, ignoring the std_log specification, if std_log = "".

    It will run through given directories and check every single file, if accessible, for the
    HTCondor log file standard, valid log files should start with lines like:

    000 (5989.000.000) 03/30 15:48:47 Job submitted from host: <10.0.8.10:9618?addrs=10.0.8.10-9618&noUDP&sock=3775629_0774_3>

    if done, it will change the global files list and store only valid log files.
    The user will be remind, what files were accepted as "valid".

    The user will also be informed if a given file was not found.

    Todo: In addition there should be an option --force, that makes the script stop, if the file was not found or marked as valid

    Todo: user should be able to get the option to decide, when the same files appear more than once
    -> my guess: yes or no question, if nothing is given in under 10 seconds, it should go with no
    -> this should prevent, that the script is stucked, if the user is for example running it over night

    """
    valid_files = list()

    for arg in file_list:

        path = os.getcwd()  # current working directory , should be condor job summariser script
        logs_path = path + "/" + arg  # absolute path

        working_dir_path = ""
        working_file_path = ""

        if os.path.isdir(arg):
            working_dir_path = arg
        elif os.path.isdir(logs_path):
            working_dir_path = logs_path

        elif os.path.isfile(arg):
            working_file_path = arg
        elif os.path.isfile(logs_path):
            working_file_path = logs_path
        # check if only the id was given and resolve it with the std_log specification
        elif os.path.isfile(arg+std_log):
            working_file_path = arg+std_log
        elif os.path.isfile(logs_path+std_log):
            working_file_path = logs_path+std_log

        # if path is a directory
        if working_dir_path.__ne__(""):
            # run through all files and separate the files into log/error and output files
            for file in os.listdir(working_dir_path):
                # ignore hidden files
                if file.startswith("."):
                    continue
                # if it's not a sub folder
                file_path = working_dir_path+"/"+file

                if os.path.isfile(file_path):
                    with open(file_path, "r") as read_file:
                        # Todo: better specification with re
                        if os.path.getsize(file_path) == 0:  # file is empty
                            continue

                        if re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)", read_file.readlines()[0]):
                            #logging.debug(f"{read_file.name} is a valid HTCondor log file")
                            valid_files.append(file_path)

                else:
                    logging.debug(f"Found a subfolder: {working_dir_path}/{file}, it will be ignored")
                    print(yellow+f"Found a subfolder: {working_dir_path}/{file}, it will be ignored"+back_to_default)

        # else if path "might" be a valid HTCondor file
        elif working_file_path.__ne__(""):
            with open(working_file_path, "r") as read_file:

                if os.path.getsize(working_file_path) == 0:  # file is empty
                    print(red+"How dare you, you gave me an empty file :("+back_to_default)

                elif re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)", read_file.readlines()[0]):
                    logging.debug(f"{read_file.name} is a valid HTCondor log file")
                    valid_files.append(working_file_path)
                else:
                    logging.debug(f"The given file {read_file.name} is not a valid HTCondor log file")
                    print(yellow+f"The given file {read_file.name} is not a valid HTCondor log file"+back_to_default)
        else:
            logging.error(f"The given file: {arg} does not exist")
            print(red+f"The given file: {arg} does not exist"+back_to_default)

    return valid_files


def main():
    """
    This is the main function, which searchs first for a given config file.
    After that, given parameters in the terminal will be interpreted, so they have a higher priority

    Then it will summarise all given log files together and print the output

    :return:
    """
    try:

        # interpret the first file, that can be interpreted as a config file and remove it, other given config files,
        # (you will see when you try, files will be interpreted as HTCondor log files)
        # so it's not possible to give multiple config files
        load_config()

        # after that, interpret the command line arguments, these will have a higher priority, than the config setup
        # and by that arguments set in the config file, will be ignored
        manage_params()

        global files
        files = validate_given_logs(files)  # validate the files, make a list of all valid config files

        if summarise:
            output_string = summarise_given_logs()  # summarise information
        else:
            output_string = output_given_logs()  # print out all given files if possible

        print(output_string)  # write it to the console

    except KeyboardInterrupt:
        print("Script was interrupted")


logging.debug("-------Start of htcompact scipt-------")
main()
logging.debug("-------End of htcompact script-------")
