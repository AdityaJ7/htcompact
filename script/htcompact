#!/usr/bin/env python3
import re
import sys
import os
import getopt
import datetime
import logging

import socket
import configparser
import pandas as pd
import numpy as np
from rich.progress import track, Progress
from tabulate import tabulate
from plotille import Figure

"""

This script is basically reading information from HTCondor log files and storing them into
pandas DataFrames. By that its actually pretty simple to analyse the data.

The script makes the information compact and easy to under stand,
that's the reason for the name htcompact

Single logs can be read quite easily,
but also it's possible to summarize a whole directory with logs
to see for ex. the average runtime and usage of all the logs

"""

# Todo: logging should be declared in setup and command line arguments

# global parameters, used for dynamical output of information
accepted_states = ["true", "yes", "y", "ja", "j", "enable", "enabled", "wahr", "0"]
files = list()
border_str = ""
option_shorts = "hsadv"
option_longs = ["help", "version",
                "std-log=", "std-err=", "std-out=",
                "show-errors", "show-output", "show-warnings",
                "ignore=", "no-config", "to-csv",
                "summarize", "analyse", "generate-log-file",
                "filter=", "extend", "print-events-table",
                "reverse-dns-lookup", "table-format="]

# variables for given parameters
show_std_output = False
show_std_warnings = False
show_std_errors = False

# ignore infromation
ignore_list = list()
allowed_ignore_values = ["execution-details", "times",
                         "gpus", "cpu"
                         "used-resources", "requested-resources", "allocated-resources", "all-resources",
                         "errors", "warnings"]
# Todo: used-resources, requested-resources, times, gpus, cpu, errors, warnings]

# if set do not use a config file, even if one was found
no_config = False

# Features:
reverse_dns_lookup = False
store_dns_lookups = dict()
summarizer_mode = False
analyser_mode = False
to_csv = False
# logging tool
generate_log_file = False

filter_mode = False
filter_keywords = ""
filter_extended = False
default_mode = False  # just for the search function to determine if default output should be printed

# escape sequences for colors
red = "\033[0;31m"
green = "\033[0;32m"
yellow = "\033[0;33m"
magenta = "\033[0;35m"
cyan = "\033[0;36m"
blue = "\033[0;34m"
lightgrey = "\033[37m"
back_to_default = "\033[0;39m"

# thresholds for bad and low usage of resources
low_usage = 0.75
bad_usage = 1.2

# global variables with default values for err/log/out files
std_log = ""
std_err = ""
std_out = ""

# global defaults
default_configfile = "htcompact.conf"
table_format = "pretty"  # ascii by default

# Todo: adjust this method to accept all values,
#  that are related to an option and interpret
#  the first one which does not as a file again
#  -> something like --ignore allocated-resources times ... should be possible
# Todo: consider regex in this implementation: --ignore alloc tim ...
#  with focus on longest prefix match
# may not be needed in future but will be used for now
def remove_files_from_args(args, short_opts, long_opts):
    """
        I want to make it easier for the user to insert many files at once.
        The script should detect option arguments and files

        It will filter the given files and remove them from the argument list

        !!!Exactly that is this method doing!!! (because getopt has no such function)

    :param args: is supposed to be a list, in any case it should be the sys.argv[1:] list
    :param short_opts: the short getopt options
    :param long_opts: the long getopt options
    :return: list of getopt arguments
    """
    new_args = args.copy()
    generate_files = list()
    index = 0
    # run through the command line arguments,
    # skip the getopt argument and interpret everything else as a file
    while True:
        if index >= len(args):
            break

        arg = args[index]

        if arg.startswith("-"):  # for short_args
            arg = arg[1:]
            if arg.startswith("-"):  # for long_args
                arg = arg[1:]

            # skip the argument if getopt arguments are setable
            if arg+":" in short_opts:
                index += 1
            elif arg+"=" in long_opts:
                index += 1
        else:
            generate_files.append(arg)
            new_args.remove(arg)

        index += 1

    # change files, if found
    if len(generate_files) > 0:
        global files
        files = generate_files

    # return valid arguments
    return new_args


def manage_prioritized_params():

    global option_shorts, option_longs
    global no_config, generate_log_file

    # if for whatever reasons files were given
    better_args = remove_files_from_args(sys.argv[1:], option_shorts, option_longs)

    try:
        opts, args = getopt.getopt(better_args, option_shorts, option_longs)

        for opt, arg in opts:

            if opt in ["-h", "--help"]:
                man_help()
                sys.exit(0)
            if opt in ["-v", "--version"]:
                # Todo:
                sys.exit(0)
            elif opt in ["--print-events-table"]:
                print(get_event_information())
                sys.exit(0)
            elif opt.__eq__("--no-config"):
                no_config = True
            elif opt.__eq__("--generate-log-file"):
                generate_log_file = True
                logging.getLogger().disabled = False

    # print error messages
    except Exception as err:
        print((red + "{0}: {1}" + back_to_default).format(err.__class__.__name__, err))
        print(small_help())
        sys.exit(1)


# Todo: thresholds by command line
def manage_params():
    """
    Interprets the given command line arguments and changes the global variables in this scrips

    """
    global files  # list of files and directories
    global std_log, std_err, std_out  # all default values for the HTCondor files
    global show_std_output, show_std_warnings, show_std_errors  # show more
    global ignore_list  # ignore information variables
    global to_csv, summarizer_mode, analyser_mode, default_mode  # features
    global filter_mode, filter_keywords, filter_extended  # search features
    global table_format  # table_format can be changed
    global reverse_dns_lookup  # if set host ip's will be looked up in dns server

    global option_shorts, option_longs

    all_args = sys.argv[1:]

    # listen to stdin and add these files
    if not sys.stdin.isatty():  # else the script will be waiting fro stdin
        logging.debug("Listening to arguments from stdin")
        for line in sys.stdin:
            all_args.append(line.rstrip('\n'))

    better_args = remove_files_from_args(all_args, option_shorts, option_longs)

    try:
        opts, args = getopt.getopt(better_args, option_shorts, option_longs)

        for opt, arg in opts:

            # catch unusual but not wrong parameters starting with -
            if arg.startswith("-"):
                print(yellow+"The argument for {0} is {1}, is that wanted?".format(opt, arg)+back_to_default)
                logging.warning("The argument for {0} is {1}, is that wanted?".format(opt, arg))

            elif opt in ["-s", "--summarize"]:
                summarizer_mode = True
                logging.debug("Summariser mode turned on")
            elif opt in ["-a", "--analyse"]:
                analyser_mode = True
                logging.debug("Analyser mode turned on")
            elif opt in ["-d", "--default"]:
                default_mode = True
            elif opt in ["--filter"]:
                filter_mode = True
                filter_keywords = arg
                logging.debug(f"Filter mode turned on")
            elif opt in ["--extend"]:
                filter_extended = True
            # all HTCondor files, given by the user if they are not saved in .log/.err/.out files
            elif opt == "--std-log":
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_log = arg
            elif opt == "--std-err":
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_err = arg
            elif opt == "--std-out":
                # to forget the . should not be painful
                if arg[0] != '.':
                    arg = "."+arg
                std_out = arg
            # all variables, to show more specific information
            elif opt.__eq__("--show-output"):
                show_std_output = True
            elif opt.__eq__("--show-warnings"):
                show_std_warnings = True
            elif opt.__eq__("--show-errors"):
                show_std_errors = True

            # all variables to ignore unwanted information
            elif opt.__eq__("--ignore"):
                for arg_s in arg.split():
                    if arg_s in allowed_ignore_values:
                        ignore_list.append(arg_s)
                    else:
                        raise_value_error(" Invalid argument '" + arg_s + "'\n"
                                          "Valid arguments:\n" + ", ".join(allowed_ignore_values))
                logging.debug("Ignore these information: " + ", ".join(ignore_list))


            # all tabulate variables
            elif opt.__eq__("--to-csv"):
                to_csv = True

            elif opt.__eq__("--table-format"):
                types = "plain,simple,github,grid,fancy_grid,pipe," \
                        "orgtbl,rst,mediawiki,html,latex,latex_raw," \
                        "latex_booktabs,tsv,pretty"
                # only valid arguments
                if arg in types.split(","):
                    table_format = arg
                else:
                    logging.debug("The given table format doesn't exist")

            elif opt.__eq__("--reverse-dns-lookup"):
                reverse_dns_lookup = True

            # these are already managed in manage_prioritized_params
            # need to be caught, cause of exception cases
            elif opt.__eq__("--no-config"):
                pass
            elif opt.__eq__("--generate-log-file"):
                pass

            else:
                print(f"{red}Option not handled yet{back_to_default}")
                print(small_help())
                sys.exit(0)
    # print error messages
    except Exception as err:
        logging.exception(err)  # write a detailed description in the stdout.log file
        print((red+"{0}: {1}"+back_to_default).format(err.__class__.__name__, err))
        print(small_help())
        sys.exit(1)

    if len(files) == 0:
        logging.debug("No files given")
        print(red+"No files given"+back_to_default)
        print(small_help())
        sys.exit(2)


def small_help():
    """
Usage: htcompact (log_files|config_file) [ Arguments ]

----------------------------Main features:---------------------------------

 -s | --summarize
         summarizes all given HTCondor log files and return a result
         regarding the averange usages and runtimes, this only makes
         sense, if multiple log files are given

 -a | --analyse
         analyses a specific file, for occurred errors, ram history ->
         histogram, execution status, runtime and much more

 --search keywords
         search inside the given files or directories
         for the given keywords,
         if you want to give more than one keyword,
         use the string notation "keyword1 keyword2 ...",


More detailed descriptions and help on other options with:
 "man htcompact" or "htcompact -h"
    """
    # returns this docstring
    return small_help.__doc__


def man_help():

    check_places = ['/share/man/man1/htcompact.1', '/env/share/man/man1/htcompact.1',
                    '/man/man1/htcompact.1', '/../man/man1/htcompact.1']
    cwd = os.getcwd()
    for place in check_places:
        path = cwd+place
        # check current working directory
        if os.path.isfile(path):
            os.system(f"groff -Tlatin1 -mandoc {path}")
            break
        # check environment
        path = sys.prefix + place
        if os.path.isfile(path):
            os.system(f"groff -Tlatin1 -mandoc {path}")
            break


def get_event_information(event_id=""):
    """
Event Number: 000
Event Name: Job submitted
Event Description: This event occurs when a user submits a job. It is the first event you will see for a job, and it should only occur once.

Event Number: 001
Event Name: Job executing
Event Description: This shows up when a job is running. It might occur more than once.

Event Number: 002
Event Name: Error in executable
Event Description: The job could not be run because the executable was bad.

Event Number: 003
Event Name: Job was checkpointed
Event Description: The job's complete state was written to a checkpoint file. This might happen without the job being removed from a machine, because the checkpointing can happen periodically.

Event Number: 004
Event Name: Job evicted from machine
Event Description: A job was removed from a machine before it finished, usually for a policy reason. Perhaps an interactive user has claimed the computer, or perhaps another job is higher priority.

Event Number: 005
Event Name: Job terminated
Event Description: The job has completed.

Event Number: 006
Event Name: Image size of job updated
Event Description: An informational event, to update the amount of memory that the job is using while running. It does not reflect the state of the job.

Event Number: 007
Event Name: Shadow exception
Event Description: The condor_shadow, a program on the submit computer that watches over the job and performs some services for the job, failed for some catastrophic reason. The job will leave the machine and go back into the queue.

Event Number: 008
Event Name: Generic log event
Event Description: Not used.

Event Number: 009
Event Name: Job aborted
Event Description: The user canceled the job.

Event Number: 010
Event Name: Job was suspended
Event Description: The job is still on the computer, but it is no longer executing. This is usually for a policy reason, such as an interactive user using the computer.

Event Number: 011
Event Name: Job was unsuspended
Event Description: The job has resumed execution, after being suspended earlier.

Event Number: 012
Event Name: Job was held
Event Description: The job has transitioned to the hold state. This might happen if the user applies the condor_hold command to the job.

Event Number: 013
Event Name: Job was released
Event Description: The job was in the hold state and is to be re-run.

Event Number: 014
Event Name: Parallel node executed
Event Description: A parallel universe program is running on a node.

Event Number: 015
Event Name: Parallel node terminated
Event Description: A parallel universe program has completed on a node.

Event Number: 016
Event Name: POST script terminated
Event Description: A node in a DAGMan work flow has a script that should be run after a job. The script is run on the submit host. This event signals that the post script has completed.

Event Number: 017
Event Name: Job submitted to Globus
Event Description: A grid job has been delegated to Globus (version 2, 3, or 4). This event is no longer used.

Event Number: 018
Event Name: Globus submit failed
Event Description: The attempt to delegate a job to Globus failed.

Event Number: 019
Event Name: Globus resource up
Event Description: The Globus resource that a job wants to run on was unavailable, but is now available. This event is no longer used.

Event Number: 020
Event Name: Detected Down Globus Resource
Event Description: The Globus resource that a job wants to run on has become unavailable. This event is no longer used.

Event Number: 021
Event Name: Remote error
Event Description: The condor_starter (which monitors the job on the execution machine) has failed.

Event Number: 022
Event Name: Remote system call socket lost
Event Description: The condor_shadow and condor_starter (which communicate while the job runs) have lost contact.

Event Number: 023
Event Name: Remote system call socket reestablished
Event Description: The condor_shadow and condor_starter (which communicate while the job runs) have been able to resume contact before the job lease expired.

Event Number: 024
Event Name: Remote system call reconnect failure
Event Description: The condor_shadow and condor_starter (which communicate while the job runs) were unable to resume contact before the job lease expired.

Event Number: 025
Event Name: Grid Resource Back Up
Event Description: A grid resource that was previously unavailable is now available.

Event Number: 026
Event Name: Detected Down Grid Resource
Event Description: The grid resource that a job is to run on is unavailable.

Event Number: 027
Event Name: Job submitted to grid resource
Event Description: A job has been submitted, and is under the auspices of the grid resource.

Event Number: 028
Event Name: Job ad information event triggered.
Event Description: Extra job ClassAd attributes are noted. This event is written as a supplement to other events when the configuration parameter EVENT_LOG_JOB_AD_INFORMATION_ATTRS is set.

Event Number: 029
Event Name: The job's remote status is unknown
Event Description: No updates of the job's remote status have been received for 15 minutes.

Event Number: 030
Event Name: The job's remote status is known again
Event Description: An update has been received for a job whose remote status was previous logged as unknown.

Event Number: 031
Event Name: Job stage in
Event Description: A grid universe job is doing the stage in of input files.

Event Number: 032
Event Name: Job stage out
Event Description: A grid universe job is doing the stage out of output files.

Event Number: 033
Event Name: Job ClassAd attribute update
Event Description: A Job ClassAd attribute is changed due to action by the condor_schedd daemon. This includes changes by condor_prio.

Event Number: 034
Event Name: Pre Skip event
Event Description: For DAGMan, this event is logged if a PRE SCRIPT exits with the defined PRE_SKIP value in the DAG input file. This makes it possible for DAGMan to do recovery in a workflow that has such an event, as it would otherwise not have any event for the DAGMan node to which the script belongs, and in recovery, DAGMan's internal tables would become corrupted.
"""
    if event_id == "":
        return get_event_information.__doc__
    doc_str = get_event_information.__doc__.split('\n')
    for i, line in enumerate(doc_str):
        match = re.match(r"Event Number: ([0-9]{3})", line)
        if match:
            if int(match[1]) == int(event_id):
                return doc_str[i:i+3]
    # else
    return ["This event number does not exist", "Valid event numbers range from 000 to 034"]


# reads all information, but returns them in two lists
def read_condor_log(file):
    """

    reads a given HTCondor std_log file and separates the information in two lists,
    for further and easier access

    :param file:    a HTCondor std_log file

    :raises: :class:'FileNotFoundError': if open does not work

    :return:    (job_event, job_event_information)
                a list of job_events and a list of job_event-relevant information
                these are related by the same index like:
                job_event[1]-> relevant information: job_event_information[1]
    """

    log_job_events = list()  # saves the job_events
    job_event_information = list()  # saves the information for each job event, if there are any
    temp_job_event_list = list()  # just a temporary list, gets inserted into job_event_information

    with open(file) as log_file:
        # for every line in the log_file
        for line in log_file:

            line = line.rstrip("\n")  # remove newlines
            match_log\
                = re.match(r"([0-9]{3})"  # matches event number
                           r" (\([0-9]+.[0-9]+.[0-9]{3}\))"  # matches clusterid and process id
                           r" ([0-9]{2}/[0-9]{2})"  # matches date
                           r" ([0-9]{2}:[0-9]{2}:[0-9]{2})"  # matches time
                           r"((?: \w+)*)."  # matches the job event name
                           r"(.*)", line)  # matches further information
            if match_log:
                job_event_number = match_log[1]  # job event number, can be found in job_description.txt
                clusterid_procid_inf = match_log[2]  # same numbers, that make the name of the file
                date = match_log[3]  # filter the date in the form Month/Day in numbers
                time = match_log[4]  # filter the time
                job_event_name = match_log[5]  # filter the job event name
                job_relevant_inf = match_log[6]  # filter the job relevant information
                log_job_events.append([job_event_number, clusterid_procid_inf,
                                       date, time, job_event_name, job_relevant_inf])
                # print(job_event_number, clusterid_procid_inf, date, time, job_event_name, job_relevant_inf)
            else:
                # end of job event
                if "..." in line:
                    job_event_information.append(temp_job_event_list)
                    temp_job_event_list = list()  # clear list
                    continue
                # else
                #if re.match("[\t ]+", line):
                else:
                    temp_job_event_list.append(line)
    return log_job_events, job_event_information


def raise_value_error(message):
    raise ValueError(message)


# Todo: time seperated ( submitting / executing / terminating)
# Todo: Read HTCondor Errors that occurred
# Todo: for loop instead of single run
# Todo: termination types are dynamical
def log_to_dataframe(file):
    try:
        job_events, job_raw_information = read_condor_log(file)

        # submitted_date = datetime.datetime.strptime(job_events[0][2] + " " + job_events[0][3], "%m/%d %H:%M:%S")

        if job_events[-1][0].__eq__("005"):  # if the last job event is : Job terminated

            # calculate the runtime for the job
            executed_date = datetime.datetime.strptime(job_events[1][2] + " " + job_events[1][3], "%m/%d %H:%M:%S")
            terminating_date = datetime.datetime.strptime(job_events[-1][2] + " " + job_events[-1][3], "%m/%d %H:%M:%S")
            runtime = terminating_date - executed_date  # calculation of the time runtime

            match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job_events[1][5])
            if match_host:
                host = match_host[1]
                # if reverse dns lookup, change the ip to cpu: last number
                if reverse_dns_lookup:
                    host = gethostbyaddr(host)
                port = match_host[2]
            else:
                logging.exception("Host and port haven't been matched correctly")
                print(
                    red + "your log file has faulty values for the host ip make sure it's a valid IPv4" + back_to_default)

            # make a fancy design for the job_information
            job_labels = ["Executing on Host", "Port", "Runtime"]  # holds the labels
            job_information = [host, port, runtime]  # holds the related job information

            # filter the termination state ...
            if True:
                # check if termination state is normal
                termination_state_inf = job_raw_information[-1][0]
                match_termination_state = re.match(r"[\t ]+\([0-9]+\) ((?:\w+ )*)", termination_state_inf)
                if match_termination_state:
                    job_labels.append("Termination State")

                    if "Normal termination" in match_termination_state[1]:
                        job_information.append(match_termination_state[1])
                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", termination_state_inf)
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)
                    elif "Abnormal termination" in match_termination_state[1]:
                        job_information.append(red+match_termination_state[1]+back_to_default)

                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", termination_state_inf)
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)

                        termination_desc = list()
                        # append the reason for the Abnormal Termination
                        for desc in job_raw_information[-1][1:]:
                            if re.match(r"[\t ]+\(0\)", desc):
                                termination_desc.append(desc[5:])
                            else:
                                break

                        if not len(termination_desc) == 0:
                            job_labels.append("Description")
                            job_information.append("\n".join(termination_desc))
                        else:
                            logging.debug(f"No termination description for {file} found")

                    else:
                        job_information.append(match_termination_state[1])
                        logging.debug(f"Don't know this Termination State: {match_termination_state[1]} yet")

                else:
                    print(red + f"Termination error in HTCondor log file: {file}" + back_to_default)

            # now put everything together in a table
            job_df = pd.DataFrame({
                "Description": job_labels,
                "Values": job_information
            })

            # these where the job information now focus on the used resources

            relevant_str = "\n".join(job_raw_information[-1])  # job information of the last job (Job terminated)

            # next part removes not useful lines
            if True:  # just for readability
                # remove unnecessary lines
                lines = relevant_str.splitlines()
                # while not lines[0].startswith("\tPartitionable"):
                while not re.match(r"[\t ]+Partitionable", lines[0]):
                    lines.remove(lines[0])

                lines.remove(lines[0])
                partitionable_res = lines
                # done, partitionable_resources contain now only information about used resources

            # match all resources
            # ****************************************************************************************************
            match = re.match(r"[\t ]+Cpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+)", partitionable_res[0])
            if match:
                cpu_usage, cpu_request, cpu_allocated = match[1], match[2], match[3]
            else:
                raise_value_error("Something went wrong reading the cpu information")

            match = re.match(r"[\t ]+Disk \(KB\) *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[1])
            if match:
                disk_usage, disk_request, disk_allocated = match[1], match[2], match[3]
            else:
                raise raise_value_error("Something went wrong reading the disk information")

            # check if the log has gpu information in between
            gpu_match_index = 2  # can be ignored if gpu is given memory has to access the next index
            gpu_found = False  # easier to check to add information in the DataFrame
            match = re.match(r"[\t ]+Gpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+) \"(.*)\"",
                             partitionable_res[gpu_match_index])
            if match:
                gpu_usage, gpu_request, gpu_allocated, gpu_name = match[1], match[2], match[3], match[4]
                gpu_match_index += 1
                gpu_found = True

            match = re.match(r"[\t ]+Memory \(MB\)  *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[gpu_match_index])
            if match:
                memory_usage, memory_request, memory_allocated = match[1], match[2], match[3]
            else:
                raise_value_error("Something went wrong reading the memory information")
            # ****************************************************************************************************

            # list of resources and their labels
            resource_labels = ["Cpu", "Disk", "Memory"]
            usage = [cpu_usage, disk_usage, memory_usage]
            requested = [cpu_request, disk_request, memory_request]
            allocated = [cpu_allocated, disk_allocated, memory_allocated]

            if gpu_found:
                resource_labels.append("Gpus")
                usage.append(gpu_usage)
                requested.append(gpu_request)
                allocated.append(gpu_allocated)

            # Error handling: change empty values to NaN in the first column
            for i in range(3):
                if usage[i] == "":
                    usage[i] = np.nan
                if requested[i] == "":
                    requested[i] = np.nan
                if allocated[i] == "":
                    allocated[i] = np.nan

            # put the data in the DataFrame
            res_df = pd.DataFrame({
                "Rescources": resource_labels,
                "Usage": usage,
                "Requested": requested,
                # "Allocated": allocated
            })

            res_df.insert(3, "Allocated", allocated)
            if gpu_found:
                res_df.insert(4, "Assigned", ["", "", "", gpu_name])

        elif job_events[-1][0].__eq__("009"):  # job aborted

            # calculate the runtime for the job
            if job_events[-1].__ne__(job_events[0]):
                executed_date = datetime.datetime.strptime(job_events[1][2] + " " + job_events[1][3], "%m/%d %H:%M:%S")
                terminating_date = datetime.datetime.strptime(job_events[-1][2] + " " + job_events[-1][3], "%m/%d %H:%M:%S")
                runtime = terminating_date - executed_date  # calculation of the time runtime
            else:
                runtime = datetime.timedelta(0)

            user = ((job_raw_information[-1][0]).split(" ")[-1])[:-1]
            job_df = pd.DataFrame({
                "Description": ['Aborted by:', 'Runtime'],
                "Values": [user, runtime]
            })
            res_df = None
            logging.debug(f"{file}: Job arboted by {user}")

    except NameError as err:
        logging.exception(err)
        print("The smart_output_logs method requires a " + std_log + " file as parameter")
    except FileNotFoundError or ValueError as err:
        logging.exception(err)
        print(red + str(err) + back_to_default)
    except IndexError as err:
        logging.debug(f"Index Error with {file}: ")
        logging.exception(err)
    except Exception as err:
        logging.debug(err)
    # finally
    else:
        return job_df, res_df


def smart_output_logs(file):
    """
    reads a given HTCondor .log file with the read_condor_logs() function

    :param file:    a HTCondor .log file
    :param header:  Shows the header of the columns
    :param index:   Shows the index of the rows

    :return:        (output_string)
                    an output string that shows information like:
                    The job procedure of : ../logs/454_199.log
                    +-------------------+--------------------+
                    | Executing on Host |      10.0.9.1      |
                    |       Port        |        9618        |
                    |      Runtime      |      1:12:20       |
                    | Termination State | Normal termination |
                    |   Return Value    |         0          |
                    +-------------------+--------------------+
                    +--------+-------+-----------+-----------+
                    |        | Usage | Requested | Allocated |
                    +--------+-------+-----------+-----------+
                    |  Cpu   | 0.30  |     1     |     1     |
                    |  Disk  |  200  |    200    |  3770656  |
                    | Memory |   3   |     1     |    128    |
                    +--------+-------+-----------+-----------+

    """
    try:

        job_df, res_df = log_to_dataframe(file)

        output_string = ""

        # Todo: csv style one line
        if to_csv:
            pass
        else:
            global border_str
            output_string += green + "The job procedure of : " + file + back_to_default + "\n"
            border_str = "-" * len(output_string) + "\n"

            if not isinstance(res_df, type(None)):  # make sure res_df is not None

                # reuested vs used disk RAM
                if float(res_df.iloc[1, 1]) / float(res_df.iloc[1, 2]) > bad_usage:
                    res_df.iloc[1, 1] = red+res_df.iloc[1, 1]+back_to_default  # changed color to red
                elif float(res_df.iloc[1, 1]) / float(res_df.iloc[1, 2]) < low_usage:
                    res_df.iloc[1, 1] = yellow+res_df.iloc[1, 1]+back_to_default  # changed color to yellow

                # requested vs used Memory
                if float(res_df.iloc[2, 1]) / float(res_df.iloc[2, 2]) > bad_usage:
                    res_df.iloc[2, 1] = red+res_df.iloc[2, 1]+back_to_default  # changed color to red
                elif float(res_df.iloc[2, 1]) / float(res_df.iloc[2, 2]) < low_usage:
                    res_df.iloc[2, 1] = yellow+res_df.iloc[2, 1]+back_to_default  # changed color to yellow

            if not "execution-details" in ignore_list:
                output_string += tabulate(job_df, tablefmt=table_format, showindex=False) + "\n"
            if not "all-resources" in ignore_list:
                output_string += tabulate(res_df, headers='keys', tablefmt=table_format, showindex=False) + "\n"

    except NameError as err:
        logging.exception(err)
        print("The smart_output_logs method requires a "+std_log+" file as parameter")
    except Exception as err:
        logging.exception(err)
        print(red+str(err)+back_to_default)
    # finally
    else:
        return output_string


# Todo: maybe less information
# just read .err files content and return it as a string
def read_condor_error(file):
    """
    Reads a HTCondor .err file and returns its content

    :param file: a HTCondor .err file

    :raises: :class:'NameError': The param file was no .err file
             :class:'FileNotFoundError': if open does not work

    :return: file content

    """
    if not file.endswith(std_err):
        raise NameError("The read_condor_error method is only for "+std_err+" files")

    err_file = open(file)
    return "".join(err_file.readlines())


# just read .out files content and return it as a string
def read_condor_output(file):
    """
        Reads a HTCondor .out file and returns its content

        :param file: a HTCondor .out file

        :raises: :class:'NameError': The param file was no .out file
                 :class:'FileNotFoundError': if open does not work

        :return: file content

        """
    if not file.endswith(std_out):
        raise NameError("The read_condor_output method is only for "+std_out+" files")

    out_file = open(file)
    return "".join(out_file.readlines())


def gethostbyaddr(ip):
    """
        this function is supposed to filter a given ip for it's representive domain name like google.com
        :return: resolved domain name, else give back the ip
    """
    try:
        if ip in list(store_dns_lookups.keys()):
            return store_dns_lookups[ip]
        # else lookup
        reversed_dns = socket.gethostbyaddr(ip)
        logging.debug('Lookup sucessful ' + ip + ' resolved as: ' + reversed_dns[0])
        # store
        store_dns_lookups[ip] = reversed_dns[0]
        # return
        return reversed_dns[0]
    except Exception:
        logging.debug('Not able to resolve the IP: '+ip)
        # also store
        store_dns_lookups[ip] = ip
        return ip


def smart_output_error(file):
    """

    :param file: a HTCondor .err file
    :return: the content of the given file as a string
    """

    # Todo:
    # - errors from htcondor are more important (std.err output)
    # - are there errors ? true or false -> maybe print those in any kind of fromat
    # - maybe check for file size !!!
    # - is the file size the same ? what is normal / different errors (feature -> for  -> later)

    output_string = ""
    try:
        error_content = read_condor_error(file)
        for line in error_content.split("\n"):
            if "err" in line.lower() and show_std_errors:
                output_string += red + line + back_to_default + "\n"
            elif "warn" in line.lower() and show_std_warnings:
                output_string += yellow + line + back_to_default + "\n"

    except NameError as err:
        logging.exception(err)
        print("The smart_output_error method requires a "+std_err+" file as parameter")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)"+std_err, relevant[1])
        print(yellow + "There is no related {0} file: {1} in the directory:{2}\n'{3}'\n"
              " with the prefix: {4}{5}"
              .format(std_err, relevant[1], cyan, os.path.abspath(relevant[0]), match[1], back_to_default))
    except TypeError as err:
        logging.exception(err)
        print(red+str(err)+back_to_default)
    finally:
        return output_string


def smart_output_output(file):
    """
    Todo: filter the output ?
    :param file: a HTCondor .out file
    :return: the content of that file as a string
    """

    output_string = ""
    try:
        output_string = read_condor_output(file)
    except NameError as err:
        logging.exception(err)
        print("The smart_output_output method requires a "+std_out+" file as parameter")
    except FileNotFoundError:
        relevant = file.split("/")[-2:]
        match = re.match(r".*?([0-9]{3,}_[0-9]+)"+std_out, relevant[1])
        print(yellow + "There is no related {0} file: {1} in the directory:{2}\n'{3}'\n"
                       " with the prefix: {4}{5}"
              .format(std_out, relevant[1], cyan, os.path.abspath(relevant[0]), match[1], back_to_default))

    finally:
        return output_string


def smart_manage_all(file):
    """
    Combine all informations, that the user wants for a given job_spec_id like 398_31.
    -the first layer represents the HTCondor .log file
    -the second layer represents the HTCondor .err file
    -the third layer represents the HTCondor .out file

    information will be put together regarding the global booleans set by the user.
    job_spec_id, is the ClusterID_ProcessID of a job executed by HTCondor like job_4323_21 or 231_0

    :param file: file name without endling like job4323_1, job4323_1.<err|out|log> will cut the end off
    :return: a string that combines HTCondor log/err and out files and returns it
    """
    try:
        if std_log.__ne__(""):
            job_spec_id = file[:-len(std_log)]
        else:
            job_spec_id = file

        output_string = smart_output_logs(job_spec_id + std_log)  # normal smart_output of log files

        if show_std_errors:  # show errors ?
            output_string += smart_output_error(job_spec_id + std_err)

        if show_std_output:  # show output content ?
            output_string += smart_output_output(job_spec_id + std_out)
    except Exception as err:
        logging.exception(err)
        print(red+str(err)+back_to_default)
        sys.exit(1)
    else:
        return output_string


# Todo: err and output not visible ???
def output_given_logs(log_files):

    output_string = ""
    current_path = os.getcwd()
    # go through all given logs and check for each if it is a directory or file and if std_log was missing or not
    for i, file in enumerate(log_files):

        # for the * operation it should skip std_err and std_out files
        if file.endswith(std_err) and not std_err.__eq__("") or file.endswith(std_out) and not std_out.__eq__(""):
            continue

        # else check if file is a valid file
        if os.path.isfile(file) or os.path.isfile(current_path + "/" + file):
            output_string += smart_manage_all(file)

        # if that did not work try std_log at the end of that file
        elif not file.endswith(std_log):
            new_path = file + std_log
            # is this now a valid file ?
            if os.path.isfile(new_path) or os.path.isfile(current_path + "/" + new_path):
                output_string += smart_manage_all(new_path)
            # no file or directory found, even after manipulating the string
            else:
                logging.error("No such file with that name or prefix: {0}".format(file))
                output_string += red+"No such file with that name or prefix: {0}".format(file)+back_to_default

        # The given .log file was not found
        else:
            output_string += red + "No such file: {0}".format(file) + back_to_default
            logging.error(f"No such file: {file}")

        # don't change output if csv style is wanted
        # Todo:
        if to_csv:
            pass

        elif not i == len(files)-1:
            output_string += "\n" + border_str + "\n"  # if empty it still contains a newline

    if output_string == "":
        print(yellow+"Nothing found, please use \"man htcompact\" or \"htcompact -h\" for help"+back_to_default, end="")

    return output_string


def find_config():
    """

        Try to find a config file in the given arguments.
        Go through the hierarchy in the order:
        - current_working_directory/file
        - current_project_folder/config/file
        - ~/.config/{script_name}/{file}"
        - /etc/{file}

        ONE config will be loaded if set: generate_logging_file

        -> else try to find default config file "htcompact.conf"
         in the same order excluding the current working directory,
         because this might lead to misbehaviour, when the user changes the directory.

        :return: file if found, else None
    """

    script_name = sys.argv[0]
    if script_name.endswith(".py"):
        script_name = sys.argv[0][:-3]  # scriptname without .py
    config = configparser.ConfigParser()
    # search for the given file, first directly, then in /etc and then in ~/.config/htcompact/
    found_config = False

    print(cyan, end="")
    arguments = sys.argv[1:]
    arguments.append(default_configfile)

    found_config_file = None
    found_default_in_cwd = False
    # run through every given argument and try to determine if it's a valid config file
    for i, file in enumerate(arguments):
        # try to find the given file in the current directory, in /etc or in ~/.config/htcompact/
        try:
            # search in current working directory
            if os.path.isfile(file) and config.read(file):
                # only use the file if given, means the default config should be ignored,
                # even if it's in cwd, this will exclude misbehaviour, when cwd is changed
                if not (file.__eq__(default_configfile) and i == len(arguments) - 1):
                    found_config = True
                    arguments.remove(file)
                else:
                    found_config_file = file
                    found_default_in_cwd = True
            # try to find the config file in the current environment hierarchy
            elif os.path.isfile(f"{sys.prefix}/config/{file}") and config.read(f"{sys.prefix}/config/{file}"):
                found_config = True
                arguments.remove(file)
            # try to find config file in ~/.config/script_name/
            elif os.path.isfile(f"~/.config/{script_name}/{file}") \
                    and config.read(f"~/.config/{script_name}/{file}"):
                found_config = True
                arguments.remove(file)
            # try to find config file in /etc
            elif os.path.isfile(f"/etc/{file}") and config.read(f"/etc/{file}"):
                found_config = True
                arguments.remove(file)

            else:
                logging.debug(f"{file} not found or is not a valid config file")

            if found_config:
                found_config_file = file  # remember the file
                break

        # File has no readable format for the configparser, probably because it's not a config file
        except configparser.MissingSectionHeaderError:
            continue
        except configparser.DuplicateOptionError as err:
            print(err)

    try:
        arguments.remove(default_configfile)  # remove again, if other config file was found
    except ValueError:
        pass

    sys.argv[1:] = arguments

    print(back_to_default, end="")

    # prepare output if not and break if not found
    if not found_config and not found_default_in_cwd:
        print(f"{yellow}No valid config file found in: \n"
              f"- {os.getcwd()},\n"
              f"- {sys.prefix}/config,\n"
              f"- ~/.config/htcompact/,\n"
              f"- /etc/ \n"
              f"-> using default settings{back_to_default}")
        return None
    elif found_default_in_cwd:
        print(f"{yellow}Found default {default_configfile} in the current working directory.\n"
              f"It will be ignore, to use this, make sure to pass it as an argument like:\n"
              f"htcompact {default_configfile} [logs] [options]\n"
              f"-> using default settings{back_to_default}")
        return None

    config.read(found_config_file)
    sections = config.sections()
    # check if logging is tuned on
    if 'features' in sections:
        if 'generate_log_file' in config['features']:
            global generate_log_file
            if not generate_log_file:  # if already set, do NOT overwrite
                generate_log_file = config['features']['generate_log_file'].lower() in accepted_states
                logging.getLogger().disabled = not generate_log_file
    return found_config_file


# search for config file ( UNIX BASED )
def load_config(config_file):

    config = configparser.ConfigParser()

    try:

        config.read(config_file)
        # all sections in the config file
        sections = config.sections()

        # now try filter the config file for all available parameters

        print(f"{blue}Load config from file: {config_file}{back_to_default}")

        # all documents like files, etc.
        if 'documents' in sections:
            global files

            if 'files' in config['documents']:
                files = config['documents']['files'].split(" ")
                logging.debug(f"Changed document files to {files}")

        # all formats like the table format
        if 'formats' in sections:
            global table_format
            if 'table_format' in config['formats']:
                table_format = config['formats']['table_format']
                logging.debug(f"Changed default table_format to: {table_format}")

        # all basic HTCondor file endings like .log, .err, .out
        if 'htc-files' in sections:
            global std_log, std_err, std_out

            if 'stdlog' in config['htc-files']:
                std_log = config['htc-files']['stdlog']
                logging.debug(f"Changed default for HTCondor .log files to: {std_log}")

            if 'stderr' in config['htc-files']:
                std_err = config['htc-files']['stderr']
                logging.debug(f"Changed default for HTCondor .err files to: {std_err}")

            if 'stdout' in config['htc-files']:
                std_out = config['htc-files']['stdout']
                logging.debug(f"Changed default for HTCondor .out files to: {std_out}")

        # if show variables are set for further output
        if 'show-more' in sections:
            global show_std_output, show_std_warnings, show_std_errors  # sources to show

            if 'show_std_errors' in config['show-more']:
                show_std_errors = config['show-more']['show_std_errors'].lower() in accepted_states
                logging.debug(f"Changed default show_std_errors to: {show_std_errors}")

            if 'show_std_output' in config['show-more']:
                show_std_output = config['show-more']['show_std_output'].lower() in accepted_states
                logging.debug(f"Changed default show_std_output to: {show_std_output}")

            if 'show_std_warnings' in config['show-more']:
                show_std_warnings = config['show-more']['show_std_warnings'].lower() in accepted_states
                logging.debug(f"Changed default show_std_warnings to: {show_std_warnings}")

        # what information should to be ignored
        if 'ignore' in sections:
            global ignore_list  # sources to ignore

            if 'ignore_list' in config['ignore']:
                for arg in config['ignore']['ignore_list'].split():
                    if arg in allowed_ignore_values:
                        ignore_list.append(arg)
                    else:
                        logging.debug("Don't know this ignore statement: "+ arg)
                logging.debug(f"Changed default ignore statements to: {ignore_list}")

        if 'thresholds' in sections:
            global low_usage, bad_usage
            if 'low_usage' in config['thresholds']:
                low_usage = float(config['thresholds']['low_usage'])
                logging.debug(f"Changed default low_usage to: {low_usage}")
            if 'bad_usage' in config['thresholds']:
                bad_usage = float(config['thresholds']['bad_usage'])
                logging.debug(f"Changed default bad_usage to: {bad_usage}")

        if "modes" in sections:
            global filter_mode, default_mode, summarizer_mode, analyser_mode
            if 'filter_mode' in config['modes']:
                filter_mode = config['modes']['filter_mode'].lower() in accepted_states
                logging.debug(f"Changed filter_mode to: {filter_mode}")

            # if 'default_mode' in config['modes']:
            #     default_mode = config['modes']['default_mode'].lower() in accepted_states
            #     logging.debug(f"Changed default_mode to: {default_mode}")

            if 'summarizer_mode' in config['modes']:
                summarizer_mode = config['modes']['summarizer_mode'].lower() in accepted_states
                logging.debug(f"Changed summarizer_mode to: {summarizer_mode}")

            if 'analyser_mode' in config['modes']:
                analyser_mode = config['modes']['analyser_mode'].lower() in accepted_states
                logging.debug(f"Changed analyser_mode to: {analyser_mode}")

        if "filter" in sections:
            global filter_keywords, filter_extended
            if 'filter_keywords' in config['filter']:
                filter_keywords = config['filter']['filter_keywords']
                logging.debug(f"Changed default filter_keywords to: {filter_keywords}")
            if 'filter_extended' in config['filter']:
                filter_extended = config['filter']['filter_extended'].lower() in accepted_states
                logging.debug(f"Changed default filter_extended to: {filter_extended}")

        # Todo: reverse DNS-Lookup etc.
        if 'features' in sections:
            global reverse_dns_lookup, to_csv  # extra parameters
            # the first thing to check should be if logging is turend on

            if 'reverse_dns_lookup' in config['features']:
                reverse_dns_lookup = config['features']['reverse_dns_lookup'].lower() in accepted_states
                logging.debug(f"Changed default reverse_dns_lookup to: {reverse_dns_lookup}")

            if 'to_csv' in config['features']:
                to_csv = config['features']['to_csv'].lower() in accepted_states
                logging.debug(f"Changed default to_csv to: {to_csv}")

        return True

    except KeyError as err:
        logging.exception(err)


# in order that plotille has nothing like a int converter,
# I have to set it up manually to show the y - label in number format
def _int_formatter(val, chars, delta, left=False):
    """
    Usage of this is shown here:
    https://github.com/tammoippen/plotille/issues/11

    :param val:
    :param chars:
    :param delta:
    :param left:
    :return:
    """
    align = '<' if left else ''
    return '{:{}{}d}'.format(int(val), align, chars)


# Todo: explicitly Job event 000, 001, 005, 006 and 009 are filtered and all other job events that throw errors
#       until now there is not the need for other filters but it might be needed to extend this fucntion
def analyse_logs(log_files):

    if len(log_files) == 0:
        return "No files to analyse"
    elif len(log_files) > 1 and sys.stdout.isatty():
        print("More than one file is given, this mode is meant to be used for single job analysis.\n"
              "This will change nothing, but you should rather do it just for a file one by one")
        if sys.stdin.isatty():
            x = input("Want to continue (y/n): ")
            if x != "y":
                return "Process stoped"

    output_string = ""

    for file in log_files:

        try:
            job_events, job_raw_information = read_condor_log(file)
        except NameError as err:
            logging.exception(err)
            print("The smart_output_logs method requires a " + std_log + " file as parameter")

        # initialise with empty values
        submitted_date, executed_date, terminated_date = None, None, None
        ram_histroy = list()
        occurred_errors = list()
        job_df, res_df = pd.DataFrame(columns=["Description", "Values"]), pd.DataFrame({})

        logging.debug(f"Analysing the HTCondor log file: {file}")
        output_string += green + "Job analysis of: " + file + back_to_default + "\n"

        for job, job_inf in zip(job_events, job_raw_information):
            job_id = job[0]

            # job submitted
            if job_id == "000":
                submitted_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job[5])
                if match_host:
                    from_host = match_host[1]
                    # port = match_host[2]
                    job_df = job_df.append(pd.DataFrame({"Description": "Submitted from host",
                                                         "Values": [from_host]}))

            # job executing
            elif job_id == "001":
                executed_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job[5])
                if match_host:
                    host = match_host[1]
                    # if reverse dns lookup, change the ip to cpu: last number
                    if reverse_dns_lookup:
                        host = gethostbyaddr(host)
                    port = match_host[2]

                    job_df = job_df.append(pd.DataFrame({"Description": ["Executing on Host", "Port"],
                                                         "Values": [host, port]}))
                else:
                    logging.exception("Host and port haven't been matched correctly")
                    print(red + "your log file has faulty values for the host ip,"
                                " make sure it's a valid IPv4" + back_to_default)

            # job terminated
            elif job_id == "005":  # if the last job event is : Job terminated
                terminated_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                # make a fancy design for the job_information
                job_labels = []  # holds the labels
                job_information = []  # holds the related job information

                # check if termination state is normal or abnormal
                match_termination_state = re.match(r"[\t ]+\([0-9]+\) ((?:\w+ )*)", job_inf[0])
                if match_termination_state:
                    job_labels.append("Termination State")  # append the termination state

                    # Normal termination
                    if "Normal termination" in match_termination_state[1]:
                        job_information.append(match_termination_state[1])
                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", job_inf[0])
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)

                    # Abnormal Termination
                    elif "Abnormal termination" in match_termination_state[1]:
                        job_information.append(red+match_termination_state[1]+back_to_default)

                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", job_inf[0])
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)
                        else:
                            job_labels.append("Return Value")
                            job_information.append(return_value)

                        termination_desc = list()
                        # append the reason for the Abnormal Termination
                        for desc in job_raw_information[-1][1:]:
                            if re.match(r"[\t ]+\(0\)", desc):
                                termination_desc.append(desc[5:])
                            else:
                                break

                        if not len(termination_desc) == 0:
                            job_labels.append("Description")
                            job_information.append("\n".join(termination_desc))
                        else:
                            logging.debug(f"No termination description for {file} found")

                    else:
                        job_information.append(yellow+match_termination_state[1]+back_to_default)
                        logging.debug(f"Don't know this Termination State: {match_termination_state[1]} yet")

                else:
                    print(red + f"Termination error in HTCondor log file: {file}" + back_to_default)

                # now put everything together in a table
                job_temp_df = pd.DataFrame({
                    "Description": job_labels,
                    "Values": job_information
                })

                job_df = job_df.append(job_temp_df)

                # these where the job information now focus on the used resources
                relevant_str = "\n".join(job_raw_information[-1])  # job information of the last job (Job terminated)

                # next part removes not useful lines
                if True:  # just for readability
                    # remove unnecessary lines
                    lines = relevant_str.splitlines()
                    # while not lines[0].startswith("\tPartitionable"):
                    while not re.match(r"[\t ]+Partitionable", lines[0]):
                        lines.remove(lines[0])

                    lines.remove(lines[0])
                    partitionable_res = lines
                    # done, partitionable_resources contain now only information about used resources

                # match all resources
                # ****************************************************************************************************
                match = re.match(r"[\t ]+Cpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+)", partitionable_res[0])
                if match:
                    cpu_usage, cpu_request, cpu_allocated = match[1], match[2], match[3]
                else:
                    raise_value_error("Something went wrong reading the cpu information")

                match = re.match(r"[\t ]+Disk \(KB\) *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[1])
                if match:
                    disk_usage, disk_request, disk_allocated = match[1], match[2], match[3]
                else:
                    raise raise_value_error("Something went wrong reading the disk information")

                # check if the log has gpu information in between
                gpu_match_index = 2  # can be ignored if gpu is given memory has to access the next index
                gpu_found = False  # easier to check to add information in the DataFrame
                match = re.match(r"[\t ]+Gpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+) \"(.*)\"",
                                 partitionable_res[gpu_match_index])
                if match:
                    gpu_usage, gpu_request, gpu_allocated, gpu_name = match[1], match[2], match[3], match[4]
                    gpu_match_index += 1
                    gpu_found = True

                match = re.match(r"[\t ]+Memory \(MB\)  *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[gpu_match_index])
                if match:
                    memory_usage, memory_request, memory_allocated = match[1], match[2], match[3]
                else:
                    raise_value_error("Something went wrong reading the memory information")
                # ****************************************************************************************************

                # list of resources and their labels
                resource_labels = ["Cpu", "Disk (KB)", "Memory (MB)"]
                usage = [cpu_usage, disk_usage, memory_usage]
                requested = [cpu_request, disk_request, memory_request]
                allocated = [cpu_allocated, disk_allocated, memory_allocated]

                if gpu_found:
                    resource_labels.append("Gpus")
                    usage.append(gpu_usage)
                    requested.append(gpu_request)
                    allocated.append(gpu_allocated)

                # Error handling: change empty values to NaN in the first column
                for i in range(3):
                    if usage[i] == "":
                        usage[i] = np.nan
                    if requested[i] == "":
                        requested[i] = np.nan
                    if allocated[i] == "":
                        allocated[i] = np.nan

                # put the data in the DataFrame
                res_df = pd.DataFrame({
                    "Rescources": resource_labels,
                    "Usage": usage,
                    "Requested": requested,
                    # "Allocated": allocated
                })
                # if the user wants allocated resources then add it to the DataFrame as well
                temp_index = 3
                if not "allocated-resources" in ignore_list:
                    res_df.insert(temp_index, "Allocated", allocated)
                    temp_index += 1
                if gpu_found:
                    res_df.insert(temp_index, "Assigned", ["", "", "", gpu_name])

            # job image size updated
            elif job_id == "006":
                date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                image_size = job[5]
                memory_usage = re.match("[\t ]+([0-9]+)", job_inf[0])[1]
                resident_set_size = re.match("[\t ]+([0-9]+)", job_inf[1])[1]
                ram_histroy.append([date, int(image_size), int(memory_usage), int(resident_set_size)])

            # job aborted
            elif job_id == "009":
                terminated_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                aborted_by_user = ((job_inf[0]).split(" ")[-1])[:-1]
                # job_df = job_df.append(pd.DataFrame({"Description": "Job was aborted by the user",
                #                                      "Values": [yellow+aborted_by_user+back_to_default]}))
                occurred_errors.append([job_id, terminated_date, "Terminated by user: "+aborted_by_user])

            else:
                # search for errors
                timestamp = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                error_strings = ["error", "err", "warning", "warn", "exception", "fatal"]
                for job_desc in job_inf:
                    for err in error_strings:
                        if err in job_desc.lower():
                            occurred_errors.append([job_id, timestamp,
                                                    re.sub(' \t', '', job_desc)])
                            break  # avoid duplicates

        # managing the time information
        time_dict = dict()
        waiting_time = None
        if submitted_date:
            time_dict["Submission date"] = submitted_date.strftime("%m/%d %H:%M:%S")
        if executed_date:
            time_dict["Execution date"] = executed_date.strftime("%m/%d %H:%M:%S")
            if submitted_date:
                waiting_time = executed_date - submitted_date
        if terminated_date:
            time_dict["Termination date"] = terminated_date.strftime("%m/%d %H:%M:%S")
            if waiting_time:
                time_dict["Waiting time"] = waiting_time
            if executed_date:
                runtime = terminated_date - executed_date
                time_dict["Execution runtime"] = runtime
            if submitted_date:
                total_time = terminated_date - submitted_date
                time_dict["Total runtime"] = total_time
        elif waiting_time:
            time_dict["Termination date"] = "No valid termination occurred"
            time_dict["Waiting time"] = waiting_time

        if len(time_dict) > 0:
            output_string += magenta + "Dates and times" + back_to_default + '\n'
            time_df = pd.DataFrame(time_dict.values(), index=time_dict.keys())
            output_string += tabulate(time_df) + "\n\n"

        if not job_df.empty:
            output_string += magenta + "Execution details" + back_to_default + '\n'
            output_string += tabulate(job_df, showindex=False) + '\n\n'
        if not res_df.empty:
            output_string += magenta + "Resource table" + back_to_default + '\n'
            output_string += tabulate(res_df, showindex=False, headers="keys", tablefmt=table_format) + '\n\n'

        # show HTCondor errors
        if len(occurred_errors) > 0:
            # Todo: is this covering all cases ?
            event_numbers = []
            time_list = []
            err_reason = []

            for err in occurred_errors:
                event_numbers.append(err[0])
                time_list.append(err[1].strftime("%m/%d %H:%M:%S"))
                # if the line is to long, split it
                if len(err[2]) > 50:
                    split = int(len(err[2])/2)
                    while err[2][split] != ' ' and split < len(err[2])-1:
                        split += 1
                    if split > len(err[2])-2:  # if no space was found, change back to normal
                        split = int(len(err[2]) / 2)
                    err_reason.append(err[2][0:split] + '\n' + err[2][split:])
                else:
                    err_reason.append(err[2])

            err_df = pd.DataFrame({
                "Time": time_list,
                "Event Number": event_numbers,
                "Reason": err_reason
            })

            output_string += magenta + "Occurred HTCondor errors" + back_to_default + '\n'
            output_string += tabulate(err_df, showindex=False, headers='keys') + '\n\n'

        # managing the ram history
        if len(ram_histroy) > 0:
            output_string += magenta + "Memory usage over time in MB" + back_to_default + '\n'
            if len(ram_histroy) > 1:
                np_ram = np.array(ram_histroy)
                ram = np_ram[:, 2]  # second column which is memory usage
                dates = np_ram[:, 0]  # first column

                fig = Figure()
                fig.width = 55
                fig.height = 15
                fig.set_x_limits(min_=min(dates))
                fig.set_y_limits(min_=min(ram))
                fig.y_label = "Usage"
                fig.x_label = "Time"

                # this will use the self written function _num_formatter, to convert the y-label to int values
                fig.register_label_formatter(float, _int_formatter)
                fig.plot(dates, ram, lc='green', label="Continuous Graph")  # Todo: implement a way to show thresholds
                fig.scatter(dates, ram, lc='red', label="Single Values")

                if not sys.stdout.isatty():  # if redirected, the Legend is useless
                    output_string += fig.show() + "\n"
                else:
                    output_string += fig.show(legend=True) + "\n"
            else:
                single_date = ram_histroy[0][0].strftime("%m/%d %H:%M:%S")
                single_ram = ram_histroy[0][2]
                output_string += f"Single memory update found:\n" \
                    f"Memory usage on the {single_date} was updatet to {single_ram} MB\n"

            output_string += "\n"

    return output_string


# Todo: what means sucessful executed ?
def summarize_logs(log_files):
    """
    Summarises all used resources and the runtime in total and average

    Runs through the log files via the log_to_dataframe function

    :return:
    """

    valid_files = len(log_files)
    # no given files
    if valid_files == 0:
        return "No files to summarize"

    # allocated all diffrent datatypes, easier to handle
    output_string = ""

    aborted_files = 0
    normal_runtime = datetime.timedelta()
    aborted_runtime = datetime.timedelta()
    cpu_nodes = dict()

    total_cpu_usage = float(0)
    total_disk_usage = int(0)
    total_memory_usage = int(0)

    total_cpu_requested = int(0)
    total_disk_requested = int(0)
    total_memory_requested = int(0)

    total_cpu_allocated = int(0)
    total_disk_allocated = int(0)
    total_memory_allocated = int(0)

    # if gpu given
    gpu_found = False
    total_gpu_usage = float(0)
    total_gpu_requested = int(0)
    total_gpu_allocated = int(0)
    list_of_gpu_names = list()

    for file in track(log_files, transient=True, description="Summarizing..."):
        try:
            log = log_to_dataframe(file)

            log_description, log_resources = log[0], log[1]

            # if the Job was aborted, it still might have a runtime
            if log_description.at[0, 'Description'].__eq__("Aborted by:"):
                # print(f"The job described in {file} was aborted")
                aborted_runtime += log_description.at[1, 'Values']
                aborted_files += 1
                continue

            normal_runtime += log_description.at[2, 'Values']
            cpu = log_description.at[0, 'Values']
            if cpu in cpu_nodes:
                cpu_nodes[cpu][0] += 1
                cpu_nodes[cpu][1] += log_description.at[2, 'Values']
            else:
                cpu_nodes[cpu] = [1, log_description.at[2, 'Values']]

            total_cpu_usage += float(log_resources.at[0, 'Usage']) \
                if str(log_resources.at[0, 'Usage']).lower() != 'nan' else 0
            total_disk_usage += int(log_resources.at[1, 'Usage'])
            total_memory_usage += int(log_resources.at[2, 'Usage'])

            total_cpu_requested += int(log_resources.at[0, 'Requested'])
            total_disk_requested += int(log_resources.at[1, 'Requested'])
            total_memory_requested += int(log_resources.at[2, 'Requested'])

            if not "allocated-resources" in ignore_list:
                total_cpu_allocated += int(log_resources.at[0, 'Allocated'])
                total_disk_allocated += int(log_resources.at[1, 'Allocated'])
                total_memory_allocated += int(log_resources.at[2, 'Allocated'])

            if 3 in log_resources.index:  # this means gpu information is given
                gpu_found = True
                total_gpu_usage += float(log_resources.at[3, 'Usage'])
                total_gpu_requested += int(log_resources.at[3, 'Requested'])
                if not "allocated-resources" in ignore_list:
                    total_gpu_allocated += int(log_resources.at[3, 'Allocated'])
                if log_resources.at[3, 'Assigned'] not in list_of_gpu_names:  # append new gpus
                    list_of_gpu_names.append(log_resources.at[3, 'Assigned'])

        # Error ocurres when Job was aborted
        except ValueError or KeyError as err:
            logging.exception(err)
            print(f"Summarisation error with {file}")
            continue

    n = valid_files - aborted_files  # calc diffrenc of successful executed jobs

    average_runtime = normal_runtime / n if n != 0 else normal_runtime
    average_runtime = datetime.timedelta(days=average_runtime.days, seconds=average_runtime.seconds)
    total_runtime = normal_runtime + aborted_runtime

    output_string += "-"*75 + "\n"

    output_string += f"{valid_files} valid HTCondor job files found\n"
    # it is more obvious, to see None when there was no time to detect
    if aborted_files > 0:
        output_string += f"{aborted_files} of the jobs were aborted\n\n"

    time_dict = dict()

    if normal_runtime != datetime.timedelta(0, 0, 0):
        time_dict["Total runtime of all successful jobs"] = normal_runtime
    if aborted_runtime != datetime.timedelta(0, 0, 0):
        # some jobs were aborted, diffrentiate between normal runtime, aborted runtime and total runtime
        time_dict["Total runtime of all aborted jobs"] = aborted_runtime
        time_dict["Total runtime"] = total_runtime
    if average_runtime:
        time_dict["Average runtime per job\n(only successful executed jobs)"] = average_runtime

    runtime_df = pd.DataFrame(time_dict.items())
    output_string += tabulate(runtime_df, showindex=False, tablefmt='simple') + "\n\n"

    if n != 0:  # do nothing, if all valid jobs were aborted
        df_total = pd.DataFrame({
            "Resources": ['Total Cpu', 'Total Disk (KB)', 'Total Memory (MB)'],
            "Usage": [str(round(total_cpu_usage, 2)),
                      str(total_disk_usage),
                      str(total_memory_usage)],  # necessary
            "Requested": [total_cpu_requested,
                          total_disk_requested,
                          total_memory_requested]

        })
        df_average = pd.DataFrame({
            "Resources": ['Average Cpu', 'Avergae Disk (KB)', 'Average Memory (MB)'],
            "Usage": [round(total_cpu_usage / n, 2),
                      round(total_disk_usage / n, 2),
                      round(total_memory_usage / n, 2)],  # necessary
            "Requested": [round(total_cpu_requested / n, 2),
                          round(total_disk_requested / n, 2),
                          round(total_memory_requested / n, 2)]

        })
        insert_index = 3  # for the row at the right place

        if not "allocated-resources" in ignore_list:
            df_total.insert(insert_index, "Allocated", [total_cpu_allocated,
                                                        total_disk_allocated,
                                                        total_memory_allocated])
            df_average.insert(insert_index, "Allocated", [round(total_cpu_allocated / n, 2),
                                                          round(total_disk_allocated / n, 2),
                                                          round(total_memory_allocated / n, 2)])

        # if gpu information was found
        if gpu_found:
            df_gpu_total = pd.DataFrame({
                "Resources": ['Total Gpu'],
                "Usage": [total_gpu_usage],  # necessary
                "Requested": [total_gpu_requested]
            })
            df_gpu_average = pd.DataFrame({
                "Resources": ['Average Gpu'],
                "Usage": [round(total_gpu_usage / n, 2)],  # Todo: divide by gpu occurrence ????
                "Requested": [round(total_gpu_requested / n, 2)]
            })
            if not "allocated-resources" in ignore_list:
                df_gpu_total.insert(insert_index, "Allocated", [total_gpu_allocated])
                df_gpu_average.insert(insert_index, "Allocated", [round(total_gpu_allocated / n, 2)])
                insert_index += 1

            df_total = df_total.append(df_gpu_total)
            df_average = df_average.append(df_gpu_average)

            # Todo: each gpu gets a single row
            df_total.insert(insert_index, "Assigned", ["", "", "", ", ".join(list_of_gpu_names)])
            df_average.insert(insert_index, "Assigned", ["", "", "", ", ".join(list_of_gpu_names)])
            insert_index += 1

        # Todo: add later
        #output_string += "Total used resources:\n"
        #output_string += tabulate(df_total, showindex=False, headers='keys', tablefmt=table_format) + "\n\n"

        output_string += "The following data only applies to successful executed jobs\n\n"
        output_string += "Used resources in average:\n"
        output_string += tabulate(df_average, showindex=False, headers='keys', tablefmt=table_format) + "\n\n"

    # Todo: cpu nodes might change over lifetime of a job
    if len(cpu_nodes) > 0:

        executed_jobs = list()
        runtime_per_node = list()
        for val in cpu_nodes.values():
            executed_jobs.append(val[0])
            average_job_duration = val[1]/val[0]
            runtime_per_node.append(datetime.timedelta(average_job_duration.days, average_job_duration.seconds))

        df_cpu = pd.DataFrame({
            "Cpu Nodes": list(cpu_nodes.keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        })
        output_string += tabulate(df_cpu, showindex=False, headers='keys', tablefmt=table_format) + "\n"
    output_string += "-"*75
    return output_string


# Todo more specific control what to print out
# Todo ram histogram, BUT HOW
# Todo get files for which the specific termination state appeared ( if not nomral ???)
# Todo: separate Gpus from another
def analysed_summary(log_files):
    """
        analyse the summarized log files,
        this is ment to give the ultimate output about verey simgle log event in average etc.

        Runs through the log files via the log_to_dataframe function

        :return:
        """

    valid_files = len(log_files)
    # no given files
    if valid_files == 0:
        return "No files for the analysed summary"

    # allocated all diffrent datatypes, easier to handle
    output_string = ""

    all_files = dict()  # fill this dict with information, execution type,
    list_of_gpu_names = list()  # list of gpus found

    for file in track(log_files, transient=True, description="Summarizing..."):

        try:
            job_events, job_raw_information = read_condor_log(file)
        except NameError as err:
            logging.exception(err)
            print(f"{red}{err}{back_to_default}")
            break

        # initialise with empty values
        event_number_order = []
        to_host = ""
        submitted_date, executed_date, terminated_date = None, None, None
        ram_histroy = list()
        occurred_errors = list()
        res_df = pd.DataFrame({})

        # analyse
        for job, job_inf in zip(job_events, job_raw_information):
            event_number = job[0]
            event_number_order.append(event_number)

            # job submitted
            if event_number == "000":
                submitted_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job[5])
                if match_host:
                    from_host = match_host[1]
                    # port = match_host[2]

            # job executing
            elif event_number == "001":
                executed_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                match_host = re.match(r".<([0-9]{1,3}(?:\.[0-9]{1,3}){2,3}):([0-9]{,5})\?.*", job[5])
                if match_host:
                    to_host = match_host[1]
                    # if resolve ip to hostname, change the ip to cpu: last number
                    if reverse_dns_lookup:
                        to_host = gethostbyaddr(to_host)
                    port = match_host[2]

                else:
                    logging.exception("Host and port haven't been matched correctly")
                    print(red + "your log file has faulty values for the host ip,"
                                " make sure it's a valid IPv4" + back_to_default)

            # job terminated
            elif event_number == "005":  # if the last job event is : Job terminated
                terminated_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                # check if termination state is normal or abnormal
                match_termination_state = re.match(r"[\t ]+\([0-9]+\) ((?:\w+ )*)", job_inf[0])
                if match_termination_state:
                    termination_state = match_termination_state[1]
                    # Normal termination
                    if "Normal termination" in termination_state:
                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", job_inf[0])
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)

                    # Abnormal Termination
                    elif "Abnormal termination" in termination_state:

                        match_return_value = re.match(r"[\t ]+\([0-9]+\) (?:\w+ )*\((?:\w+ )*([0-9]+)\)", job_inf[0])
                        return_value = match_return_value[1] if match_return_value else "None"
                        if return_value == "None":
                            logging.debug(f"Not a valid return state in the HTCondor log file: {file}")
                            print(red + f"Not a valid return state in the HTCondor log file: {file}" + back_to_default)

                    else:
                        logging.debug(f"Don't know this Termination State: {match_termination_state[1]} yet")

                else:
                    print(red + f"Termination error in HTCondor log file: {file}" + back_to_default)

                # these where the job information now focus on the used resources
                relevant_str = "\n".join(job_raw_information[-1])  # job information of the last job (Job terminated)

                # next part removes not useful lines
                if True:  # just for readability
                    # remove unnecessary lines
                    lines = relevant_str.splitlines()
                    # while not lines[0].startswith("\tPartitionable"):
                    while not re.match(r"[\t ]+Partitionable", lines[0]):
                        lines.remove(lines[0])

                    lines.remove(lines[0])
                    partitionable_res = lines
                    # done, partitionable_resources contain now only information about used resources

                # match all resources
                # ****************************************************************************************************
                match = re.match(r"[\t ]+Cpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+)", partitionable_res[0])
                if match:
                    cpu_usage, cpu_request, cpu_allocated = match[1], match[2], match[3]
                else:
                    raise_value_error("Something went wrong reading the cpu information")

                match = re.match(r"[\t ]+Disk \(KB\) *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[1])
                if match:
                    disk_usage, disk_request, disk_allocated = match[1], match[2], match[3]
                else:
                    raise raise_value_error("Something went wrong reading the disk information")

                # check if the log has gpu information in between
                gpu_match_index = 2  # can be ignored if gpu is given memory has to access the next index
                gpu_found = False  # easier to check to add information in the DataFrame
                match = re.match(r"[\t ]+Gpus *: *([0-9]?(?:\.[0-9]{,3})?) *([0-9]+) *([0-9]+) \"(.*)\"",
                                 partitionable_res[gpu_match_index])
                if match:
                    gpu_usage, gpu_request, gpu_allocated, gpu_name = match[1], match[2], match[3], match[4]
                    gpu_match_index += 1
                    termination_state += "(GPU)"
                    gpu_found = True
                else:
                    termination_state += "(CPU)"

                match = re.match(r"[\t ]+Memory \(MB\)  *: *([0-9]+) *([0-9]+) *([0-9]+)", partitionable_res[gpu_match_index])
                if match:
                    memory_usage, memory_request, memory_allocated = match[1], match[2], match[3]
                else:
                    raise_value_error("Something went wrong reading the memory information")
                # ****************************************************************************************************

                # list of resources and their labels
                resource_labels = ["Cpu", "Disk (KB)", "Memory (MB)"]

                # Error handling: change empty values to NaN in the first column

                if cpu_usage == "":
                    cpu_usage = 0

                usage = [float(cpu_usage), int(disk_usage), int(memory_usage)]
                requested = [float(cpu_request), int(disk_request), int(memory_request)]
                allocated = [float(cpu_allocated), int(disk_allocated), int(memory_allocated)]

                if gpu_found:
                    resource_labels.append("Gpus")
                    usage.append(float(gpu_usage))
                    requested.append(float(gpu_request))
                    allocated.append(float(gpu_allocated))

                # put the data in the DataFrame
                res_df = pd.DataFrame({
                    "Rescources": resource_labels,
                    "Usage": usage,
                    "Requested": requested,
                    # "Allocated": allocated
                })
                # if the user wants allocated resources then add it to the DataFrame as well
                temp_index = 3
                if not "allocated-resources" in ignore_list:
                    res_df.insert(temp_index, "Allocated", allocated)
                    temp_index += 1
                if gpu_found:
                    res_df.insert(temp_index, "Assigned", ["", "", "", gpu_name])
                    if gpu_name not in list_of_gpu_names:  # append new gpus
                        list_of_gpu_names.append(gpu_name)

            # job image size updated
            elif event_number == "006":
                date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                image_size = job[5]
                memory_usage = re.match("[\t ]+([0-9]+)", job_inf[0])[1]
                resident_set_size = re.match("[\t ]+([0-9]+)", job_inf[1])[1]
                ram_histroy.append([date, int(image_size), int(memory_usage), int(resident_set_size)])

            # job aborted
            elif event_number == "009":
                terminated_date = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")

                aborted_by_user = ((job_inf[0]).split(" ")[-1])[:-1]
                occurred_errors.append([event_number, terminated_date, "aborted", "Terminated by user: "+aborted_by_user])

            else:
                # search for errors
                timestamp = datetime.datetime.strptime(job[2] + " " + job[3], "%m/%d %H:%M:%S")
                error_strings = ["error", "err", "warning", "warn", "exception", "fatal"]
                for job_desc in job_inf:
                    for err in error_strings:
                        if err in job_desc.lower():
                            occurred_errors.append([event_number, timestamp, err,
                                                    re.sub(' \t', '', job_desc)])
                            break  # avoid duplicates

        # managing the time information
        waiting_time = datetime.timedelta()
        runtime = datetime.timedelta()
        total_time = datetime.timedelta()
        if executed_date and submitted_date:
            waiting_time = executed_date - submitted_date
        if terminated_date and executed_date:
            runtime = terminated_date - executed_date
        if terminated_date and submitted_date:
            total_time = terminated_date - submitted_date

        # show HTCondor errors
        if len(occurred_errors) > 0:
            # Todo: is this covering all cases ?
            event_numbers = []
            time_list = []
            err_keyword = []
            err_reason = []

            for err in occurred_errors:
                event_numbers.append(err[0])
                time_list.append(err[1].strftime("%m/%d %H:%M:%S"))
                err_keyword.append(err[2].lower())
                # if the line is to long, split it
                if len(err[3]) > 50:
                    split = int(len(err[3]) / 2)
                    while err[3][split] != ' ' and split < len(err[2]) - 1:
                        split += 1
                    if split > len(err[3]) - 2:  # if no space was found, change back to normal
                        split = int(len(err[3]) / 2)
                    err_reason.append(err[3][0:split] + '\n' + err[3][split:])
                else:
                    err_reason.append(err[3])

            # err_df = pd.DataFrame({
            #     "Time": time_list,
            #     "Event Number": event_numbers,
            #     "Error": err_keyword,
            #     "Reason": err_reason
            # })

        # if last termination was not 005, get last occurred error
        if event_number_order[-1].__ne__("005"):
            termination_type = err_keyword[-1]
        else:
            termination_type = termination_state

        # now insert into all_files dictionary
        if termination_type in all_files:
            all_files[termination_type][0] += 1  # count number
            all_files[termination_type][1] += waiting_time
            all_files[termination_type][2] += runtime
            all_files[termination_type][3] += total_time
            if not all_files[termination_type][4].empty:
                # add usages
                all_files[termination_type][4]["Usage"] = pd.Series(
                    all_files[termination_type][4]["Usage"].values +
                    res_df["Usage"].values)
                # add requested
                all_files[termination_type][4]["Requested"] = pd.Series(
                    all_files[termination_type][4]["Requested"].values +
                    res_df["Requested"].values)
                # allocated
                all_files[termination_type][4]["Allocated"] = pd.Series(
                    all_files[termination_type][4]["Allocated"].values + res_df["Allocated"].values)

            # add cpu
            if to_host != "":
                if to_host in all_files[termination_type][5].keys():
                    all_files[termination_type][5][to_host][0] += 1
                    all_files[termination_type][5][to_host][1] += total_time
                else:
                    all_files[termination_type][5][to_host] = [1, total_time]
            elif 'Aborted before submission' in all_files[termination_type][5].keys():
                    all_files[termination_type][5]['Aborted before submission'][0] += 1
                    all_files[termination_type][5]['Aborted before submission'][1] += total_time
            else:
                count_cpus = dict()
                count_cpus['Aborted before submission'] = [1, total_time]
                all_files[termination_type][5] = count_cpus

        # else new entry
        else:
            # if host exists
            if to_host != "":
                count_cpus = dict()
                count_cpus[to_host] = [1, total_time]
                all_files[termination_type] = [1, waiting_time, runtime, total_time, res_df, count_cpus]
            else:
                count_cpus = dict()
                count_cpus['Aborted before submission'] = [1, total_time]
                all_files[termination_type] = [1, waiting_time, runtime, total_time, res_df, count_cpus]

    # now print out the summary
    for term_state in all_files:
        term_info = all_files[term_state]

        output_string += f"\n{blue}All files summarized with the Termination State: {term_state}{back_to_default}\n"

        desc_df = pd.DataFrame({
            "Description": ["Occurence", "Waiting Time", "Runtime", "Total"],
            "Values": [term_info[0], term_info[1], term_info[2], term_info[3]]
        })

        output_string += tabulate(desc_df, showindex=False, headers='keys', tablefmt=table_format) + '\n'

        if not term_info[4].empty:
            average_res_df = term_info[4]
            average_res_df['Usage'] = pd.Series(np.round(average_res_df['Usage'].values / term_info[0], 2))
            average_res_df['Requested'] = pd.Series(np.round(average_res_df['Requested'].values / term_info[0], 2))
            average_res_df['Allocated'] = pd.Series(np.round(average_res_df['Allocated'].values / term_info[0], 2))
            new_headers = ['Resources', 'Average Usage', 'Average Requested', 'Average Allocated']
            if 'Assigned' in average_res_df.keys():
                new_headers.append('Assigned')
                average_res_df['Assigned'] = pd.Series(['', '', '', ", ".join(list_of_gpu_names)])
            output_string += tabulate(average_res_df, showindex=False, headers=new_headers, tablefmt=table_format) + '\n'

        executed_jobs = list()
        runtime_per_node = list()
        for val in term_info[5].values():
            executed_jobs.append(val[0])
            average_job_duration = val[1] / val[0]
            runtime_per_node.append(datetime.timedelta(average_job_duration.days, average_job_duration.seconds))
        cpu_df = pd.DataFrame({
            "Cpu Nodes": list(term_info[5].keys()),
            "Executed Jobs": executed_jobs,
            "Average job duration": runtime_per_node
        })

        output_string += tabulate(cpu_df, showindex=False, headers='keys', tablefmt=table_format) + '\n'

    return output_string


# search in the files for the keywords
# Todo: show which keywords have been found and which not
# Todo: AND option, ONLY option
def filter_for(log_files, keywords, extend=False):

    keyword_list = keywords.split()

    # if extend is set, keywords like err will also look for keywords like warn exception, aborted, etc.
    if extend:  # apply some rules
        # the error list
        err_list = ["err", "warn", "exception", "aborted", "abortion", "abnormal", "fatal"]

        # remove keyword if already in err_list
        for i in range(len(keyword_list)):
            if (keyword_list[i]).lower() in err_list:
                keyword_list.remove(keyword_list[i])

        keyword_list.extend(err_list) # extend search

        print(green + f"Keyword List was extended, now search for these keywords: {keyword_list}" + back_to_default)
    else:
        print(green + f"Search for these keywords: {keyword_list}" + back_to_default)

    if len(keyword_list) == 1 and keyword_list[0] == "":
        logging.debug("Empty filter, don't know what to do")
        return f"{yellow}Don't know what to do with an empty filter,\n" \
            f"if you activate the filter mode in the config file, \n" \
            f"please add a [filter] section with the filter_keywords = your_filter"

    logging.debug(f"These are the keywords to look for: {keyword_list}")

    # now search
    found_at_least_one = False
    found_logs = []
    for file in log_files:
        found = False
        with open(file, "r") as read_file:
            for line in read_file:
                for keyword in keyword_list:
                    if re.search(keyword.lower(), line.lower()):
                        if not found_at_least_one:
                            print("Matches:")
                        # Todo: save numbers of occurence for each keyword, if more than one is given
                        # Todo: can be commenetd, or maybe should be hidden, if more than like 50 files are found
                        print(f"{lightgrey}{keyword} in: \t{file}{back_to_default}")
                        found = True
                        found_at_least_one = True
                        break
                if found:
                    found_logs.append(file)
                    break

    output_string = ""
    if not found_at_least_one:
        print(red + f"Unable to find these keywords: {keyword_list}" + back_to_default)
        print(red + f"maybe try again with similar expressions" + back_to_default)

    else:
        print(f"Total count: {len(found_logs)}")
        if default_mode:
            output_string = output_given_logs(found_logs)
        elif analyser_mode and summarizer_mode:
            print(f"{magenta}Try to give an analysed summary for these files{back_to_default}")
            output_string = analysed_summary(found_logs)
        elif summarizer_mode:
            print(f"{magenta}Try to summarize these files{back_to_default}")
            output_string = summarize_logs(found_logs)
        elif analyser_mode:
            print(f"{magenta}Try to analyse these files{back_to_default}")
            output_string = analyse_logs(found_logs)
        elif sys.stdin.isatty() and sys.stdout.isatty():  # if not redirected
            x = input(f"{blue}Want to do more? \n[print default(d), summarize(s),"
                      f" analyse(a), analysed summary(as), exit(e)]: {back_to_default}")
            if x == "d":
                output_string = output_given_logs(found_logs)
            elif x == "s":
                output_string = summarize_logs(found_logs)
            elif x == "a":
                output_string = analyse_logs(found_logs)
            elif x == "as":
                output_string = analysed_summary(found_logs)

    return output_string


def validate_given_logs(file_list):
    """
    This method is supposed to take the given log files (by config or command line argument)
    and tries to determine if these are valid log files, ignoring the std_log specification, if std_log = "".

    It will run through given directories and check every single file, if accessible, for the
    HTCondor log file standard, valid log files should start with lines like:

    000 (5989.000.000) 03/30 15:48:47 Job submitted from host: <10.0.8.10:9618?addrs=10.0.8.10-9618&noUDP&sock=3775629_0774_3>

    if done, it will change the global files list and store only valid log files.
    The user will be remind, what files were accepted as "valid".

    The user will also be informed if a given file was not found.

    Todo: In addition there should be an option --force, that makes the script stop, if the file was not found or marked as valid

    Todo: user should be able to get the option to decide, when the same files appear more than once
    -> my guess: yes or no question, if nothing is given in under 10 seconds, it should go with no
    -> this should prevent, that the script is stucked, if the user is for example running it over night

    """
    valid_files = list()
    total = len(file_list)

    with Progress(transient=True, redirect_stdout=False) as progress:

        task = progress.add_task("Validating...", total=total, start=False)

        for arg in file_list:

            path = os.getcwd()  # current working directory , should be condor job summarizer script
            logs_path = path + "/" + arg  # absolute path

            working_dir_path = ""
            working_file_path = ""

            if os.path.isdir(arg):
                working_dir_path = arg
            elif os.path.isdir(logs_path):
                working_dir_path = logs_path

            elif os.path.isfile(arg):
                working_file_path = arg
            elif os.path.isfile(logs_path):
                working_file_path = logs_path
            # check if only the id was given and resolve it with the std_log specification
            elif os.path.isfile(arg+std_log):
                working_file_path = arg+std_log
            elif os.path.isfile(logs_path+std_log):
                working_file_path = logs_path+std_log

            # if path is a directory
            if working_dir_path.__ne__(""):
                print(f"{magenta}Try to find valid log files in the given directory:{back_to_default}\n"
                      f"{working_dir_path}")
                # run through all files and separate the files into log/error and output files
                valid_dir_files = list()
                file_dir = os.listdir(working_dir_path)
                total += len(file_dir) - 1
                for file in file_dir:
                    progress.update(task, total=total, advance=1)
                    # ignore hidden files
                    if file.startswith("."):
                        continue
                    # if it's not a sub folder
                    if working_dir_path.endswith('/'):
                        file_path = working_dir_path + file
                    else:
                        file_path = working_dir_path + '/' + file

                    if os.path.isfile(file_path):
                        with open(file_path, "r") as read_file:
                            # Todo: better specification with re
                            if os.path.getsize(file_path) == 0:  # file is empty
                                continue

                            if re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)", read_file.readlines()[0]):
                                # logging.debug(f"{read_file.name} is a valid HTCondor log file")
                                valid_dir_files.append(file_path)

                    else:
                        logging.debug(f"Found a subfolder: {working_dir_path}/{file}, it will be ignored")
                        progress.console.print(f"[yellow]Found a subfolder: {working_dir_path}/{file},"
                                               f" it will be ignored[/yellow]")

                valid_files.extend(valid_dir_files)
                print(f"{green}Found {len(valid_dir_files)} valid log files out of {len(file_dir)} files{back_to_default}")
            # else if path "might" be a valid HTCondor file
            elif working_file_path.__ne__(""):
                progress.update(task, advance=1)
                with open(working_file_path, "r") as read_file:

                    if os.path.getsize(working_file_path) == 0:  # file is empty
                        progress.console.print(f"[red]How dare you, the file is empty: {read_file.name} :([/red]")

                    elif re.match(r"[0-9]{3} \([0-9]+.[0-9]+.[0-9]{3}\)", read_file.readlines()[0]):
                        logging.debug(f"{read_file.name} is a valid HTCondor log file")
                        valid_files.append(working_file_path)
                    else:
                        logging.debug(f"The given file {read_file.name} is not a valid HTCondor log file")
                        progress.console.print(f"[yellow]The given file {read_file.name} "
                                               f"is not a valid HTCondor log file[/yellow]",)
            else:
                logging.error(f"The given file: {arg} does not exist")
                print(red+f"The given file: {arg} does not exist"+back_to_default)

    return valid_files


def escape_seq_when_redirect():
    """
    This method should be activated first, when output is generated
    it does nothing, if output is printed to the terminal.
    If output gets redirected with > or | or other redirection tools, ignore escape sequences
    by setting them to ""
    :return:
    """
    # if output gets redirected with > or | or other redirection tools, ignore escape sequences
    if not sys.stdout.isatty():
        global red, green, yellow, magenta, cyan, blue, lightgrey, back_to_default
        red = ""
        green = ""
        yellow = ""
        magenta = ""
        cyan = ""
        blue = ""
        lightgrey = ""
        back_to_default = ""


if __name__ == "__main__":
    """
    This is the main function, which searchs first for a given config file.
    After that, given parameters in the terminal will be interpreted, so they have a higher priority

    Then it will summarize all given log files together and print the output

    :return:
    """
    try:
        logging.getLogger().disabled = True

        escape_seq_when_redirect()  # set the color sequenzes to ""

        # if exit parameters are given that will interrupt this script, like --help,
        # catch them here so the config won't be unnecessary loaded
        manage_prioritized_params()

        # if not --no-config is set:
        # interpret the first file, that can be interpreted as a config file and remove it, other given config files,
        # (you will see when you try, files will be interpreted as HTCondor log files)
        # so it's not possible to give multiple config files
        if not no_config:
            found_conf = find_config()

        # set up logging tool
        if not logging.getLogger().disabled:
            # I don't know why a handler is already set,
            # but we have to remove him in order to change the basicConfig
            if len(logging.root.handlers) == 1:
                default_handler = logging.root.handlers[0]
                logging.root.removeHandler(default_handler)
            logging_format = '%(asctime)s - [%(funcName)s:%(lineno)d] %(levelname)s : %(message)s'
            logging.basicConfig(filename="htcompact.log",
                                level=logging.DEBUG,
                                format=logging_format)

        logging.debug("-------Start of htcompact scipt-------")

        if found_conf is not None:
            load_config(found_conf)

        # after that, interpret the command line arguments, these will have a higher priority, than the config setup
        # and by that arguments set in the config file, will be ignored
        manage_params()

        files = validate_given_logs(files)  # validate the files, make a list of all valid config files

        if filter_mode:
            output_str = filter_for(files, filter_keywords, filter_extended)
        elif default_mode:
            output_str = "\n" + output_given_logs(files)  # force default with -d
        elif summarizer_mode and analyser_mode:
            output_str = analysed_summary(files)  # analysed summary ?
        elif summarizer_mode:
            output_str = summarize_logs(files)  # summarize information
        elif analyser_mode:
            output_str = analyse_logs(files)  # analyse the given files
        else:
            output_str = "\n" + output_given_logs(files)  # anyways try to print default output

        print(output_str)  # write it to the console

        logging.debug("-------End of htcompact script-------")

    except KeyboardInterrupt:
        print("Script was interrupted")


